<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Automatic Differentiation - FreeTensor</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
        <link href="../../assets/_mkdocstrings.css" rel="stylesheet" />
        <link href="../../extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Automatic Differentiation";
        var mkdocs_page_input_path = "guide/ad.md";
        var mkdocs_page_url = null;
      </script>
    
    <script src="../../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../..">
          <img src="../../resource/logo-light.png" class="logo" alt="Logo"/>
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">FreeTensor</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://github.com/roastduck/FreeTensor">GitHub</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">User Guide</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../">Get Started</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../build-and-run/">Build and Run</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../first-program/">Your First Program with FreeTenor</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../schedules/">Optimize a Program with Schedules</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../hint/">Optimize a Program with Hints</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../gpu/">Running on a GPU</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">Automatic Differentiation</a>
    <ul class="current">
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">API Reference</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../api/">Python API</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../doxygen/html/">Internal C++ Interface</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">About</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../about/contrib/">Contributing</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../about/pub/">Publication</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://github.com/roastduck/FreeTensor/blob/master/LICENSE">License</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">FreeTensor</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" alt="Docs"></a> &raquo;</li>
          <li>User Guide &raquo;</li>
      <li>Automatic Differentiation</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="automatic-differentiation">Automatic Differentiation<a class="headerlink" href="#automatic-differentiation" title="Permanent link">&para;</a></h1>
<div class="toc">
<ul>
<li><a href="#reverse-mode-ad">Reverse-Mode AD</a></li>
<li><a href="#providing-your-custom-gradients">Providing Your Custom Gradients</a><ul>
<li><a href="#why-or-when-do-we-need-custom-gradients">Why or When do We Need Custom Gradients</a></li>
<li><a href="#how-to-write-custom-gradients-in-freetensor">How to Write Custom Gradients in FreeTensor</a></li>
<li><a href="#additional-descriptions-on-push_for_backward">Additional Descriptions on push_for_backward</a></li>
</ul>
</li>
</ul>
</div>
<p>Automatic Differentiation (AD) transforms a program to another program that computes the original one's derivative or gradient. FreeTensor supports Reverse-Mode AD, and there is a plan to support Forward-Mode AD in the future.</p>
<h2 id="reverse-mode-ad">Reverse-Mode AD<a class="headerlink" href="#reverse-mode-ad" title="Permanent link">&para;</a></h2>
<p>Suppose there is a program <code>x -&gt; y -&gt; z -&gt; w</code> that computes an output <code>w</code> from intermediate variables <code>z</code> and <code>y</code>, and an input variable <code>x</code>. Reverse-Mode AD generates a gradient program <code>dw/dw=1 -&gt; dw/dz -&gt; dw/dy -&gt; dw/dx</code> that computes <code>dw/dx</code> by Chain Rule. <code>y</code>, <code>z</code> and <code>w</code> may be saved in a "tape" when evaluation the original program, to be reused in the gradient one.</p>
<p>If FreeTensor is built with <code>WITH_PYTORCH=ON</code>, you can skip this section and turn to the <a href="../first-program/#copy-free-interface-fromto-pytorch"><code>@optimize_to_pytorch</code> integration</a>, which integrates seamlessly with PyTorch's autograd mechanism, but will incur some runtime overhead.</p>
<p>Here is an example of Reverse-Mode AD in FreeTensor:</p>
<pre><code class="language-python">import freetensor as ft
import numpy as np

n = 4

@ft.optimize
@ft.grad(requires=['a', 'b'], provides=[ft.Return()], attach_backward=True)
def test(a: ft.Var[(n,), &quot;float32&quot;], b: ft.Var[(n,), &quot;float32&quot;]):
    y = ft.zeros((), &quot;float32&quot;)
    for i in range(n):
        y[()] += a[i] * b[i]
    return y

a = np.array([0, 1, 2, 3], dtype=&quot;float32&quot;)
b = np.array([3, 2, 1, 0], dtype=&quot;float32&quot;)
y = test(a, b)
print(y.numpy())
dzdy = np.array(1, dtype='float32')
input_grads = test.input_name_to_gradient_name
output_grads = test.output_name_to_gradient_name
dzda, dzdb = test.backward(
    **{output_grads[ft.Return()]: dzdy})[input_grads['a'], input_grads['b']]
print(dzda.numpy())
print(dzdb.numpy())
</code></pre>
<p>You need to call <a href="../../api/#freetensor.core.autograd.grad"><code>ft.grad</code></a> (or the inplace version <a href="../../api/#freetensor.core.autograd.grad_"><code>ft.grad_</code></a>) to generate a <em>forward</em> function and a <em>backward</em> function. In this example, the backward function is attached as the <code>test.backward</code> property because <code>attach_backward</code> is set to <code>True</code>. You can set it to <code>False</code> and <code>ft.grad</code> will return both functions. Please note that <code>test</code> is updated by <code>ft.grad</code> and becomes different than the original function, as it may save some intermediate tensors to a global <code>tape</code>, and it must be executed before the backward <code>test.backward</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note on JIT</p>
<p><a href="../first-program/#just-in-time-jit-compilation">JIT</a> is only supported when <code>attach_backward = True</code>.</p>
</div>
<p>After that, you call <code>ft.optimize</code> to optimize and compile the program just as in previous examples. This time it is done for both <code>test</code> and <code>test.backward</code>.</p>
<p>Finally, you execute <code>test</code> and <code>test.backward</code>. The parameters and return values of <code>test.backward</code> are the gradients of <code>a</code>, <code>b</code> and <code>y</code>, which have their own names. To set and get these parameters and return values, you look up for them in two dictionaries <code>test.input_name_to_gradient_name</code> and <code>test.output_name_to_gradient_name</code> (in type <a href="../../api/#freetensor.core.autograd.ParamRetDict"><code>ft.ParamRetDict</code></a>. These two dictionaries accept either a name of a parameter, or a special <a href="../../api/#freetensor.core.autograd.Return"><code>ft.Return</code></a> to specify a return value. When invoking <code>test.backward</code>, parameters can be set via keyword arguments, and return values can be collect via a bracket (from a special type <a href="../../api/#freetensor.core.driver.ReturnValuesPack"><code>ft.ReturnValuesPack</code></a>). These two maps are attached to <code>test</code> because <code>attach_backward</code> is <code>True</code>. Otherwise, they are returned as return values from <code>ft.grad</code>.</p>
<p>Intermediate variables are not always have to be saved to the "tape" from the forward function. If a variable is need in the backward function but not saved, it will be re-computed, which is sometimes even faster than saving it due to better locality. By default, FreeTensor uses heuristics to determine which variable to save. To get better performance, you may want to control which intermediate variables should be saved by setting an optional <code>tapes</code> parameter in <code>ft.grad</code>. <code>tapes</code> can either be a different mode, or a explicit list of AST node IDs of all <code>VarDef</code> nodes of the variables you want to save.</p>
<h2 id="providing-your-custom-gradients">Providing Your Custom Gradients<a class="headerlink" href="#providing-your-custom-gradients" title="Permanent link">&para;</a></h2>
<h3 id="why-or-when-do-we-need-custom-gradients">Why or When do We Need Custom Gradients<a class="headerlink" href="#why-or-when-do-we-need-custom-gradients" title="Permanent link">&para;</a></h3>
<p>Sometimes neither reverse-mode or forward-mode AD produces the most elegant form of gradients. FreeTensor allows you to provide your own gradients for part of the program.</p>
<p>Take softmax as an example: The <span class="arithmatex">\(\mathbf{y} = softmax(\mathbf{x})\)</span> function is <strong>mathematically defined</strong> by the following steps:</p>
<div class="arithmatex">\[\begin{align}
e_i &amp;= \mathrm{e}^{x_i} \label{eq:softmax-1} \\
s &amp;= \sum_i{e_i} \label{eq:softmax-2} \\
y_i &amp;= \frac{e_i}{s} \label{eq:softmax-3}
\end{align}\]</div>
<p>Suppose the final output of the program (the loss) is <span class="arithmatex">\(z\)</span>. If using reverse-mode AD, the gradient of the input: <span class="arithmatex">\(\frac{\partial z}{\partial x}\)</span> can be computed by the following steps:</p>
<div class="arithmatex">\[\begin{align}
\frac{\partial z}{\partial s} &amp;= -\sum_i{\frac{\partial z}{\partial y_i} \frac{y_i}{s}} \label{eq:softmax-grad-1} \\
\frac{\partial z}{\partial e_i} &amp;= \frac{\partial z}{\partial y_i} \frac{1}{s} + \frac{\partial z}{\partial s} \label{eq:softmax-grad-2} \\
\frac{\partial z}{\partial x_i} &amp;= \frac{\partial z}{\partial e_i} e_i \label{eq:softmax-grad-3}
\end{align}\]</div>
<p>However, usually we can NOT compute softmax by Equation <span class="arithmatex">\(\eqref{eq:softmax-1}\eqref{eq:softmax-2}\eqref{eq:softmax-3}\)</span> for numerical stability issues. Pratically, we <strong>compute</strong> softmax with additional normalization on <span class="arithmatex">\(\mathbf{x}\)</span>:</p>
<div class="arithmatex">\[\begin{align}
m &amp;= \max_i{x_i} \label{eq:softmax-norm-1} \\
e_i &amp;= \mathrm{e}^{x_i - m} \label{eq:softmax-norm-2} \\
s &amp;= \sum_i{e_i} \label{eq:softmax-norm-3} \\
y_i &amp;= \frac{e_i}{s} \label{eq:softmax-norm-4}
\end{align}\]</div>
<p>If we directly apply reverse-mode AD on Equation <span class="arithmatex">\(\eqref{eq:softmax-norm-1}\eqref{eq:softmax-norm-2}\eqref{eq:softmax-norm-3}\eqref{eq:softmax-norm-4}\)</span>, the backward program will be like:</p>
<div class="arithmatex">\[\begin{align}
\frac{\partial z}{\partial s} &amp;= -\sum_i{\frac{\partial z}{\partial y_i} \frac{y_i}{s}} \\
\frac{\partial z}{\partial e_i} &amp;= \frac{\partial z}{\partial y_i} \frac{1}{s} + \frac{\partial z}{\partial s} \\
\frac{\partial z}{\partial m} &amp;= -\sum_i{\frac{\partial z}{\partial e_i}e_i} \\
\frac{\partial z}{\partial x_i} &amp;= \frac{\partial z}{\partial e_i} e_i + \begin{cases}\frac{\partial z}{\partial m}, &amp;i = \arg\max_j{x_j} \\ 0, &amp;i \neq \arg\max_j{x_j}\end{cases}
\end{align}\]</div>
<p>You may have found that there is an extra <span class="arithmatex">\(\frac{\partial z}{\partial m}\)</span> involved. Apparently, the gradient should be the same no matter if we do the normalization. This is because <span class="arithmatex">\(\frac{\partial z}{\partial m}\)</span> actually always equals to <span class="arithmatex">\(0\)</span>. FreeTensor can not dig out this mathematical property, so the computation on <span class="arithmatex">\(\frac{\partial z}{\partial m}\)</span> will remain and will be wasted.</p>
<h3 id="how-to-write-custom-gradients-in-freetensor">How to Write Custom Gradients in FreeTensor<a class="headerlink" href="#how-to-write-custom-gradients-in-freetensor" title="Permanent link">&para;</a></h3>
<p>The following examples will demonstrate how to provide your own custom gradients, to override the default AD behaviour. <strong>Please note that this is only for demonstration. If you are just going to use softmax, call it from <a href="../../api/#freetensor.libop.softmax"><code>libop.softmax</code></a>, which has already implemented the following code.</strong></p>
<p>First we show a softmax implementation with full AD:</p>
<pre><code class="language-python">import freetensor as ft
import torch

n = 4

@ft.optimize  # Set verbose=1 to see the code
@ft.grad(requires=['x'], provides=[ft.Return()], attach_backward=True)
def test(x: ft.Var[(n,), &quot;float32&quot;]):
    # Automatically decide gradients for this statement
    m = ft.reduce_max(x, axes=[-1])
    e = ft.exp(x - m)
    s = ft.reduce_sum(e, axes=[-1])
    y = e / s
    return y

# Check forward result
x = torch.rand(n, dtype=torch.float32)
x.requires_grad = True
y_ft = test(x).torch()
y_torch = torch.softmax(x, axis=-1)
assert torch.all(torch.isclose(y_ft, y_torch))

# Check backward result
y_torch.grad = dzdy = torch.rand(n, dtype=torch.float32)
input_grads = test.input_name_to_gradient_name
output_grads = test.output_name_to_gradient_name
dzdx_ft = test.backward(**{output_grads[ft.Return()]: dzdy}).torch()
y_torch.backward(y_torch.grad)
dzdx_torch = x.grad
assert torch.all(torch.isclose(dzdx_ft, dzdx_torch, 1e-4, 1e-7))
</code></pre>
<p>Then, we add our own gradient to it:</p>
<pre><code class="language-python">import freetensor as ft
import torch

n = 4

@ft.optimize  # Set verbose=1 to see the code
@ft.grad(requires=['x'], provides=[ft.Return()], attach_backward=True)
def test(x: ft.Var[(n,), &quot;float32&quot;]):
    # Mark the range that you want to provide graident for, with `StmtRange`
    with ft.StmtRange() as rng:
        m = ft.reduce_max(x, axes=[-1])
        e = ft.exp(x - m)
        s = ft.reduce_sum(e, axes=[-1])
        y = e / s

        # Call `push_for_backward` so we can use forward values in backward
        e_now = ft.push_for_backward(e)
        s_now = ft.push_for_backward(s)
        y_now = ft.push_for_backward(y)
    # Define gradient in `UserGrad`
    with ft.UserGrad(x, y, stmt_range=rng) as (dzdx, dzdy):
        # Retrieve forward value from `y_now`, NOT `y`
        dzds = -ft.reduce_sum(dzdy * y_now, axes=[-1]) / s_now
        dzde = dzdy / s_now + dzds
        dzdx[...] += dzde * e_now  # Use `+=` here
    return y

# Check forward result
x = torch.rand(n, dtype=torch.float32)
x.requires_grad = True
y_ft = test(x).torch()
y_torch = torch.softmax(x, axis=-1)
assert torch.all(torch.isclose(y_ft, y_torch))

# Check backward result
y_torch.grad = dzdy = torch.rand(n, dtype=torch.float32)
input_grads = test.input_name_to_gradient_name
output_grads = test.output_name_to_gradient_name
dzdx_ft = test.backward(**{output_grads[ft.Return()]: dzdy}).torch()
y_torch.backward(y_torch.grad)
dzdx_torch = x.grad
assert torch.all(torch.isclose(dzdx_ft, dzdx_torch, 1e-4, 1e-7))
</code></pre>
<p>First, <strong>we mark the range of code that we want to provide gradient for, with <code>ft.StmtRange</code>,</strong> as a name <code>rng</code>. In the range, we write the code to compute <code>softmax</code> as usual. Additionaly, for the values that we want to reuse in the gradient, <strong>we call <code>ft.push_for_backward</code> to save it.</strong> <code>push_for_backward</code> returns a handle that you can use as a usual tensor in the gradient code. If your <code>StmtRange</code> is inside an outer loop, the handle will always reflect the correct iteration (see the next example). Besides, <code>push_for_backward</code> does not mean the value will be physically saved in tape: it only means the value will be logically reused in the backward, no matter by saving or by recomputing. <code>push_for_backward</code> is orthogonal with the <code>tapes</code> parameter in <code>ft.grad</code>.</p>
<p>Next, <strong>we define our custom gradient with a <code>ft.UserGrad</code> scope.</strong> The scopes receives a special parameter <code>stmt_range</code>, which should be set to the <code>StmtRange</code> we have just defined. Beside <code>stmt_range</code>, <code>UserGrand</code> receives an arbitrary number of parameters, in this case, <code>x</code> and <code>y</code>, and returns the same number of variables, <code>dzdx</code> and <code>dzdy</code>, so we have the mapping between each variable and its gradient. What we are going to do is update <code>dzdx</code> from <code>dzdy</code>.</p>
<p>We define our gradient code in the <code>UserGrad</code> code of Equation <span class="arithmatex">\(\eqref{eq:softmax-grad-1}\eqref{eq:softmax-grad-2}\eqref{eq:softmax-grad-3}\)</span>. We want to use the forward value <code>y</code>, <code>s</code> and <code>e</code>. But <strong>do NOT directly use its name, use the <code>push_for_backward</code> handler <code>y_now</code>, <code>s_now</code> and <code>e_now</code> instead.</strong> Finally, plase note that <strong>we update <code>dzdx</code> with <code>+=</code> instead of <code>=</code>,</strong> because we may be only computing a partial derivative: there may be other functions of <code>x</code> other than <code>y</code>.</p>
<p>And it is all done.</p>
<h3 id="additional-descriptions-on-push_for_backward">Additional Descriptions on <code>push_for_backward</code><a class="headerlink" href="#additional-descriptions-on-push_for_backward" title="Permanent link">&para;</a></h3>
<p>We have mentioned <code>push_for_backward</code> will automatically handle multiple versions of a variable. If you are familiar with PyTorch, you may have found the name is similar to PyTorch's <code>save_for_backward</code>. Here, versioning is the major difference: <code>ft.push_for_backward</code> can be called multiple times on a variable, to save multiple version (or snapshot of it), while the variable can keep changing.</p>
<p>Here is an additional example: a softmax written in a loop form, where we receives a 2-d input, and apply softmax on the second dimension. Again, this is only for demonstration, and there are multiple ways to implement a softmax.</p>
<pre><code class="language-python">import freetensor as ft
import torch

n = 4

@ft.optimize  # Set verbose=1 to see the code
@ft.grad(requires=['x'], provides=[ft.Return()], attach_backward=True)
def test(x: ft.Var[(n, n), &quot;float32&quot;]):
    y = ft.empty((n, n), &quot;float32&quot;)
    for i in range(n):
        # Mark the range that you want to provide graident for, with `StmtRange`
        with ft.StmtRange() as rng:
            # `m`, `e` and `s` are local to `i`
            m = ft.reduce_max(x[i], axes=[-1])
            e = ft.exp(x[i] - m)
            s = ft.reduce_sum(e, axes=[-1])
            y[i] = e / s

            # Call `push_for_backward` so we can use forward values in backward
            e_now = ft.push_for_backward(e)
            s_now = ft.push_for_backward(s)
            y_now = ft.push_for_backward(y)
        # Define gradient in `UserGrad`
        with ft.UserGrad(x, y, stmt_range=rng) as (dzdx, dzdy):
            # Retrieve forward value from `y_now`, NOT `y`
            dzds = -ft.reduce_sum(dzdy[i] * y_now[i], axes=[-1]) / s_now
            dzde = dzdy[i] / s_now + dzds
            dzdx[i] += dzde * e_now  # Use `+=` here
    return y

# Check forward result
x = torch.rand(n, n, dtype=torch.float32)
x.requires_grad = True
y_ft = test(x).torch()
y_torch = torch.softmax(x, axis=-1)
assert torch.all(torch.isclose(y_ft, y_torch))

# Check backward result
y_torch.grad = dzdy = torch.rand(n, n, dtype=torch.float32)
input_grads = test.input_name_to_gradient_name
output_grads = test.output_name_to_gradient_name
dzdx_ft = test.backward(**{output_grads[ft.Return()]: dzdy}).torch()
y_torch.backward(y_torch.grad)
dzdx_torch = x.grad
assert torch.all(torch.isclose(dzdx_ft, dzdx_torch, 1e-4, 1e-7))
</code></pre>
<p>Here our gradient scope is inside a loop, where <code>m</code>, <code>e</code> and <code>s</code> are local to the loop iteration. When we load the value from their <code>push_for_backward</code> handlers, we get the version of value at the exact iteration we need.</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../gpu/" class="btn btn-neutral float-left" title="Running on a GPU"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../../api/" class="btn btn-neutral float-right" title="Python API">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../gpu/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../../api/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme_extra.js" defer></script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../javascripts/mathjax.js" defer></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6" defer></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" defer></script>
      <script src="../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
