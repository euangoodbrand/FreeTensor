{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FreeTensor \u00b6 A language and compiler for irregular tensor programs. GitHub User Guide API Reference Publication License Features by Example \u00b6 Write a simple vector addition with loops that compiles to native code: import freetensor as ft import numpy as np n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) If you are not willing to compile the program once for each different n , you can set n as another function argument (but you may lose some performance). In FreeTensor, all variables are tensors, where scalars are 0-D tensors. import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) assert np.array_equal(y, [3, 5, 7, 9]) If building with CUDA, you can also run the program on a GPU. This time, a \" schedule \" (an explicit program transformation) is needed, and memory types of variables should be properly set. import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize(\"Li\", \"threadIdx.x\")) # Use \"byvalue\" for `n` so it can be used both during kernel launching # and inside a kernel def test(n: ft.Var[(), \"int32\", \"input\", \"byvalue\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li # Name the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Some common tensor operations, including tensor addition (broadcasting is supported), are pre-defined functions in FreeTensor. They are defiend in freetensor.libop , and they can also be invoked using operator overloading. These functions are pure Python functions, which will be inlined into your code, and will enjoy a joint optimization. import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = a + b # Or y = ft.add(a, b) return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) FreeTensor also supports reverse-mode Automatic Differentiation: import freetensor as ft import numpy as np n = 4 def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.zeros((), \"float32\") for i in range(n): y[()] += a[i] * b[i] return y fwd, bwd, input_grads, output_grads = ft.grad(test, ['a', 'b'], [ft.Return()]) fwd = ft.optimize(fwd) bwd = ft.optimize(bwd) a = np.array([0, 1, 2, 3], dtype=\"float32\") b = np.array([3, 2, 1, 0], dtype=\"float32\") y = fwd(a, b) print(y.numpy()) dzdy = np.array(1, dtype='float32') dzda, dzdb = bwd(**{output_grads[ft.Return()]: dzdy})[input_grads['a'], input_grads['b']] print(dzda.numpy()) print(dzdb.numpy())","title":"Home"},{"location":"#features-by-example","text":"Write a simple vector addition with loops that compiles to native code: import freetensor as ft import numpy as np n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) If you are not willing to compile the program once for each different n , you can set n as another function argument (but you may lose some performance). In FreeTensor, all variables are tensors, where scalars are 0-D tensors. import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) assert np.array_equal(y, [3, 5, 7, 9]) If building with CUDA, you can also run the program on a GPU. This time, a \" schedule \" (an explicit program transformation) is needed, and memory types of variables should be properly set. import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize(\"Li\", \"threadIdx.x\")) # Use \"byvalue\" for `n` so it can be used both during kernel launching # and inside a kernel def test(n: ft.Var[(), \"int32\", \"input\", \"byvalue\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li # Name the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Some common tensor operations, including tensor addition (broadcasting is supported), are pre-defined functions in FreeTensor. They are defiend in freetensor.libop , and they can also be invoked using operator overloading. These functions are pure Python functions, which will be inlined into your code, and will enjoy a joint optimization. import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = a + b # Or y = ft.add(a, b) return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) FreeTensor also supports reverse-mode Automatic Differentiation: import freetensor as ft import numpy as np n = 4 def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.zeros((), \"float32\") for i in range(n): y[()] += a[i] * b[i] return y fwd, bwd, input_grads, output_grads = ft.grad(test, ['a', 'b'], [ft.Return()]) fwd = ft.optimize(fwd) bwd = ft.optimize(bwd) a = np.array([0, 1, 2, 3], dtype=\"float32\") b = np.array([3, 2, 1, 0], dtype=\"float32\") y = fwd(a, b) print(y.numpy()) dzdy = np.array(1, dtype='float32') dzda, dzdb = bwd(**{output_grads[ft.Return()]: dzdy})[input_grads['a'], input_grads['b']] print(dzda.numpy()) print(dzdb.numpy())","title":"Features by Example"},{"location":"api/","text":"Python API \u00b6 freetensor.core \u00b6 freetensor.core.autograd \u00b6 freetensor . core . autograd . grad ( func = None , requires = None , provides = None , tapes = GradTapeMode . NoReuseOnly , tape_in_closure = True , invert = True , user_grads = None , attach_backward = False , jit_cache = functools . cache , verbose = None , _override_func_params = None ) \u00b6 Reverse mode automatic differentiation (out-of-place version) It returns a forward function, a backward function, and two maps on names. The forward function computes the original results. The backward function computes the gradients. The maps map from the names of the original arguments and return values, to the names of their gradients. If tape_in_closure is True (by default), the forward function has the same interface of the original function, but it will store some intermediate tensors (the tape) in some hidden global states. The backward functions receives the same inputs as the original function plus the graidents of the outputs, and also reads from the hidden states. The outputs of the original function are no longer exist in the backward function, and the input-outputs of the original function are converted to pure inputs. As grad is an out-of-place AD interface, the backward function returns the resulting gradients as additional return values. Names of the additional arguments and return values can be looked up in the maps returned by grad . If tape_in_closure is False, global states described above will be passed by explicit arguments and return values, so you can store or manipluate these states between the forward run and the backward run. Parameters: func ( AST ) \u2013 The original function requires ( Sequence [ Union [ str , Parameter ]] ) \u2013 Name of input variables that need gradients. A parameter of a function can also be specified with a Parameter object by position provides ( Sequence [ Union [ str , Parameter , Return ]] ) \u2013 Name of output variables whose gradients are known. A mutable parameter of a function can also be specified with a Parameter object by position. A return value of a function can also be specified with a Return object by position tapes ( Union [ Sequence , GradTapeMode ] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure ( bool ) \u2013 True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True invert ( bool ) \u2013 If set to true, it can reduce the amount of recomputation or taping required. However, this may result in a loss of precision for floating-point numbers. Defaults user_grads ( List [ ffi . StmtSetToUserGrad ] ) \u2013 For custom gradient. You do not have to explicitly set this parameter unless you are manipulating func by yourself (not getting it from the Python frontend). See UserGrad for details attach_backward ( bool ) \u2013 If True, the forward function will be the only return value, with backward function and other metadata attached to it jit_cache ( Callable [ Callable , Callable ] ) \u2013 Function decorator used to cache JIT instances verbose ( Optional [ int ] ) \u2013 Verbosity level Returns: Func if attach_backward else Tuple \u2013 If attach_backward is False, return a tuple. Return[0]: forward AST. Return[1]: backward AST. Return[2]: Mapping from names in requries to its gradient name. Return[3]: Mapping from names in provides to its gradient name. If attach_backward is True, only the forward AST is returned, and others can be get by .backward , .input_name_to_gradient_name and .output_name_to_gradient_name on the return value, respectively. freetensor . core . autograd . grad_ ( func = None , requires = None , provides = None , tapes = GradTapeMode . NoReuseOnly , tape_in_closure = True , invert = True , user_grads = None , attach_backward = False , jit_cache = functools . cache , verbose = None , _override_func_params = None ) \u00b6 Reverse mode automatic differentiation (in-place version) It returns a forward function, a backward function, and two maps on names. The forward function computes the original results. The backward function computes the gradients. The maps map from the names of the original arguments and return values, to the names of their gradients. If tape_in_closure is True (by default), the forward function has the same interface of the original function, but it will store some intermediate tensors (the tape) in some hidden global states. The backward functions receives the same inputs as the original function plus the graidents of the outputs, and also reads from the hidden states. The outputs of the original function are no longer exist in the backward function, and the input-outputs of the original function are converted to pure inputs. As grad_ is an in-place AD interface, the backward function passes the resulting gradients by additional mutable arguments. Names of the additional arguments can be looked up in the maps returned by grad_ . If tape_in_closure is False, global states described above will be passed by explicit arguments and return values, so you can store or manipluate these states between the forward run and the backward run. Parameters: func ( AST ) \u2013 The original function requires ( Sequence [ Union [ str , Parameter ]] ) \u2013 Name of input variables that need gradients. A parameter of a function can also be specified with a Parameter object by position provides ( Sequence [ Union [ str , Parameter , Return ]] ) \u2013 Name of output variables whose gradients are known. A mutable parameter of a function can also be specified with a Parameter object by position. A return value of a function can also be specified with a Return object by position tapes ( Union [ Sequence , GradTapeMode ] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure ( bool ) \u2013 True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True invert ( bool ) \u2013 If set to true, it can reduce the amount of recomputation or taping required. However, this may result in a loss of precision for floating-point numbers. Defaults to true. user_grads ( List [ ffi . StmtSetToUserGrad ] ) \u2013 For custom gradient. You do not have to explicitly set this parameter unless you are manipulating func by yourself (not getting it from the Python frontend). See UserGrad for details attach_backward ( bool ) \u2013 If True, the forward function will be the only return value, with backward function and other metadata attached to it jit_cache ( Callable [ Callable , Callable ] ) \u2013 Function decorator used to cache JIT instances verbose ( Optional [ int ] ) \u2013 Verbosity level Returns: Func if attach_backward else Tuple \u2013 If attach_backward is False, return a tuple. Return[0]: forward AST. Return[1]: backward AST. Return[2]: Mapping from names in requries to its gradient name. Return[3]: Mapping from names in provides to its gradient name. If attach_backward is True, only the forward AST is returned, and others can be get by .backward , .input_name_to_gradient_name and .output_name_to_gradient_name on the return value, respectively. freetensor . core . autograd . grad_body ( stmt , requires , provides , tapes = GradTapeMode . NoReuseOnly , invert = True , user_grads = []) \u00b6 grad or grad_ on a function body (for internal tests only) freetensor . core . autograd . jacrev ( func = None , inputs = None , output = None , flatten = False , tapes = GradTapeMode . NoReuseOnly , invert = True , user_grads = None , attach_backward = False , jit_cache = functools . cache , verbose = None , _override_func_params = None ) \u00b6 Compute Jacobian tensors using Reverse mode automatic differentiation (out-of-place) jacrev computes one Jacobian tensor for one output and one or more inputs. Each Jacobian tensor consists of derivatives of all elements in the output tensor w.r.t. all elements in each inputs tensor. It returns a forward function, a backward function, and a map on names. The forward function computes the original results. The backward function computes the Jacobian tensors. The map maps from the names of the original arguments to the names of their Jacobian tensors. The forward function has the same interface of the original function, but it will store some intermediate tensors (the tape) in some hidden global states. The backward functions receives the same inputs as the original function, and also reads from the hidden states. The outputs of the original function are no longer exist in the backward function, and the input-outputs of the original function are converted to pure inputs. As jacrev is an out-of-place interface, the backward function returns the resulting Jacobian as additional return values. Names of the additional return values can be looked up in the map returned by jacrev . Suppose the output's shape is (d1, d2, ...) , and there are two inputs, whose shapes are (e1, e2, ...) and (f1, f2, ...) , respectively. If flatten is False (by default), the Jacobian tensors' shape will be (d1, d2, ..., e1, e2, ...) and (d1, d2, ..., f1, f2, ...) , respectively. If flatten is True, there will be only one Jacbian tensor, whose shape will be (d1 * d2 * ..., e1 * e2 * ... + f1 * f2 * ...) . Parameters: func ( AST ) \u2013 The original function inputs ( Sequence [ Union [ str , Parameter ]] ) \u2013 Name of input variables that the Jacobian tensors are for. output ( Union [ str , Parameter , Return ] ) \u2013 Name of one output variables that the Jacobian tensors are for. A return value of a function can be specified with a Return object flatten ( bool ) \u2013 If True, concatenate all Jacobian tensors together, to form an (n, m) -shaped output, where n is the total number of elements in the specified output, and m is the total number of elements in the specified inputs. This requires all involved inputs having the same data type and memory type. In this case, the name of the Jacobian tensor will be \"jacrev.flatten\" , and the returned name map will be empty tapes ( Union [ Sequence , GradTapeMode ] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history invert ( bool ) \u2013 If set to true, it can reduce the amount of recomputation or taping required. However, this may result in a loss of precision for floating-point numbers. Defaults user_grads ( Optional [ Sequence [ ffi . StmtSetToUserGrad ]] ) \u2013 For custom gradient. You do not have to explicitly set this parameter unless you are manipulating func by yourself (not getting it from the Python frontend). See UserGrad for details attach_backward ( bool ) \u2013 If True, the forward function will be the only return value, with backward function and other metadata attached to it jit_cache ( Callable [ Callable , Callable ] ) \u2013 Function decorator used to cache JIT instances verbose ( Optional [ int ] ) \u2013 Verbosity level Returns: Func if attach_backward else Tuple \u2013 If attach_backward is False, return a tuple. Return[0]: forward AST. Return[1]: backward AST. Return[2]: Mapping from names in inputs to its Jacobian tensor name. If attach_backward is True, only the forward AST is returned, and others can be get by .backward and .input_name_to_gradient_name on the return value, respectively. freetensor . core . autograd . jacrev_ ( func = None , inputs = None , output = None , flatten = False , tapes = GradTapeMode . NoReuseOnly , invert = True , user_grads = None , attach_backward = False , jit_cache = functools . cache , verbose = None , _override_func_params = None ) \u00b6 Compute Jacobian tensors using Reverse mode automatic differentiation (in-place) jacrev computes one Jacobian tensor for one output and one or more inputs. Each Jacobian tensor consists of derivatives of all elements in the output tensor w.r.t. all elements in each inputs tensor. It returns a forward function, a backward function, and a map on names. The forward function computes the original results. The backward function computes the Jacobian tensors. The map maps from the names of the original arguments to the names of their Jacobian tensors. The forward function has the same interface of the original function, but it will store some intermediate tensors (the tape) in some hidden global states. The backward functions receives the same inputs as the original function, and also reads from the hidden states. The outputs of the original function are no longer exist in the backward function, and the input-outputs of the original function are converted to pure inputs. As jacrev_ is an in-place interface, the backward function passes the resulting gradients by additional mutable arguments. Names of the additional arguments can be looked up in the map returned by jacrev_ . Suppose the output's shape is (d1, d2, ...) , and there are two inputs, whose shapes are (e1, e2, ...) and (f1, f2, ...) , respectively. If flatten is False (by default), the Jacobian tensors' shape will be (d1, d2, ..., e1, e2, ...) and (d1, d2, ..., f1, f2, ...) , respectively. If flatten is True, there will be only one Jacbian tensor, whose shape will be (d1 * d2 * ..., e1 * e2 * ... + f1 * f2 * ...) . Parameters: func ( AST ) \u2013 The original function inputs ( Sequence [ Union [ str , Parameter ]] ) \u2013 Name of input variables that the Jacobian tensors are for. A parameter of a function can also be specified with a Parameter object by position output ( Union [ str , Parameter , Return ] ) \u2013 Name of one output variables that the Jacobian tensors are for. A mutable parameter of a function can also be specified with a Parameter object by position. A return value of a function can also be specified with a Return object by position flatten ( bool ) \u2013 If True, concatenate all Jacobian tensors together, to form an (n, m) -shaped output, where n is the total number of elements in the specified output, and m is the total number of elements in the specified inputs. This requires all involved inputs having the same data type and memory type. In this case, the name of the Jacobian tensor will be \"jacrev.flatten\" , and the returned name map will be empty tapes ( Union [ Sequence , GradTapeMode ] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history invert ( bool ) \u2013 If set to true, it can reduce the amount of recomputation or taping required. However, this may result in a loss of precision for floating-point numbers. Defaults user_grads ( Optional [ Sequence [ ffi . StmtSetToUserGrad ]] ) \u2013 For custom gradient. You do not have to explicitly set this parameter unless you are manipulating func by yourself (not getting it from the Python frontend). See UserGrad for details attach_backward ( bool ) \u2013 If True, the forward function will be the only return value, with backward function and other metadata attached to it jit_cache ( Callable [ Callable , Callable ] ) \u2013 Function decorator used to cache JIT instances verbose ( Optional [ int ] ) \u2013 Verbosity level Returns: Func if attach_backward else Tuple \u2013 If attach_backward is False, return a tuple. Return[0]: forward AST. Return[1]: backward AST. Return[2]: Mapping from names in inputs to its Jacobian tensor name. If attach_backward is True, only the forward AST is returned, and others can be get by .backward and .input_name_to_gradient_name on the return value, respectively. freetensor.core.codegen \u00b6 freetensor . core . codegen . codegen ( ast = None , target = None , jit_cache = functools . cache , verbose = False ) \u00b6 Generate native code Parameters: ast ( Func ) \u2013 The AST to be lowered. It must includes function signature to determine parameters and return values. If not specified, a partial function is returned, which can be used as a decorator jit_cache ( Callable [ Callable , Callable ] ) \u2013 Function decorator used to cache JIT instances target ( Target ( Optional ) ) \u2013 The target architecture. If omitted, use the default one in config Returns: NativeCode or JITTemplate \u2013 Return a NativeCode for the generated code if there is no JIT parameters. Return a JITTemplate that generates a NativeCode if there is at least one freetensor.core.config \u00b6 Global configurations freetensor.core.context \u00b6 Facility to pick statements to build an AST Classes and functions in this module are internally used by transformer to construct ASTs. They are also used by some internal tests. API of these classes and functions are subject to changes. End users are encouraged to use transformer , instead of this module. freetensor.core.context.ContextStack \u00b6 freetensor . core . context . ContextStack . get_last_stmt_id () \u00b6 Can be used inside the staged code, to get the ID of the immediately preceding statement freetensor . core . context . ContextStack . push_append_stmt_callback ( callback ) \u00b6 Add a callback to be called with all next statements to be appended. For If statement, it can be called twice, one without \"else\" branch, and then maybe one more with \"else\" branch freetensor.core.context.StmtRange \u00b6 Record a set of statement in a program, can be used for custom gradient Usage: with StmtRange() as rng: # Some statements StmtRange can be used interleaved with AST scopes. In these cases, you can directly call __enter__ and __exit__ . E.g., rng = StmtRange() rng.__enter__() # Some statements with VarDef(...) # Some scopes # Some other statements rng.__exit__(None, None, None) freetensor . core . context . pop_ast ( verbose = False ) \u00b6 Get AST and reset context Internally used by transformer and tests freetensor . core . context . pop_ast_and_user_grads ( verbose = False ) \u00b6 Get AST and reset context. Return an extra list for custom gradients Set UserGrad for details freetensor.core.driver \u00b6 freetensor.core.driver.Device \u00b6 Bases: ffi . Device A computing device can be constructed from 1. (TargetType, DeviceNumber) 2. (TargetType, getDeviceByName): cuda uses best matches criteria. 3. (TargetType, FullName, nth): get nth(from 0) device named Fullname . E.g. Device(TargetType::GPU, 0) means the 0-th GPU (device) Device(TargetType::GPU, \"V100\") means a GPU which best matches \"V100\" Device(TargetType::GPU, \"NVIDIA GeForce RTX 3060 Laptop GPU\", 0) A Device can be used as a \"with\" scope, then all the Array s and Driver s will use it by default. In this style, it also sets the default Target. E.g: with Device(...): ast = lower(ast) # Use the Target of the Device above by default a = Array(...) # Use the Device above by default freetensor.core.driver.Driver \u00b6 Bases: EnableAttachBackward , ffi . Driver freetensor . core . driver . Driver . __call__ ( * args , ** kws ) \u00b6 Set argument, execute the binary code, and collect the returns If there is only one return value, it is returned directly. Otherwise, the return values are packed in a ReturnValuesPack This function will introduce some overhaed handling arguments and return values. For an accurate execution time measurement, plase call self.set_args first, then self.time , and finally self.collect_returns freetensor . core . driver . Driver . __init__ ( func , src , device = None , host_device = None , verbose = None ) \u00b6 Compile a program using a backend compiler and load it into memory This class is for internal use. Please consider using build_binary Parameters: func ( ffi . Func ) \u2013 AST of the function, where the function signature is needed to determine the parameters and return values src ( str ) \u2013 Native code generated from codegen device ( Device ( Optional ) ) \u2013 The device to run the program. If omitted, use the default device in config verbose ( bool ( Optional ) ) \u2013 True to print extra infomation freetensor . core . driver . Driver . collect_returns ( always_return_pack = False ) \u00b6 Collect return values from an invocation Return values must be collect. Otherwise there will be memory leaks If there is only one return value, it is returned directly. Otherwise, or if always_return_pack is set, the return values are packed in a ReturnValuesPack freetensor . core . driver . Driver . native_code () \u00b6 Get native code compiled by backend compiler freetensor . core . driver . Driver . set_args ( * args , ** kws ) \u00b6 Set argument for an invocation freetensor.core.driver.ReturnValuesPack \u00b6 Hold return values from a Driver invocation Return values can be retrieved in an anonymous manner: x, y, z = pack , or in a named manner: pack['x'] Please note that a ReturnValuesPack is different from a OrderedDict, as OrderedDict unpacks to keys rather than values freetensor . core . driver . ReturnValuesPack . __contains__ ( key ) \u00b6 Test if a return value exists freetensor . core . driver . ReturnValuesPack . __getitem__ ( key ) \u00b6 Get a return value with a name. Tuple is supported for multiple values freetensor . core . driver . ReturnValuesPack . __iter__ () \u00b6 Get all return values in the order declared in Func freetensor . core . driver . array ( data , dtype = None , dont_drop_borrow = False , moved = False ) \u00b6 Factory function for Array This function is preferred over directly calling Array 's constructor, because it accepts more data format. If data is another FreeTensor Array , the original object will be returned, with dont_drop_borrow and moved set to new values. If dtype is set and different from the original data type, the Array will be copied first to convert the data type. If data is Numpy Array or PyTorch Tensor , it will be converted to FreeTensor Array . Memory copy will be avoided in most cases, but it is inevitable if the data is strided. If dtype is set and different from the original data type, the Array or Tensor will be copied first to convert the data type. Otherwise, the data will be treated as an n-dimensional array-like object, and will be parsed according the rules in NumPy. The data type is also set accordingly, unless dtype is set. Parameters: data ( FreeTensor Array, Numpy Array, PyTorch Tensor, or other array-like objects ) \u2013 Data to be copied to or borrowed by the new Array object dtype ( ft.DataType or str ) \u2013 If data is not in dtype , convert it to dtype first before constructing the Array dont_drop_borrow ( bool ) \u2013 If true, report an error if we have to drop a borrwed data. This flag is set to true when the Array is cunstructed IMPLICITLY (not by this function) from a user object by borrowing from it, where users may expect they are acutually manipulating the their user object, instead of this Array moved ( bool ) \u2013 If true, it means we do not care about data in this Array any more after the program runs. Variables with \"input-mutable\" access type may modify the Array freetensor . core . driver . build_binary ( code = None , device = None , host_device = None , jit_cache = functools . cache , verbose = False ) \u00b6 Compile a program using a backend compiler and load it into memory Parameters: code ( NativeCode ) \u2013 Native code generated by codegen . If not specified, a partial function is returned, which can be used as a decorator device ( Device ( Optional ) ) \u2013 The device to run the program. If omitted, use the default device in config jit_cache ( Callable [ Callable , Callable ] ) \u2013 Function decorator used to cache JIT instances verbose ( int ) \u2013 Verbosity level Returns: Driver or JITTemplate \u2013 Return a Driver for the executable if there is no JIT parameters. Return a JITTemplate that generates a Driver if there is at least one freetensor . core . driver . move ( data ) \u00b6 Alias for array(data, dont_drop_borrow=False, moved=True) freetensor.core.enable_attach_backward \u00b6 freetensor.core.enable_attach_backward.EnableAttachBackward \u00b6 Get backward object (Func, Driver, etc) and other meta data from a forward object This class is a Mixin Class. It should be inherited BEFORE other base classes in multiple inheritance. freetensor . core . enable_attach_backward . EnableAttachBackward . __init__ ( * args , ** kvs ) \u00b6 Forward all arguments to other base classes In Python, super(). init calls the next base class in the full inheritance graph of the final class, not only base classes of BackwardAttachedMixin. See https://docs.python.org/3/tutorial/classes.html#multiple-inheritance freetensor.core.expr \u00b6 Facility to build AST expressions Classes and functions in this module are not only used internally for constructing AST nodes, and also exposed to users via multi-stage programming freetensor.core.expr.AlreadyMadeReduceTo \u00b6 A single-value type that marks a ReduceTo node is already made, and there is no need to make another Store node In standard Python data model, functions like iadd returns the modified self, and setitem does a self-assignment. We do the augmenting assignment directly in iadd and return AlreadyMadeReduceTo, so we do not have to Store it again freetensor.core.expr.UndeclaredParam dataclass \u00b6 Error type. For error reporting only. freetensor.core.expr.VarRef \u00b6 Bases: ffi . FrontendVar Variable of FreeTensor All variables in FreeTensor DSL (declared via Var , created by empty or var , returned by libop , etc.), and their slices, are VarRef objects. Operations on VarRef objects generates AST nodes freetensor . core . expr . VarRef . shape ( dim = None ) \u00b6 Return lengths of all dimensions or the length of one dimension .shape() -> list of lengths of all dimensions .shape(dim) -> length of dimension dim , where dim can be int or Expr All lengths can be Expr (if the length is dynamically decided) or int (if statically decided) freetensor.core.expr.VarRefFromVarDef \u00b6 Bases: VarRef VarRef with extra checks freetensor.core.expr.VarVersionRef \u00b6 Bases: VarRef Special VarRef used for custom gradient, generated from mark_version freetensor . core . expr . abs ( expr ) \u00b6 Absolute value For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.abs Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The absolute value freetensor . core . expr . add ( lhs , rhs ) \u00b6 lhs + rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.add Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The sum freetensor . core . expr . any () \u00b6 Create an AnyExpr node (only for testing and type inference) Any nodes matches any expression nodes in ast.match freetensor . core . expr . cast ( expr , dtype ) \u00b6 Cast to another type Parameters: expr ( VarRef or Number ) \u2013 The operand dtype ( DataTypr or str ) \u2013 The target data type Returns: VarRef or Number \u2013 The result freetensor . core . expr . ceil ( expr ) \u00b6 Round a float up to an interger (towards +inf) For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ceil Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result freetensor . core . expr . ceildiv ( lhs , rhs ) \u00b6 Ceiling integer division of lhs dividing by rhs The result rounds towards positive infinity For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ceildiv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient freetensor . core . expr . dtype ( var ) \u00b6 Get element data type of a variable freetensor . core . expr . eq ( lhs , rhs ) \u00b6 lhs == rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.eq Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison freetensor . core . expr . exp ( expr ) \u00b6 Natural exponent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.exp Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The exponent freetensor . core . expr . floor ( expr ) \u00b6 Round a float down to an interger (towards -inf) For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.floor Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result freetensor . core . expr . floordiv ( lhs , rhs ) \u00b6 Floored integer division of lhs dividing by rhs The result rounds towards negative infinity (following Python convention, instead of C) This function is recommended over round_towards_0_div , as it enjoys more optimizations For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.floordiv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient freetensor . core . expr . ge ( lhs , rhs ) \u00b6 lhs >= rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ge Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison freetensor . core . expr . gt ( lhs , rhs ) \u00b6 lhs > rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.gt Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison freetensor . core . expr . if_then_else ( cond , then_case , else_case ) \u00b6 Similar to then_case if cond else else_case NOTE: there is NO guarantee that only one branch will be executed. In some cases, both branches will be executed and the result of one of them will be picked. Therefore, please do NOT use if_then_else to guard an out-of-bound array indexing Parameters: cond ( VarRef of Number ) \u2013 Condition then_case ( VarRef or Number ) \u2013 Then-case experssion else_case ( VarRef or Number ) \u2013 Else-case expression Returns: VarRef or Number \u2013 The result freetensor . core . expr . intrinsic ( fmt , * params , ret_type = 'void' , has_side_effect = False ) \u00b6 Invoke whatever target code Parameters: fmt ( str ) \u2013 What to run. \"%\" is filled by parameters one by one. E.g. sinf(%) *params ( Sequence [ Expr ] ) \u2013 (Positional variadic) Parameters to fmt ret_type ( DataType or str ) \u2013 (Keyword argument only) The return type. Void for no return type. Defaults to Void has_side_effect ( bool ) \u2013 (Keyword argument only) True to indicate the intrinsic modifes something other than the return value. Defaults to false freetensor . core . expr . l_and ( lhs , rhs ) \u00b6 Logical and of lhs and rhs NOTE: Short-circuit evaluation is NOT supported For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_and Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The logical and freetensor . core . expr . l_not ( expr ) \u00b6 Logical not For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_not Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The logical not freetensor . core . expr . l_or ( lhs , rhs ) \u00b6 Logical or of lhs and rhs NOTE: Short-circuit evaluation is NOT supported For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_or Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The logical or freetensor . core . expr . le ( lhs , rhs ) \u00b6 lhs <= rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.le Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison freetensor . core . expr . ln ( expr ) \u00b6 Natural logarithm For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ln Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The exponent freetensor . core . expr . load_at_version ( tape_name , dtype , * indices ) \u00b6 Create an LoadAtVersion node (only for custom gradient) This node is only used for custom gradient. See UserGradForPrevStmt . freetensor . core . expr . lt ( lhs , rhs ) \u00b6 lhs < rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.lt Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison freetensor . core . expr . max ( lhs , rhs ) \u00b6 Maximum of lhs and rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.max Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The maximum freetensor . core . expr . min ( lhs , rhs ) \u00b6 Minimum of lhs and rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.min Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The minimum freetensor . core . expr . mod ( lhs , rhs ) \u00b6 lhs modulus rhs The result is always non-negative (following Python convention, instead of C). This function is recommended over remainder , as it enjoys more optimizations For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.mod Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The modulo freetensor . core . expr . mtype ( var ) \u00b6 Get memory type of a variable freetensor . core . expr . mul ( lhs , rhs ) \u00b6 lhs * rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.mul Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The product freetensor . core . expr . ndim ( var ) \u00b6 Get the number of dimensions of a variable freetensor . core . expr . ne ( lhs , rhs ) \u00b6 lhs != rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ne Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison freetensor . core . expr . remainder ( lhs , rhs ) \u00b6 Remainder of lhs dividing rhs The result can be positive or negative (following C convention, instead of Python). End users are encouraged to use lhs % rhs instead, which follows Python convetion, and enjoys better optimization in FreeTensor For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.remainder Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The remainder freetensor . core . expr . round_towards_0_div ( lhs , rhs ) \u00b6 C-style integer division of lhs dividing by rhs The result rounds towards 0 (following C convention, instead of Python) End users are encouraged to use lhs // rhs instead, which follows Python convetion, and enjoys better optimization in FreeTensor For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.round_towards_0_div Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient freetensor . core . expr . shape ( var , i = None ) \u00b6 shape(var, i): Get size of specified dimension of a variable shape(var): Get sizes of all dimensions of a variable freetensor . core . expr . sigmoid ( expr ) \u00b6 Sigmoid For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sigmoid Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result freetensor . core . expr . sqrt ( expr ) \u00b6 Square root For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sqrt Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The square root freetensor . core . expr . square ( expr ) \u00b6 Square For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.square Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The square freetensor . core . expr . sub ( lhs , rhs ) \u00b6 lhs - rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sub Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The difference freetensor . core . expr . tanh ( expr ) \u00b6 Hyperbolic tangent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.tanh Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result freetensor . core . expr . truediv ( lhs , rhs ) \u00b6 Floating point division of lhs dividing by rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.truediv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient freetensor.core.frontend \u00b6 A frontend transforming user Python functions to ASTs via staging. freetensor.core.frontend.FreeTensorOverload \u00b6 Bases: StagingOverload Helper class managing context in IR staging. freetensor . core . frontend . FreeTensorOverload . fullname ( name ) \u00b6 Get distinct name. freetensor.core.frontend.LifetimeScope \u00b6 This scope is used to register multiple scopes inside a single lifetime scope. The inner scopes might be used to register variables, etc. They will be exited in reverse order of their registration. freetensor.core.frontend.UserGrad \u00b6 Bases: UserGradStaged Define a custom gradient Follow the following steps to define custom gradient: Add some mark_version statements in the program. mark_version('y0', y) marks the specific versions of variable y at the program position of the statement and at all iterations as 'y0' . Add a UserGrad scope. 2.1. UserGrad optionally receives parameter stmt_range , recorded by the StmtRange helper class, which means the gradient is for the code specified in the range. Ignoring the parameter means setting gradient for the previous statement of the scope. 2.2. Other parameters of UserGrad sets the mapping from original variables to gradient variables. with UserGradForPrevStmt(x, y) as (dx, dy) provides VarRef dx and dy as gradient variables to be used inside the scope. In order to use the value from the forward pass in the backward pass, do not access the forward variables directly in the scope. Instead, use load_at_version expressions. load_at_version(y0, i, j) loads from y[i, j] at the specific version marked by y0 = mark_version(y) , saved from the same iteration in the forward pass . (If directly writing staged code, it is MarkVersion('y0', y) ). In other words, after AD, the position of mark_version and the dynamic loop iterator together makes up the actual version number for the tape. Build the AST with pop_ast_and_user_grads instead of pop_ast . An extra list will be returned together with the AST, which you need to pass as grad 's user_grads argument. This list records the forward-to-backward relation of the nodes. If you are directly writing staged code, use UserGradStaged instead. Parameters: *args ( Sequence [ VarRef ] ) \u2013 (Positional variadic) Mapping from original variables to gradient variables stmt_range ( StmtRange ) \u2013 The range in the original program that we are setting custom gradient for freetensor.core.frontend.Var \u00b6 Bases: StagedTypeAnnotation freetensor . core . frontend . Var . __init__ ( shape , dtype , atype = 'input' , mtype = None ) \u00b6 Declare a variable Parameters: shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable atype ( str or AccessType ) \u2013 Access type of the variable. It specifies whether (and how) the variable is an I/O variable of the function it belongs to. Defaults to \"input\" mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used freetensor.core.frontend.VarCreator dataclass \u00b6 Bases: StagedAssignable freetensor . core . frontend . VarCreator . assign ( name ) \u00b6 Customized assign behavior. Creates a VarDef with its full name. freetensor.core.frontend.VersionMarker dataclass \u00b6 Bases: StagedAssignable freetensor . core . frontend . VersionMarker . assign ( tape_name ) \u00b6 Customized assign behavior. Creates a MarkVersion with its full name. freetensor.core.frontend.dynamic_range \u00b6 Bases: StagedIterable Dynamic range that generates For loop in IR tree. freetensor . core . frontend . dynamic_range . __init__ ( start , stop = None , step = 1 ) \u00b6 Initialize a dynamic range. Arguments semantic identical to builtin range . freetensor . core . frontend . dynamic_range . foreach ( name , body ) \u00b6 Customized foreach behavior. Creates a For loop. freetensor . core . frontend . push_for_backward ( var ) \u00b6 Push the current value from the forward pass to be used at the backward pass This function is for custom gradients. See UserGrad for details on how to provide custom gradients. You may imagine there is a virtual stack for each variable. Each time you call x_handle = push_for_backward(x) in the forward pass, the value of x at the current iteration will be \"pushed\" to the virtual stack. You can access x_handle at the backward pass. Each time you access x_handle , you will \"pop\" the stack and get the value of x pushed at the same iteration . Since the \"stack\" is virtual, you do NOT need to \"pop\" the same count as \"push\"es: the version numbering is fully automatic. Besides, there may not be a real stack at runtime: it can be compiled to any data structure. This function will be staged to mark_version statement in the IR. freetensor.core.func \u00b6 freetensor.core.func.Func \u00b6 Bases: EnableAttachBackward , ffi . Func freetensor . core . func . Func . __call__ ( * args , ** kvs ) \u00b6 Enable invoking a transformed AST in another function being transformed, via inlined_invoke freetensor.core.jit \u00b6 freetensor.core.jit.JIT \u00b6 Declare a function parameter as a JIT parameter A function with one or more JIT parameters will be compiled to a JIT template. It can be instantiate after the JIT paraemters are provided Usage: x: JIT or x: JIT[AnyPythonType] . The latter form has no syntactic meanings, and is only for documentation. NOTE 1: The JIT type annotation can only be used for parameter of the outer-most function intended for @transform (or @optimize , etc). It can NOT be used for inner functions intended for @inline . NOTE 2: The JIT type annoation can only annotated on parameters inside the function signature. It is NOT supported in annotations for statements. freetensor.core.jit.JITTemplate \u00b6 Bases: abc . ABC A template that can be instantiated given concrete arguments By calling instantiate with actual arguments you are expecting to run a JIT function with, an instantiated object will be returned. Subclasses of JITTemplate should override instantiate_by_only_jit_args , and define what is actually returned. Parameters: params ( OrderedDict ) \u2013 Parameter list from inspect.signature(func).parameters jit_param_names ( Sequence ) \u2013 Sequence of names of JIT parameters in the original order defined in the function freetensor . core . jit . JITTemplate . instantiate ( * args , ** kvs ) \u00b6 Get an instance with the arguments you are expecting to run a JIT function with freetensor . core . jit . JITTemplate . instantiate_and_call ( * args , ** kvs ) \u00b6 Get an instance and call it with the arguments you are expecting to run a JIT function with freetensor . core . jit . JITTemplate . instantiate_by_only_jit_args ( * jit_args ) abstractmethod \u00b6 Get an instance with only JIT arguments This function accpets a tuple of arguments. Keyword arguments is NOT supported, so memoization can be easier, with considering the order of the arguments. freetensor . core . jit . JITTemplate . separate_args ( * args , ** kvs ) \u00b6 Return a list of non-JIT args, and a list of JIT args freetensor.core.optimize \u00b6 freetensor . core . optimize . optimize ( func = None , schedule_callback = None , backward_schedule_callback = None , target = None , device = None , default_dynamic_range = True , jit_cache = functools . cache , verbose = 0 ) \u00b6 An one-click optimization from Python function to binary executable Usage: @optimize def f(...): ... It is equivalent to: @build_binary @codegen @lower @transform def f(...): ... Parameters: func ( Python function or AST ) \u2013 The user function to optimize. If not specified, a partial function will be returend, which can be used as a decorator schedule_callback ( Callable ( Optional ) ) \u2013 Schedule(s) to apply backward_callback ( Callable ) \u2013 Specify what schedule(s) to do for the backward function, if ast is returned from AD with attach_backward=True . Defaults to be the same with callback target ( Target ( Optional ) ) \u2013 The target architecture. You don't have to set target if you set device device ( Device ( Optional ) ) \u2013 Where to run the program default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( int ( Optional ) ) \u2013 Verbosity level. Can be 0, 1 or 2 freetensor . core . optimize . optimize_to_pytorch ( func = None , tapes = GradTapeMode . NoReuseOnly , forward_schedule_callback = None , backward_schedule_callback = None , target = None , device = None , default_dynamic_range = True , verbose = 0 ) \u00b6 Compile a FreeTensor function to a PyTorch call, whose gradient can be recognized by PyTorch The compiled function will be a typical PyTorch's \"function\" (rather than a PyTorch's \"module\"). Technically, this means it is a wrapper function around a PyTorch's Function 's apply method Schedules (if any) must be applied to the forward function and the backward function separated. For this reason, currently only first-order gradient is supported Parameters: func ( Python function or AST ) \u2013 The user function to optimize. If not specified, a partial function will be returend, which can be used as a decorator tapes ( Union [ Sequence , GradTapeMode ] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history forward_schedule_callback ( Callable ( Optional ) ) \u2013 Schedule(s) to apply to the forward function backward_schedule_callback ( Callable ( Optional ) ) \u2013 Schedule(s) to apply to the backward function target ( Target ( Optional ) ) \u2013 The target architecture. You don't have to set target if you set device device ( Device ( Optional ) ) \u2013 Where to run the program default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( int ( Optional ) ) \u2013 Verbosity level. Can be 0, 1 or 2 freetensor.core.param_ret_dict \u00b6 freetensor.core.param_ret_dict.ParamRetDict \u00b6 Look an object using either a function parameter or return value's name or position freetensor . core . param_ret_dict . ParamRetDict . __init__ ( d , * , func = None , func_name = None , param_names = None , return_names = None ) \u00b6 Either func or ( func_name and param_names and return_names ) should be provided freetensor.core.param_ret_dict.Parameter \u00b6 Alias of a parameter of a function by position instead of by name Parameter(n) represents the n-th parameter (counted from 0) Parameter() can be used if there is only one parameter freetensor.core.param_ret_dict.Return \u00b6 Alias of a return value of a function by position instead of by name Return(n) represents the n-th return value (counted from 0) Return() can be used if there is only one return value freetensor.core.passes \u00b6 freetensor . core . passes . lower ( ast = None , target = None , skip_passes = [], jit_cache = functools . cache , verbose = 0 ) \u00b6 Lower an AST using a series of passes Parameters: ast ( AST ) \u2013 The AST to be lowered. Can be a Func or a Stmt . If not specified, a partial function of lower will be returned, which can be used as a decorator target ( Target ( Optional ) ) \u2013 Lower the AST to a target with target-specific passes, then the AST can be used for codegen. If not set, use the default Target in Config skip_passes ( Sequence [ str ]( Optional ) ) \u2013 Skip some pass for testing or debugging. Names in skip_passes are in underscore_style, as in Python. Please note that some passes will not be skipped even specified in these parameter, because they are indirectly called in some other passes jit_cache ( Callable [ Callable , Callable ] ) \u2013 Function decorator used to cache JIT instances verbose ( int ( Optional ) ) \u2013 0 = print nothing. 1 = print the lowered AST. 2 = print AST after every single passes Returns: Func or JITTemplate \u2013 Return a Func for an AST if there is no JIT parameters. Return a JITTemplate that generates a Func if there is at least one freetensor.core.staging \u00b6 A staging framework to support the FreeTensor frontend. freetensor.core.staging.AllowShortcutScope dataclass \u00b6 Allow return scope. This is a context manager that allows return in statically deterministic control flow. freetensor.core.staging.BreakException \u00b6 Bases: Exception Exception to be raised by StagingOverload.break_stmt. Breaks from a for loop. freetensor.core.staging.ContinueException \u00b6 Bases: Exception Exception to be raised by StagingOverload.continue_stmt. Continues a for loop. freetensor.core.staging.ReturnException \u00b6 Bases: Exception Exception to be raised by StagingOverload.return_stmt. Holds a return value that will be passed through to the function wrapper. freetensor.core.staging.StagingError \u00b6 Bases: Exception Error occurred during staging function execution (i.e. IR tree generation). freetensor.core.staging.StagingOverload \u00b6 freetensor . core . staging . StagingOverload . allow_shortcut_scope ( allow ) \u00b6 Opens a scope that allows shortcut control flows in a statically deterministic context. Need to be closed by with statement. freetensor . core . staging . StagingOverload . assert_stmt ( test ) \u00b6 Assert staging tool. freetensor . core . staging . StagingOverload . assign_stmt ( name , value ) \u00b6 Customized assign wrapper. If value is instance of StagedAssignable , it's regarded as a customized assign behavior and gets executed with the assigned target variable name. This wrapper is used for initializing a variable. freetensor . core . staging . StagingOverload . at_position ( filename , lineno ) \u00b6 Code position handler. Defaults to a no-op. Can be overridden by subclasses. Parameters: filename ( str ) \u2013 Name of the file containing code for the next statement. lineno ( int ) \u2013 Line number of the next statement. freetensor . core . staging . StagingOverload . break_stmt () \u00b6 Break staging tool. Only allow break in static control flow. freetensor . core . staging . StagingOverload . continue_stmt () \u00b6 Continue staging tool. Only allow continue in static control flow. freetensor . core . staging . StagingOverload . custom_attr ( obj , attr ) \u00b6 Customized attribute accessor. The framework first looks for a Python native attribute. If not found, it looks for this overloaded custom attribute resolver. The default implementation provides no custom attribute. Can be overridden by subclasses. Parameters: obj ( Any ) \u2013 Object to access attribute. attr ( str ) \u2013 Attribute name. Returns: Any ( Any ) \u2013 The attribute value. Throws \u00b6 AttributeError : If the attribute is not found. freetensor . core . staging . StagingOverload . foreach ( names , iter , body ) \u00b6 Customized foreach wrapper. If value is instance of StagedIterable , its regarded as a customized foreach behavior and used to generate code for the python for loop. Otherwise, we try to execute the loop as usual. freetensor . core . staging . StagingOverload . functiondef_wrapper ( filename , func ) \u00b6 Function definition wrapper. This wrapper performs extra initialization and cleanup for function definition. freetensor . core . staging . StagingOverload . if_then_else_expr ( predicate , then_expr , else_expr ) \u00b6 If-then-else expression staging tool. freetensor . core . staging . StagingOverload . if_then_else_stmt ( predicate , then_body , else_body = None ) \u00b6 If-then-else statement staging tool. When predicate is deterministic in staging, only one branch is generated. Otherwise, a If node in IR is generated. freetensor . core . staging . StagingOverload . load_attr ( obj , attr ) \u00b6 Load attribute staging tool. Allows customization of reading attributes. freetensor . core . staging . StagingOverload . metadata ( content ) \u00b6 Metadata handler. A metadata line is a comment starting with #! and followed by a metadata, represented as a string parameter. Defaults to a no-op. Can be overridden by subclasses. Parameters: content ( str ) \u2013 The metadata content. freetensor . core . staging . StagingOverload . return_stmt ( value , funcname ) \u00b6 Return staging tool. Only allow return in static control flow. freetensor . core . staging . StagingOverload . unpack_assign_stmt ( names , values ) \u00b6 Customized assign wrapper for one or more targets. If values is instance of StagedUnpackAssignable , it's regarded as a customized assign behavior and gets executed with all the assigned targets' names. Otherwise, it calls assign_stmt with each sub-assignments. Please note that names can be nested tuples like (\"a\", (\"b\", \"c\")) . Please also note that names can also be a single string like \"a\" even if values is a tuple. There is no unpacking in this case freetensor . core . staging . StagingOverload . while_stmt ( fpred , body ) \u00b6 While statement staging tool. freetensor.core.staging.TransformError \u00b6 Bases: Exception Error occurred during AST transforming from python function to staging function that generates IR tree. freetensor.core.staging.Transformer dataclass \u00b6 Bases: ast . NodeTransformer freetensor . core . staging . Transformer . visit_AnnAssign ( old_node ) \u00b6 Rule: x: Ty -> freetensor__annotate__x = annotate_stmt('x', Ty) if freetensor__annotate__x: x = freetensor__annotate__x : pure annotation freetensor . core . staging . Transformer . visit_Assign ( old_node ) \u00b6 Rule: lhs = rhs -> lhs = unpack_assign_stmt('lhs', rhs) x.lhs = rhs -> x.lhs = unpack_assign_stmt('lhs', rhs) a, (b, c) = (x, (y, z)) -> a, (b, c) = unpack_assign_stmt(('a', ('b', 'c')), (x, (y, z))) a = b = c -> a = unpack_assign_stmt('a', c); b = unpack_assign_stmt('b', c) If unpack_assign_stmt is not overloaded, assign_stmt will be called for each item freetensor . core . staging . Transformer . visit_Compare ( old_node ) \u00b6 Expand multiple comparison into and expression. freetensor . core . staging . Transformer . visit_For ( old_node ) \u00b6 Rule: for x in iter: body -> def for_body(x): body foreach('x', iter, for_body) freetensor . core . staging . Transformer . visit_If ( old_node ) \u00b6 Rule: if pred: body else: orelse -> def then_body(): body def else_body(): orelse if_then_else_stmt(pred, then_body, else_body) freetensor . core . staging . Transformer . visit_IfExp ( old_node ) \u00b6 Rule: body if test else orelse -> if_then_else_expr(test, body, orelse) freetensor . core . staging . Transformer . visit_While ( old_node ) \u00b6 Rule: while pred: body -> def while_body(): body while_stmt(lambda: pred, while_body) freetensor . core . staging . call_helper ( callee , * args , ** kwargs ) \u00b6 Call helper that generates a python AST Call node with given callee (overload member) and arguments AST node. freetensor . core . staging . function_helper ( name , args , body , nonlocals ) \u00b6 Function helper that generates a python AST FunctionDef node with given name, arguments name, and body. freetensor.core.stmt \u00b6 Facility to build AST statements Classes and functions in this module are internally used by transformer to construct ASTs. They are also used by some internal tests. API of these classes and functions are subject to changes. End users are encouraged to use transformer , instead of this module. Classes and functions in this module are all in BigCamel naming style, to distinguish from expressions in expr.py freetensor.core.stmt.Assert \u00b6 Scope used to create an Assert node This scope is internally used by transformer and tests E.g.: with Assert(i > 0): ... # Assertion body freetensor.core.stmt.Else \u00b6 Scope used to create an else branch of an If node This scope is internally used by transformer and tests E.g.: with If(i > 0): ... # True branch with Else(): ... # Else branch freetensor.core.stmt.For \u00b6 Scope used to create a For node This scope is internally used by transformer and tests E.g.: with For('i', 0, n) as i: ... # Loop body freetensor.core.stmt.If \u00b6 Scope used to create an If node This scope is internally used by transformer and tests E.g.: with If(i > 0): ... # Branch body freetensor.core.stmt.Invoke \u00b6 Inlined invocation of another AST Invoke is used as a scope ( with Invoke(...) as returned_vars ), so that variables returned by the callee can be used in the socpe Invoke can be used for invoking a gradient function, which has already been lowered as an AST. Please note that once a user function has been lowered as an AST, the dimensionalities of its tensors get fixed. Therefore, to invoke ordinary user functions, please use inline in transformer instead, which supports generic types freetensor.core.stmt.NamedScope \u00b6 Scope used to create an StmtSeq node with an explicit labels E.g.: with NamedScope(): ... # body This scope is used for testing only. StmtSeq nodes can be deleted in many lowering passes freetensor.core.stmt.UserGradStaged \u00b6 Internal staged implementation of UserGrad freetensor . core . stmt . Any () \u00b6 Create an Any node (only for testing) Any nodes matches any statement nodes in ast.match freetensor . core . stmt . Eval ( expr ) \u00b6 Create an Eval node This scope is internally used by transformer and tests freetensor . core . stmt . MarkLabel ( label ) \u00b6 Mark the ID of the following statement This scope is internally used by transformer and tests freetensor . core . stmt . MarkVersion ( tape_name , var ) \u00b6 Create an MarkVersion node (only for custom gradient) This node is only used for custom gradient. See UserGrad . freetensor . core . stmt . VarDef ( * args , ** kvs ) \u00b6 A factory function that creates a VarDef or a series of nested VarDef s This scope is internally used by transformer and tests freetensor.core.utils \u00b6 freetensor . core . utils . as_decorator ( f ) \u00b6 Enable a multi-parameter function f to be used as a decorator Suppose g = as_decorator(f) , enable the following usages: @g def h(...): ... @g(a=a, b=b, c=c) def h(...): ... Formally, g will have the same parameters as f . f 's first parameter should be the function it decorate, say h , and may have other parameters with default values. If h is set when called, g will return the decorated function, just as f does. If h is not set, g will return an f 's partial function with all other parameters set, and the partial function can then be decorate another h again. freetensor.libop \u00b6 freetensor.libop.constant \u00b6 freetensor . libop . constant . zeros ( shape , dtype , mtype = None ) \u00b6 Create a zero tensor Parameters: shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used Returns: VarRef \u2013 The zero tensor freetensor . libop . constant . zeros_ ( y ) \u00b6 Fill zeros to a tensor Parameters: y ( VarRef ) \u2013 The tensor to fill freetensor.libop.element_wise \u00b6 freetensor . libop . element_wise . binary_op ( op , a , b ) \u00b6 (Broadcasted) any element-wise operation on two tensors and return the result Parameters: op ( Callable ) \u2013 The operation applied to each item a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . binary_op_ ( op , a , b , out ) \u00b6 (Broadcasted) any element-wise operation on two tensors. The result is written to another tensor Parameters: op ( Callable ) \u2013 The operation applied to each item a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . unary_op ( op , x ) \u00b6 Any element-wise operation on a tensor and return the result Parameters: op ( Callable ) \u2013 The operation applied to each item x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . unary_op_ ( op , x , y ) \u00b6 Any element-wise operation on a tensor. The result is written to another tensor Parameters: op ( Callable ) \u2013 The operation applied to each item x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor freetensor.libop.pooling \u00b6 freetensor . libop . pooling . global_avg_pool ( X ) \u00b6 Global averaging pooling. The result is returned Parameters follow ONNX convention. Currently only 2-D pooling is supported freetensor . libop . pooling . global_avg_pool_ ( X , Y ) \u00b6 Global averaging pooling. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D pooling is supported freetensor . libop . pooling . max_pool ( X , auto_pad = 'NOTSET' , dilations = None , kernel_shape = None , pads = None , strides = None ) \u00b6 Maximum pooling. The result is returned Parameters follow ONNX convention. Currently only 2-D pooling is supported freetensor . libop . pooling . max_pool_ ( X , Y , auto_pad = 'NOTSET' , dilations = None , kernel_shape = None , pads = None , strides = None ) \u00b6 Maximum pooling. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D pooling is supported freetensor.libop.reduction \u00b6 freetensor . libop . reduction . reduce_max ( x , axes , keepdims = True ) \u00b6 Maximum of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence [ int ]( Optional ) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ( Optional ) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor freetensor . libop . reduction . reduce_max_ ( x , y , axes , keepdims = True ) \u00b6 Maximum of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence [ int ]( Optional ) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ( Optional ) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True freetensor . libop . reduction . reduce_min ( x , axes , keepdims = True ) \u00b6 Minimum of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence [ int ]( Optional ) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ( Optional ) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor freetensor . libop . reduction . reduce_min_ ( x , y , axes , keepdims = True ) \u00b6 Minimum of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence [ int ]( Optional ) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ( Optional ) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True","title":"Python API"},{"location":"api/#freetensor.core","text":"","title":"core"},{"location":"api/#freetensor.core.autograd","text":"","title":"autograd"},{"location":"api/#freetensor.core.autograd.grad","text":"Reverse mode automatic differentiation (out-of-place version) It returns a forward function, a backward function, and two maps on names. The forward function computes the original results. The backward function computes the gradients. The maps map from the names of the original arguments and return values, to the names of their gradients. If tape_in_closure is True (by default), the forward function has the same interface of the original function, but it will store some intermediate tensors (the tape) in some hidden global states. The backward functions receives the same inputs as the original function plus the graidents of the outputs, and also reads from the hidden states. The outputs of the original function are no longer exist in the backward function, and the input-outputs of the original function are converted to pure inputs. As grad is an out-of-place AD interface, the backward function returns the resulting gradients as additional return values. Names of the additional arguments and return values can be looked up in the maps returned by grad . If tape_in_closure is False, global states described above will be passed by explicit arguments and return values, so you can store or manipluate these states between the forward run and the backward run. Parameters: func ( AST ) \u2013 The original function requires ( Sequence [ Union [ str , Parameter ]] ) \u2013 Name of input variables that need gradients. A parameter of a function can also be specified with a Parameter object by position provides ( Sequence [ Union [ str , Parameter , Return ]] ) \u2013 Name of output variables whose gradients are known. A mutable parameter of a function can also be specified with a Parameter object by position. A return value of a function can also be specified with a Return object by position tapes ( Union [ Sequence , GradTapeMode ] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure ( bool ) \u2013 True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True invert ( bool ) \u2013 If set to true, it can reduce the amount of recomputation or taping required. However, this may result in a loss of precision for floating-point numbers. Defaults user_grads ( List [ ffi . StmtSetToUserGrad ] ) \u2013 For custom gradient. You do not have to explicitly set this parameter unless you are manipulating func by yourself (not getting it from the Python frontend). See UserGrad for details attach_backward ( bool ) \u2013 If True, the forward function will be the only return value, with backward function and other metadata attached to it jit_cache ( Callable [ Callable , Callable ] ) \u2013 Function decorator used to cache JIT instances verbose ( Optional [ int ] ) \u2013 Verbosity level Returns: Func if attach_backward else Tuple \u2013 If attach_backward is False, return a tuple. Return[0]: forward AST. Return[1]: backward AST. Return[2]: Mapping from names in requries to its gradient name. Return[3]: Mapping from names in provides to its gradient name. If attach_backward is True, only the forward AST is returned, and others can be get by .backward , .input_name_to_gradient_name and .output_name_to_gradient_name on the return value, respectively.","title":"grad()"},{"location":"api/#freetensor.core.autograd.grad_","text":"Reverse mode automatic differentiation (in-place version) It returns a forward function, a backward function, and two maps on names. The forward function computes the original results. The backward function computes the gradients. The maps map from the names of the original arguments and return values, to the names of their gradients. If tape_in_closure is True (by default), the forward function has the same interface of the original function, but it will store some intermediate tensors (the tape) in some hidden global states. The backward functions receives the same inputs as the original function plus the graidents of the outputs, and also reads from the hidden states. The outputs of the original function are no longer exist in the backward function, and the input-outputs of the original function are converted to pure inputs. As grad_ is an in-place AD interface, the backward function passes the resulting gradients by additional mutable arguments. Names of the additional arguments can be looked up in the maps returned by grad_ . If tape_in_closure is False, global states described above will be passed by explicit arguments and return values, so you can store or manipluate these states between the forward run and the backward run. Parameters: func ( AST ) \u2013 The original function requires ( Sequence [ Union [ str , Parameter ]] ) \u2013 Name of input variables that need gradients. A parameter of a function can also be specified with a Parameter object by position provides ( Sequence [ Union [ str , Parameter , Return ]] ) \u2013 Name of output variables whose gradients are known. A mutable parameter of a function can also be specified with a Parameter object by position. A return value of a function can also be specified with a Return object by position tapes ( Union [ Sequence , GradTapeMode ] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure ( bool ) \u2013 True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True invert ( bool ) \u2013 If set to true, it can reduce the amount of recomputation or taping required. However, this may result in a loss of precision for floating-point numbers. Defaults to true. user_grads ( List [ ffi . StmtSetToUserGrad ] ) \u2013 For custom gradient. You do not have to explicitly set this parameter unless you are manipulating func by yourself (not getting it from the Python frontend). See UserGrad for details attach_backward ( bool ) \u2013 If True, the forward function will be the only return value, with backward function and other metadata attached to it jit_cache ( Callable [ Callable , Callable ] ) \u2013 Function decorator used to cache JIT instances verbose ( Optional [ int ] ) \u2013 Verbosity level Returns: Func if attach_backward else Tuple \u2013 If attach_backward is False, return a tuple. Return[0]: forward AST. Return[1]: backward AST. Return[2]: Mapping from names in requries to its gradient name. Return[3]: Mapping from names in provides to its gradient name. If attach_backward is True, only the forward AST is returned, and others can be get by .backward , .input_name_to_gradient_name and .output_name_to_gradient_name on the return value, respectively.","title":"grad_()"},{"location":"api/#freetensor.core.autograd.grad_body","text":"grad or grad_ on a function body (for internal tests only)","title":"grad_body()"},{"location":"api/#freetensor.core.autograd.jacrev","text":"Compute Jacobian tensors using Reverse mode automatic differentiation (out-of-place) jacrev computes one Jacobian tensor for one output and one or more inputs. Each Jacobian tensor consists of derivatives of all elements in the output tensor w.r.t. all elements in each inputs tensor. It returns a forward function, a backward function, and a map on names. The forward function computes the original results. The backward function computes the Jacobian tensors. The map maps from the names of the original arguments to the names of their Jacobian tensors. The forward function has the same interface of the original function, but it will store some intermediate tensors (the tape) in some hidden global states. The backward functions receives the same inputs as the original function, and also reads from the hidden states. The outputs of the original function are no longer exist in the backward function, and the input-outputs of the original function are converted to pure inputs. As jacrev is an out-of-place interface, the backward function returns the resulting Jacobian as additional return values. Names of the additional return values can be looked up in the map returned by jacrev . Suppose the output's shape is (d1, d2, ...) , and there are two inputs, whose shapes are (e1, e2, ...) and (f1, f2, ...) , respectively. If flatten is False (by default), the Jacobian tensors' shape will be (d1, d2, ..., e1, e2, ...) and (d1, d2, ..., f1, f2, ...) , respectively. If flatten is True, there will be only one Jacbian tensor, whose shape will be (d1 * d2 * ..., e1 * e2 * ... + f1 * f2 * ...) . Parameters: func ( AST ) \u2013 The original function inputs ( Sequence [ Union [ str , Parameter ]] ) \u2013 Name of input variables that the Jacobian tensors are for. output ( Union [ str , Parameter , Return ] ) \u2013 Name of one output variables that the Jacobian tensors are for. A return value of a function can be specified with a Return object flatten ( bool ) \u2013 If True, concatenate all Jacobian tensors together, to form an (n, m) -shaped output, where n is the total number of elements in the specified output, and m is the total number of elements in the specified inputs. This requires all involved inputs having the same data type and memory type. In this case, the name of the Jacobian tensor will be \"jacrev.flatten\" , and the returned name map will be empty tapes ( Union [ Sequence , GradTapeMode ] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history invert ( bool ) \u2013 If set to true, it can reduce the amount of recomputation or taping required. However, this may result in a loss of precision for floating-point numbers. Defaults user_grads ( Optional [ Sequence [ ffi . StmtSetToUserGrad ]] ) \u2013 For custom gradient. You do not have to explicitly set this parameter unless you are manipulating func by yourself (not getting it from the Python frontend). See UserGrad for details attach_backward ( bool ) \u2013 If True, the forward function will be the only return value, with backward function and other metadata attached to it jit_cache ( Callable [ Callable , Callable ] ) \u2013 Function decorator used to cache JIT instances verbose ( Optional [ int ] ) \u2013 Verbosity level Returns: Func if attach_backward else Tuple \u2013 If attach_backward is False, return a tuple. Return[0]: forward AST. Return[1]: backward AST. Return[2]: Mapping from names in inputs to its Jacobian tensor name. If attach_backward is True, only the forward AST is returned, and others can be get by .backward and .input_name_to_gradient_name on the return value, respectively.","title":"jacrev()"},{"location":"api/#freetensor.core.autograd.jacrev_","text":"Compute Jacobian tensors using Reverse mode automatic differentiation (in-place) jacrev computes one Jacobian tensor for one output and one or more inputs. Each Jacobian tensor consists of derivatives of all elements in the output tensor w.r.t. all elements in each inputs tensor. It returns a forward function, a backward function, and a map on names. The forward function computes the original results. The backward function computes the Jacobian tensors. The map maps from the names of the original arguments to the names of their Jacobian tensors. The forward function has the same interface of the original function, but it will store some intermediate tensors (the tape) in some hidden global states. The backward functions receives the same inputs as the original function, and also reads from the hidden states. The outputs of the original function are no longer exist in the backward function, and the input-outputs of the original function are converted to pure inputs. As jacrev_ is an in-place interface, the backward function passes the resulting gradients by additional mutable arguments. Names of the additional arguments can be looked up in the map returned by jacrev_ . Suppose the output's shape is (d1, d2, ...) , and there are two inputs, whose shapes are (e1, e2, ...) and (f1, f2, ...) , respectively. If flatten is False (by default), the Jacobian tensors' shape will be (d1, d2, ..., e1, e2, ...) and (d1, d2, ..., f1, f2, ...) , respectively. If flatten is True, there will be only one Jacbian tensor, whose shape will be (d1 * d2 * ..., e1 * e2 * ... + f1 * f2 * ...) . Parameters: func ( AST ) \u2013 The original function inputs ( Sequence [ Union [ str , Parameter ]] ) \u2013 Name of input variables that the Jacobian tensors are for. A parameter of a function can also be specified with a Parameter object by position output ( Union [ str , Parameter , Return ] ) \u2013 Name of one output variables that the Jacobian tensors are for. A mutable parameter of a function can also be specified with a Parameter object by position. A return value of a function can also be specified with a Return object by position flatten ( bool ) \u2013 If True, concatenate all Jacobian tensors together, to form an (n, m) -shaped output, where n is the total number of elements in the specified output, and m is the total number of elements in the specified inputs. This requires all involved inputs having the same data type and memory type. In this case, the name of the Jacobian tensor will be \"jacrev.flatten\" , and the returned name map will be empty tapes ( Union [ Sequence , GradTapeMode ] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history invert ( bool ) \u2013 If set to true, it can reduce the amount of recomputation or taping required. However, this may result in a loss of precision for floating-point numbers. Defaults user_grads ( Optional [ Sequence [ ffi . StmtSetToUserGrad ]] ) \u2013 For custom gradient. You do not have to explicitly set this parameter unless you are manipulating func by yourself (not getting it from the Python frontend). See UserGrad for details attach_backward ( bool ) \u2013 If True, the forward function will be the only return value, with backward function and other metadata attached to it jit_cache ( Callable [ Callable , Callable ] ) \u2013 Function decorator used to cache JIT instances verbose ( Optional [ int ] ) \u2013 Verbosity level Returns: Func if attach_backward else Tuple \u2013 If attach_backward is False, return a tuple. Return[0]: forward AST. Return[1]: backward AST. Return[2]: Mapping from names in inputs to its Jacobian tensor name. If attach_backward is True, only the forward AST is returned, and others can be get by .backward and .input_name_to_gradient_name on the return value, respectively.","title":"jacrev_()"},{"location":"api/#freetensor.core.codegen","text":"","title":"codegen"},{"location":"api/#freetensor.core.codegen.codegen","text":"Generate native code Parameters: ast ( Func ) \u2013 The AST to be lowered. It must includes function signature to determine parameters and return values. If not specified, a partial function is returned, which can be used as a decorator jit_cache ( Callable [ Callable , Callable ] ) \u2013 Function decorator used to cache JIT instances target ( Target ( Optional ) ) \u2013 The target architecture. If omitted, use the default one in config Returns: NativeCode or JITTemplate \u2013 Return a NativeCode for the generated code if there is no JIT parameters. Return a JITTemplate that generates a NativeCode if there is at least one","title":"codegen()"},{"location":"api/#freetensor.core.config","text":"Global configurations","title":"config"},{"location":"api/#freetensor.core.context","text":"Facility to pick statements to build an AST Classes and functions in this module are internally used by transformer to construct ASTs. They are also used by some internal tests. API of these classes and functions are subject to changes. End users are encouraged to use transformer , instead of this module.","title":"context"},{"location":"api/#freetensor.core.context.ContextStack","text":"","title":"ContextStack"},{"location":"api/#freetensor.core.context.ContextStack.get_last_stmt_id","text":"Can be used inside the staged code, to get the ID of the immediately preceding statement","title":"get_last_stmt_id()"},{"location":"api/#freetensor.core.context.ContextStack.push_append_stmt_callback","text":"Add a callback to be called with all next statements to be appended. For If statement, it can be called twice, one without \"else\" branch, and then maybe one more with \"else\" branch","title":"push_append_stmt_callback()"},{"location":"api/#freetensor.core.context.StmtRange","text":"Record a set of statement in a program, can be used for custom gradient Usage: with StmtRange() as rng: # Some statements StmtRange can be used interleaved with AST scopes. In these cases, you can directly call __enter__ and __exit__ . E.g., rng = StmtRange() rng.__enter__() # Some statements with VarDef(...) # Some scopes # Some other statements rng.__exit__(None, None, None)","title":"StmtRange"},{"location":"api/#freetensor.core.context.pop_ast","text":"Get AST and reset context Internally used by transformer and tests","title":"pop_ast()"},{"location":"api/#freetensor.core.context.pop_ast_and_user_grads","text":"Get AST and reset context. Return an extra list for custom gradients Set UserGrad for details","title":"pop_ast_and_user_grads()"},{"location":"api/#freetensor.core.driver","text":"","title":"driver"},{"location":"api/#freetensor.core.driver.Device","text":"Bases: ffi . Device A computing device can be constructed from 1. (TargetType, DeviceNumber) 2. (TargetType, getDeviceByName): cuda uses best matches criteria. 3. (TargetType, FullName, nth): get nth(from 0) device named Fullname . E.g. Device(TargetType::GPU, 0) means the 0-th GPU (device) Device(TargetType::GPU, \"V100\") means a GPU which best matches \"V100\" Device(TargetType::GPU, \"NVIDIA GeForce RTX 3060 Laptop GPU\", 0) A Device can be used as a \"with\" scope, then all the Array s and Driver s will use it by default. In this style, it also sets the default Target. E.g: with Device(...): ast = lower(ast) # Use the Target of the Device above by default a = Array(...) # Use the Device above by default","title":"Device"},{"location":"api/#freetensor.core.driver.Driver","text":"Bases: EnableAttachBackward , ffi . Driver","title":"Driver"},{"location":"api/#freetensor.core.driver.Driver.__call__","text":"Set argument, execute the binary code, and collect the returns If there is only one return value, it is returned directly. Otherwise, the return values are packed in a ReturnValuesPack This function will introduce some overhaed handling arguments and return values. For an accurate execution time measurement, plase call self.set_args first, then self.time , and finally self.collect_returns","title":"__call__()"},{"location":"api/#freetensor.core.driver.Driver.__init__","text":"Compile a program using a backend compiler and load it into memory This class is for internal use. Please consider using build_binary Parameters: func ( ffi . Func ) \u2013 AST of the function, where the function signature is needed to determine the parameters and return values src ( str ) \u2013 Native code generated from codegen device ( Device ( Optional ) ) \u2013 The device to run the program. If omitted, use the default device in config verbose ( bool ( Optional ) ) \u2013 True to print extra infomation","title":"__init__()"},{"location":"api/#freetensor.core.driver.Driver.collect_returns","text":"Collect return values from an invocation Return values must be collect. Otherwise there will be memory leaks If there is only one return value, it is returned directly. Otherwise, or if always_return_pack is set, the return values are packed in a ReturnValuesPack","title":"collect_returns()"},{"location":"api/#freetensor.core.driver.Driver.native_code","text":"Get native code compiled by backend compiler","title":"native_code()"},{"location":"api/#freetensor.core.driver.Driver.set_args","text":"Set argument for an invocation","title":"set_args()"},{"location":"api/#freetensor.core.driver.ReturnValuesPack","text":"Hold return values from a Driver invocation Return values can be retrieved in an anonymous manner: x, y, z = pack , or in a named manner: pack['x'] Please note that a ReturnValuesPack is different from a OrderedDict, as OrderedDict unpacks to keys rather than values","title":"ReturnValuesPack"},{"location":"api/#freetensor.core.driver.ReturnValuesPack.__contains__","text":"Test if a return value exists","title":"__contains__()"},{"location":"api/#freetensor.core.driver.ReturnValuesPack.__getitem__","text":"Get a return value with a name. Tuple is supported for multiple values","title":"__getitem__()"},{"location":"api/#freetensor.core.driver.ReturnValuesPack.__iter__","text":"Get all return values in the order declared in Func","title":"__iter__()"},{"location":"api/#freetensor.core.driver.array","text":"Factory function for Array This function is preferred over directly calling Array 's constructor, because it accepts more data format. If data is another FreeTensor Array , the original object will be returned, with dont_drop_borrow and moved set to new values. If dtype is set and different from the original data type, the Array will be copied first to convert the data type. If data is Numpy Array or PyTorch Tensor , it will be converted to FreeTensor Array . Memory copy will be avoided in most cases, but it is inevitable if the data is strided. If dtype is set and different from the original data type, the Array or Tensor will be copied first to convert the data type. Otherwise, the data will be treated as an n-dimensional array-like object, and will be parsed according the rules in NumPy. The data type is also set accordingly, unless dtype is set. Parameters: data ( FreeTensor Array, Numpy Array, PyTorch Tensor, or other array-like objects ) \u2013 Data to be copied to or borrowed by the new Array object dtype ( ft.DataType or str ) \u2013 If data is not in dtype , convert it to dtype first before constructing the Array dont_drop_borrow ( bool ) \u2013 If true, report an error if we have to drop a borrwed data. This flag is set to true when the Array is cunstructed IMPLICITLY (not by this function) from a user object by borrowing from it, where users may expect they are acutually manipulating the their user object, instead of this Array moved ( bool ) \u2013 If true, it means we do not care about data in this Array any more after the program runs. Variables with \"input-mutable\" access type may modify the Array","title":"array()"},{"location":"api/#freetensor.core.driver.build_binary","text":"Compile a program using a backend compiler and load it into memory Parameters: code ( NativeCode ) \u2013 Native code generated by codegen . If not specified, a partial function is returned, which can be used as a decorator device ( Device ( Optional ) ) \u2013 The device to run the program. If omitted, use the default device in config jit_cache ( Callable [ Callable , Callable ] ) \u2013 Function decorator used to cache JIT instances verbose ( int ) \u2013 Verbosity level Returns: Driver or JITTemplate \u2013 Return a Driver for the executable if there is no JIT parameters. Return a JITTemplate that generates a Driver if there is at least one","title":"build_binary()"},{"location":"api/#freetensor.core.driver.move","text":"Alias for array(data, dont_drop_borrow=False, moved=True)","title":"move()"},{"location":"api/#freetensor.core.enable_attach_backward","text":"","title":"enable_attach_backward"},{"location":"api/#freetensor.core.enable_attach_backward.EnableAttachBackward","text":"Get backward object (Func, Driver, etc) and other meta data from a forward object This class is a Mixin Class. It should be inherited BEFORE other base classes in multiple inheritance.","title":"EnableAttachBackward"},{"location":"api/#freetensor.core.enable_attach_backward.EnableAttachBackward.__init__","text":"Forward all arguments to other base classes In Python, super(). init calls the next base class in the full inheritance graph of the final class, not only base classes of BackwardAttachedMixin. See https://docs.python.org/3/tutorial/classes.html#multiple-inheritance","title":"__init__()"},{"location":"api/#freetensor.core.expr","text":"Facility to build AST expressions Classes and functions in this module are not only used internally for constructing AST nodes, and also exposed to users via multi-stage programming","title":"expr"},{"location":"api/#freetensor.core.expr.AlreadyMadeReduceTo","text":"A single-value type that marks a ReduceTo node is already made, and there is no need to make another Store node In standard Python data model, functions like iadd returns the modified self, and setitem does a self-assignment. We do the augmenting assignment directly in iadd and return AlreadyMadeReduceTo, so we do not have to Store it again","title":"AlreadyMadeReduceTo"},{"location":"api/#freetensor.core.expr.UndeclaredParam","text":"Error type. For error reporting only.","title":"UndeclaredParam"},{"location":"api/#freetensor.core.expr.VarRef","text":"Bases: ffi . FrontendVar Variable of FreeTensor All variables in FreeTensor DSL (declared via Var , created by empty or var , returned by libop , etc.), and their slices, are VarRef objects. Operations on VarRef objects generates AST nodes","title":"VarRef"},{"location":"api/#freetensor.core.expr.VarRef.shape","text":"Return lengths of all dimensions or the length of one dimension .shape() -> list of lengths of all dimensions .shape(dim) -> length of dimension dim , where dim can be int or Expr All lengths can be Expr (if the length is dynamically decided) or int (if statically decided)","title":"shape()"},{"location":"api/#freetensor.core.expr.VarRefFromVarDef","text":"Bases: VarRef VarRef with extra checks","title":"VarRefFromVarDef"},{"location":"api/#freetensor.core.expr.VarVersionRef","text":"Bases: VarRef Special VarRef used for custom gradient, generated from mark_version","title":"VarVersionRef"},{"location":"api/#freetensor.core.expr.abs","text":"Absolute value For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.abs Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The absolute value","title":"abs()"},{"location":"api/#freetensor.core.expr.add","text":"lhs + rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.add Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The sum","title":"add()"},{"location":"api/#freetensor.core.expr.any","text":"Create an AnyExpr node (only for testing and type inference) Any nodes matches any expression nodes in ast.match","title":"any()"},{"location":"api/#freetensor.core.expr.cast","text":"Cast to another type Parameters: expr ( VarRef or Number ) \u2013 The operand dtype ( DataTypr or str ) \u2013 The target data type Returns: VarRef or Number \u2013 The result","title":"cast()"},{"location":"api/#freetensor.core.expr.ceil","text":"Round a float up to an interger (towards +inf) For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ceil Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result","title":"ceil()"},{"location":"api/#freetensor.core.expr.ceildiv","text":"Ceiling integer division of lhs dividing by rhs The result rounds towards positive infinity For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ceildiv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient","title":"ceildiv()"},{"location":"api/#freetensor.core.expr.dtype","text":"Get element data type of a variable","title":"dtype()"},{"location":"api/#freetensor.core.expr.eq","text":"lhs == rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.eq Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison","title":"eq()"},{"location":"api/#freetensor.core.expr.exp","text":"Natural exponent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.exp Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The exponent","title":"exp()"},{"location":"api/#freetensor.core.expr.floor","text":"Round a float down to an interger (towards -inf) For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.floor Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result","title":"floor()"},{"location":"api/#freetensor.core.expr.floordiv","text":"Floored integer division of lhs dividing by rhs The result rounds towards negative infinity (following Python convention, instead of C) This function is recommended over round_towards_0_div , as it enjoys more optimizations For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.floordiv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient","title":"floordiv()"},{"location":"api/#freetensor.core.expr.ge","text":"lhs >= rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ge Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison","title":"ge()"},{"location":"api/#freetensor.core.expr.gt","text":"lhs > rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.gt Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison","title":"gt()"},{"location":"api/#freetensor.core.expr.if_then_else","text":"Similar to then_case if cond else else_case NOTE: there is NO guarantee that only one branch will be executed. In some cases, both branches will be executed and the result of one of them will be picked. Therefore, please do NOT use if_then_else to guard an out-of-bound array indexing Parameters: cond ( VarRef of Number ) \u2013 Condition then_case ( VarRef or Number ) \u2013 Then-case experssion else_case ( VarRef or Number ) \u2013 Else-case expression Returns: VarRef or Number \u2013 The result","title":"if_then_else()"},{"location":"api/#freetensor.core.expr.intrinsic","text":"Invoke whatever target code Parameters: fmt ( str ) \u2013 What to run. \"%\" is filled by parameters one by one. E.g. sinf(%) *params ( Sequence [ Expr ] ) \u2013 (Positional variadic) Parameters to fmt ret_type ( DataType or str ) \u2013 (Keyword argument only) The return type. Void for no return type. Defaults to Void has_side_effect ( bool ) \u2013 (Keyword argument only) True to indicate the intrinsic modifes something other than the return value. Defaults to false","title":"intrinsic()"},{"location":"api/#freetensor.core.expr.l_and","text":"Logical and of lhs and rhs NOTE: Short-circuit evaluation is NOT supported For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_and Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The logical and","title":"l_and()"},{"location":"api/#freetensor.core.expr.l_not","text":"Logical not For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_not Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The logical not","title":"l_not()"},{"location":"api/#freetensor.core.expr.l_or","text":"Logical or of lhs and rhs NOTE: Short-circuit evaluation is NOT supported For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_or Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The logical or","title":"l_or()"},{"location":"api/#freetensor.core.expr.le","text":"lhs <= rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.le Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison","title":"le()"},{"location":"api/#freetensor.core.expr.ln","text":"Natural logarithm For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ln Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The exponent","title":"ln()"},{"location":"api/#freetensor.core.expr.load_at_version","text":"Create an LoadAtVersion node (only for custom gradient) This node is only used for custom gradient. See UserGradForPrevStmt .","title":"load_at_version()"},{"location":"api/#freetensor.core.expr.lt","text":"lhs < rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.lt Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison","title":"lt()"},{"location":"api/#freetensor.core.expr.max","text":"Maximum of lhs and rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.max Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The maximum","title":"max()"},{"location":"api/#freetensor.core.expr.min","text":"Minimum of lhs and rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.min Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The minimum","title":"min()"},{"location":"api/#freetensor.core.expr.mod","text":"lhs modulus rhs The result is always non-negative (following Python convention, instead of C). This function is recommended over remainder , as it enjoys more optimizations For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.mod Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The modulo","title":"mod()"},{"location":"api/#freetensor.core.expr.mtype","text":"Get memory type of a variable","title":"mtype()"},{"location":"api/#freetensor.core.expr.mul","text":"lhs * rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.mul Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The product","title":"mul()"},{"location":"api/#freetensor.core.expr.ndim","text":"Get the number of dimensions of a variable","title":"ndim()"},{"location":"api/#freetensor.core.expr.ne","text":"lhs != rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ne Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison","title":"ne()"},{"location":"api/#freetensor.core.expr.remainder","text":"Remainder of lhs dividing rhs The result can be positive or negative (following C convention, instead of Python). End users are encouraged to use lhs % rhs instead, which follows Python convetion, and enjoys better optimization in FreeTensor For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.remainder Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The remainder","title":"remainder()"},{"location":"api/#freetensor.core.expr.round_towards_0_div","text":"C-style integer division of lhs dividing by rhs The result rounds towards 0 (following C convention, instead of Python) End users are encouraged to use lhs // rhs instead, which follows Python convetion, and enjoys better optimization in FreeTensor For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.round_towards_0_div Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient","title":"round_towards_0_div()"},{"location":"api/#freetensor.core.expr.shape","text":"shape(var, i): Get size of specified dimension of a variable shape(var): Get sizes of all dimensions of a variable","title":"shape()"},{"location":"api/#freetensor.core.expr.sigmoid","text":"Sigmoid For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sigmoid Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result","title":"sigmoid()"},{"location":"api/#freetensor.core.expr.sqrt","text":"Square root For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sqrt Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The square root","title":"sqrt()"},{"location":"api/#freetensor.core.expr.square","text":"Square For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.square Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The square","title":"square()"},{"location":"api/#freetensor.core.expr.sub","text":"lhs - rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sub Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The difference","title":"sub()"},{"location":"api/#freetensor.core.expr.tanh","text":"Hyperbolic tangent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.tanh Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result","title":"tanh()"},{"location":"api/#freetensor.core.expr.truediv","text":"Floating point division of lhs dividing by rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.truediv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient","title":"truediv()"},{"location":"api/#freetensor.core.frontend","text":"A frontend transforming user Python functions to ASTs via staging.","title":"frontend"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload","text":"Bases: StagingOverload Helper class managing context in IR staging.","title":"FreeTensorOverload"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.fullname","text":"Get distinct name.","title":"fullname()"},{"location":"api/#freetensor.core.frontend.LifetimeScope","text":"This scope is used to register multiple scopes inside a single lifetime scope. The inner scopes might be used to register variables, etc. They will be exited in reverse order of their registration.","title":"LifetimeScope"},{"location":"api/#freetensor.core.frontend.UserGrad","text":"Bases: UserGradStaged Define a custom gradient Follow the following steps to define custom gradient: Add some mark_version statements in the program. mark_version('y0', y) marks the specific versions of variable y at the program position of the statement and at all iterations as 'y0' . Add a UserGrad scope. 2.1. UserGrad optionally receives parameter stmt_range , recorded by the StmtRange helper class, which means the gradient is for the code specified in the range. Ignoring the parameter means setting gradient for the previous statement of the scope. 2.2. Other parameters of UserGrad sets the mapping from original variables to gradient variables. with UserGradForPrevStmt(x, y) as (dx, dy) provides VarRef dx and dy as gradient variables to be used inside the scope. In order to use the value from the forward pass in the backward pass, do not access the forward variables directly in the scope. Instead, use load_at_version expressions. load_at_version(y0, i, j) loads from y[i, j] at the specific version marked by y0 = mark_version(y) , saved from the same iteration in the forward pass . (If directly writing staged code, it is MarkVersion('y0', y) ). In other words, after AD, the position of mark_version and the dynamic loop iterator together makes up the actual version number for the tape. Build the AST with pop_ast_and_user_grads instead of pop_ast . An extra list will be returned together with the AST, which you need to pass as grad 's user_grads argument. This list records the forward-to-backward relation of the nodes. If you are directly writing staged code, use UserGradStaged instead. Parameters: *args ( Sequence [ VarRef ] ) \u2013 (Positional variadic) Mapping from original variables to gradient variables stmt_range ( StmtRange ) \u2013 The range in the original program that we are setting custom gradient for","title":"UserGrad"},{"location":"api/#freetensor.core.frontend.Var","text":"Bases: StagedTypeAnnotation","title":"Var"},{"location":"api/#freetensor.core.frontend.Var.__init__","text":"Declare a variable Parameters: shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable atype ( str or AccessType ) \u2013 Access type of the variable. It specifies whether (and how) the variable is an I/O variable of the function it belongs to. Defaults to \"input\" mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used","title":"__init__()"},{"location":"api/#freetensor.core.frontend.VarCreator","text":"Bases: StagedAssignable","title":"VarCreator"},{"location":"api/#freetensor.core.frontend.VarCreator.assign","text":"Customized assign behavior. Creates a VarDef with its full name.","title":"assign()"},{"location":"api/#freetensor.core.frontend.VersionMarker","text":"Bases: StagedAssignable","title":"VersionMarker"},{"location":"api/#freetensor.core.frontend.VersionMarker.assign","text":"Customized assign behavior. Creates a MarkVersion with its full name.","title":"assign()"},{"location":"api/#freetensor.core.frontend.dynamic_range","text":"Bases: StagedIterable Dynamic range that generates For loop in IR tree.","title":"dynamic_range"},{"location":"api/#freetensor.core.frontend.dynamic_range.__init__","text":"Initialize a dynamic range. Arguments semantic identical to builtin range .","title":"__init__()"},{"location":"api/#freetensor.core.frontend.dynamic_range.foreach","text":"Customized foreach behavior. Creates a For loop.","title":"foreach()"},{"location":"api/#freetensor.core.frontend.push_for_backward","text":"Push the current value from the forward pass to be used at the backward pass This function is for custom gradients. See UserGrad for details on how to provide custom gradients. You may imagine there is a virtual stack for each variable. Each time you call x_handle = push_for_backward(x) in the forward pass, the value of x at the current iteration will be \"pushed\" to the virtual stack. You can access x_handle at the backward pass. Each time you access x_handle , you will \"pop\" the stack and get the value of x pushed at the same iteration . Since the \"stack\" is virtual, you do NOT need to \"pop\" the same count as \"push\"es: the version numbering is fully automatic. Besides, there may not be a real stack at runtime: it can be compiled to any data structure. This function will be staged to mark_version statement in the IR.","title":"push_for_backward()"},{"location":"api/#freetensor.core.func","text":"","title":"func"},{"location":"api/#freetensor.core.func.Func","text":"Bases: EnableAttachBackward , ffi . Func","title":"Func"},{"location":"api/#freetensor.core.func.Func.__call__","text":"Enable invoking a transformed AST in another function being transformed, via inlined_invoke","title":"__call__()"},{"location":"api/#freetensor.core.jit","text":"","title":"jit"},{"location":"api/#freetensor.core.jit.JIT","text":"Declare a function parameter as a JIT parameter A function with one or more JIT parameters will be compiled to a JIT template. It can be instantiate after the JIT paraemters are provided Usage: x: JIT or x: JIT[AnyPythonType] . The latter form has no syntactic meanings, and is only for documentation. NOTE 1: The JIT type annotation can only be used for parameter of the outer-most function intended for @transform (or @optimize , etc). It can NOT be used for inner functions intended for @inline . NOTE 2: The JIT type annoation can only annotated on parameters inside the function signature. It is NOT supported in annotations for statements.","title":"JIT"},{"location":"api/#freetensor.core.jit.JITTemplate","text":"Bases: abc . ABC A template that can be instantiated given concrete arguments By calling instantiate with actual arguments you are expecting to run a JIT function with, an instantiated object will be returned. Subclasses of JITTemplate should override instantiate_by_only_jit_args , and define what is actually returned. Parameters: params ( OrderedDict ) \u2013 Parameter list from inspect.signature(func).parameters jit_param_names ( Sequence ) \u2013 Sequence of names of JIT parameters in the original order defined in the function","title":"JITTemplate"},{"location":"api/#freetensor.core.jit.JITTemplate.instantiate","text":"Get an instance with the arguments you are expecting to run a JIT function with","title":"instantiate()"},{"location":"api/#freetensor.core.jit.JITTemplate.instantiate_and_call","text":"Get an instance and call it with the arguments you are expecting to run a JIT function with","title":"instantiate_and_call()"},{"location":"api/#freetensor.core.jit.JITTemplate.instantiate_by_only_jit_args","text":"Get an instance with only JIT arguments This function accpets a tuple of arguments. Keyword arguments is NOT supported, so memoization can be easier, with considering the order of the arguments.","title":"instantiate_by_only_jit_args()"},{"location":"api/#freetensor.core.jit.JITTemplate.separate_args","text":"Return a list of non-JIT args, and a list of JIT args","title":"separate_args()"},{"location":"api/#freetensor.core.optimize","text":"","title":"optimize"},{"location":"api/#freetensor.core.optimize.optimize","text":"An one-click optimization from Python function to binary executable Usage: @optimize def f(...): ... It is equivalent to: @build_binary @codegen @lower @transform def f(...): ... Parameters: func ( Python function or AST ) \u2013 The user function to optimize. If not specified, a partial function will be returend, which can be used as a decorator schedule_callback ( Callable ( Optional ) ) \u2013 Schedule(s) to apply backward_callback ( Callable ) \u2013 Specify what schedule(s) to do for the backward function, if ast is returned from AD with attach_backward=True . Defaults to be the same with callback target ( Target ( Optional ) ) \u2013 The target architecture. You don't have to set target if you set device device ( Device ( Optional ) ) \u2013 Where to run the program default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( int ( Optional ) ) \u2013 Verbosity level. Can be 0, 1 or 2","title":"optimize()"},{"location":"api/#freetensor.core.optimize.optimize_to_pytorch","text":"Compile a FreeTensor function to a PyTorch call, whose gradient can be recognized by PyTorch The compiled function will be a typical PyTorch's \"function\" (rather than a PyTorch's \"module\"). Technically, this means it is a wrapper function around a PyTorch's Function 's apply method Schedules (if any) must be applied to the forward function and the backward function separated. For this reason, currently only first-order gradient is supported Parameters: func ( Python function or AST ) \u2013 The user function to optimize. If not specified, a partial function will be returend, which can be used as a decorator tapes ( Union [ Sequence , GradTapeMode ] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history forward_schedule_callback ( Callable ( Optional ) ) \u2013 Schedule(s) to apply to the forward function backward_schedule_callback ( Callable ( Optional ) ) \u2013 Schedule(s) to apply to the backward function target ( Target ( Optional ) ) \u2013 The target architecture. You don't have to set target if you set device device ( Device ( Optional ) ) \u2013 Where to run the program default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( int ( Optional ) ) \u2013 Verbosity level. Can be 0, 1 or 2","title":"optimize_to_pytorch()"},{"location":"api/#freetensor.core.param_ret_dict","text":"","title":"param_ret_dict"},{"location":"api/#freetensor.core.param_ret_dict.ParamRetDict","text":"Look an object using either a function parameter or return value's name or position","title":"ParamRetDict"},{"location":"api/#freetensor.core.param_ret_dict.ParamRetDict.__init__","text":"Either func or ( func_name and param_names and return_names ) should be provided","title":"__init__()"},{"location":"api/#freetensor.core.param_ret_dict.Parameter","text":"Alias of a parameter of a function by position instead of by name Parameter(n) represents the n-th parameter (counted from 0) Parameter() can be used if there is only one parameter","title":"Parameter"},{"location":"api/#freetensor.core.param_ret_dict.Return","text":"Alias of a return value of a function by position instead of by name Return(n) represents the n-th return value (counted from 0) Return() can be used if there is only one return value","title":"Return"},{"location":"api/#freetensor.core.passes","text":"","title":"passes"},{"location":"api/#freetensor.core.passes.lower","text":"Lower an AST using a series of passes Parameters: ast ( AST ) \u2013 The AST to be lowered. Can be a Func or a Stmt . If not specified, a partial function of lower will be returned, which can be used as a decorator target ( Target ( Optional ) ) \u2013 Lower the AST to a target with target-specific passes, then the AST can be used for codegen. If not set, use the default Target in Config skip_passes ( Sequence [ str ]( Optional ) ) \u2013 Skip some pass for testing or debugging. Names in skip_passes are in underscore_style, as in Python. Please note that some passes will not be skipped even specified in these parameter, because they are indirectly called in some other passes jit_cache ( Callable [ Callable , Callable ] ) \u2013 Function decorator used to cache JIT instances verbose ( int ( Optional ) ) \u2013 0 = print nothing. 1 = print the lowered AST. 2 = print AST after every single passes Returns: Func or JITTemplate \u2013 Return a Func for an AST if there is no JIT parameters. Return a JITTemplate that generates a Func if there is at least one","title":"lower()"},{"location":"api/#freetensor.core.staging","text":"A staging framework to support the FreeTensor frontend.","title":"staging"},{"location":"api/#freetensor.core.staging.AllowShortcutScope","text":"Allow return scope. This is a context manager that allows return in statically deterministic control flow.","title":"AllowShortcutScope"},{"location":"api/#freetensor.core.staging.BreakException","text":"Bases: Exception Exception to be raised by StagingOverload.break_stmt. Breaks from a for loop.","title":"BreakException"},{"location":"api/#freetensor.core.staging.ContinueException","text":"Bases: Exception Exception to be raised by StagingOverload.continue_stmt. Continues a for loop.","title":"ContinueException"},{"location":"api/#freetensor.core.staging.ReturnException","text":"Bases: Exception Exception to be raised by StagingOverload.return_stmt. Holds a return value that will be passed through to the function wrapper.","title":"ReturnException"},{"location":"api/#freetensor.core.staging.StagingError","text":"Bases: Exception Error occurred during staging function execution (i.e. IR tree generation).","title":"StagingError"},{"location":"api/#freetensor.core.staging.StagingOverload","text":"","title":"StagingOverload"},{"location":"api/#freetensor.core.staging.StagingOverload.allow_shortcut_scope","text":"Opens a scope that allows shortcut control flows in a statically deterministic context. Need to be closed by with statement.","title":"allow_shortcut_scope()"},{"location":"api/#freetensor.core.staging.StagingOverload.assert_stmt","text":"Assert staging tool.","title":"assert_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.assign_stmt","text":"Customized assign wrapper. If value is instance of StagedAssignable , it's regarded as a customized assign behavior and gets executed with the assigned target variable name. This wrapper is used for initializing a variable.","title":"assign_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.at_position","text":"Code position handler. Defaults to a no-op. Can be overridden by subclasses. Parameters: filename ( str ) \u2013 Name of the file containing code for the next statement. lineno ( int ) \u2013 Line number of the next statement.","title":"at_position()"},{"location":"api/#freetensor.core.staging.StagingOverload.break_stmt","text":"Break staging tool. Only allow break in static control flow.","title":"break_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.continue_stmt","text":"Continue staging tool. Only allow continue in static control flow.","title":"continue_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.custom_attr","text":"Customized attribute accessor. The framework first looks for a Python native attribute. If not found, it looks for this overloaded custom attribute resolver. The default implementation provides no custom attribute. Can be overridden by subclasses. Parameters: obj ( Any ) \u2013 Object to access attribute. attr ( str ) \u2013 Attribute name. Returns: Any ( Any ) \u2013 The attribute value.","title":"custom_attr()"},{"location":"api/#freetensor.core.staging.StagingOverload.foreach","text":"Customized foreach wrapper. If value is instance of StagedIterable , its regarded as a customized foreach behavior and used to generate code for the python for loop. Otherwise, we try to execute the loop as usual.","title":"foreach()"},{"location":"api/#freetensor.core.staging.StagingOverload.functiondef_wrapper","text":"Function definition wrapper. This wrapper performs extra initialization and cleanup for function definition.","title":"functiondef_wrapper()"},{"location":"api/#freetensor.core.staging.StagingOverload.if_then_else_expr","text":"If-then-else expression staging tool.","title":"if_then_else_expr()"},{"location":"api/#freetensor.core.staging.StagingOverload.if_then_else_stmt","text":"If-then-else statement staging tool. When predicate is deterministic in staging, only one branch is generated. Otherwise, a If node in IR is generated.","title":"if_then_else_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.load_attr","text":"Load attribute staging tool. Allows customization of reading attributes.","title":"load_attr()"},{"location":"api/#freetensor.core.staging.StagingOverload.metadata","text":"Metadata handler. A metadata line is a comment starting with #! and followed by a metadata, represented as a string parameter. Defaults to a no-op. Can be overridden by subclasses. Parameters: content ( str ) \u2013 The metadata content.","title":"metadata()"},{"location":"api/#freetensor.core.staging.StagingOverload.return_stmt","text":"Return staging tool. Only allow return in static control flow.","title":"return_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.unpack_assign_stmt","text":"Customized assign wrapper for one or more targets. If values is instance of StagedUnpackAssignable , it's regarded as a customized assign behavior and gets executed with all the assigned targets' names. Otherwise, it calls assign_stmt with each sub-assignments. Please note that names can be nested tuples like (\"a\", (\"b\", \"c\")) . Please also note that names can also be a single string like \"a\" even if values is a tuple. There is no unpacking in this case","title":"unpack_assign_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.while_stmt","text":"While statement staging tool.","title":"while_stmt()"},{"location":"api/#freetensor.core.staging.TransformError","text":"Bases: Exception Error occurred during AST transforming from python function to staging function that generates IR tree.","title":"TransformError"},{"location":"api/#freetensor.core.staging.Transformer","text":"Bases: ast . NodeTransformer","title":"Transformer"},{"location":"api/#freetensor.core.staging.Transformer.visit_AnnAssign","text":"Rule: x: Ty -> freetensor__annotate__x = annotate_stmt('x', Ty) if freetensor__annotate__x: x = freetensor__annotate__x : pure annotation","title":"visit_AnnAssign()"},{"location":"api/#freetensor.core.staging.Transformer.visit_Assign","text":"Rule: lhs = rhs -> lhs = unpack_assign_stmt('lhs', rhs) x.lhs = rhs -> x.lhs = unpack_assign_stmt('lhs', rhs) a, (b, c) = (x, (y, z)) -> a, (b, c) = unpack_assign_stmt(('a', ('b', 'c')), (x, (y, z))) a = b = c -> a = unpack_assign_stmt('a', c); b = unpack_assign_stmt('b', c) If unpack_assign_stmt is not overloaded, assign_stmt will be called for each item","title":"visit_Assign()"},{"location":"api/#freetensor.core.staging.Transformer.visit_Compare","text":"Expand multiple comparison into and expression.","title":"visit_Compare()"},{"location":"api/#freetensor.core.staging.Transformer.visit_For","text":"Rule: for x in iter: body -> def for_body(x): body foreach('x', iter, for_body)","title":"visit_For()"},{"location":"api/#freetensor.core.staging.Transformer.visit_If","text":"Rule: if pred: body else: orelse -> def then_body(): body def else_body(): orelse if_then_else_stmt(pred, then_body, else_body)","title":"visit_If()"},{"location":"api/#freetensor.core.staging.Transformer.visit_IfExp","text":"Rule: body if test else orelse -> if_then_else_expr(test, body, orelse)","title":"visit_IfExp()"},{"location":"api/#freetensor.core.staging.Transformer.visit_While","text":"Rule: while pred: body -> def while_body(): body while_stmt(lambda: pred, while_body)","title":"visit_While()"},{"location":"api/#freetensor.core.staging.call_helper","text":"Call helper that generates a python AST Call node with given callee (overload member) and arguments AST node.","title":"call_helper()"},{"location":"api/#freetensor.core.staging.function_helper","text":"Function helper that generates a python AST FunctionDef node with given name, arguments name, and body.","title":"function_helper()"},{"location":"api/#freetensor.core.stmt","text":"Facility to build AST statements Classes and functions in this module are internally used by transformer to construct ASTs. They are also used by some internal tests. API of these classes and functions are subject to changes. End users are encouraged to use transformer , instead of this module. Classes and functions in this module are all in BigCamel naming style, to distinguish from expressions in expr.py","title":"stmt"},{"location":"api/#freetensor.core.stmt.Assert","text":"Scope used to create an Assert node This scope is internally used by transformer and tests E.g.: with Assert(i > 0): ... # Assertion body","title":"Assert"},{"location":"api/#freetensor.core.stmt.Else","text":"Scope used to create an else branch of an If node This scope is internally used by transformer and tests E.g.: with If(i > 0): ... # True branch with Else(): ... # Else branch","title":"Else"},{"location":"api/#freetensor.core.stmt.For","text":"Scope used to create a For node This scope is internally used by transformer and tests E.g.: with For('i', 0, n) as i: ... # Loop body","title":"For"},{"location":"api/#freetensor.core.stmt.If","text":"Scope used to create an If node This scope is internally used by transformer and tests E.g.: with If(i > 0): ... # Branch body","title":"If"},{"location":"api/#freetensor.core.stmt.Invoke","text":"Inlined invocation of another AST Invoke is used as a scope ( with Invoke(...) as returned_vars ), so that variables returned by the callee can be used in the socpe Invoke can be used for invoking a gradient function, which has already been lowered as an AST. Please note that once a user function has been lowered as an AST, the dimensionalities of its tensors get fixed. Therefore, to invoke ordinary user functions, please use inline in transformer instead, which supports generic types","title":"Invoke"},{"location":"api/#freetensor.core.stmt.NamedScope","text":"Scope used to create an StmtSeq node with an explicit labels E.g.: with NamedScope(): ... # body This scope is used for testing only. StmtSeq nodes can be deleted in many lowering passes","title":"NamedScope"},{"location":"api/#freetensor.core.stmt.UserGradStaged","text":"Internal staged implementation of UserGrad","title":"UserGradStaged"},{"location":"api/#freetensor.core.stmt.Any","text":"Create an Any node (only for testing) Any nodes matches any statement nodes in ast.match","title":"Any()"},{"location":"api/#freetensor.core.stmt.Eval","text":"Create an Eval node This scope is internally used by transformer and tests","title":"Eval()"},{"location":"api/#freetensor.core.stmt.MarkLabel","text":"Mark the ID of the following statement This scope is internally used by transformer and tests","title":"MarkLabel()"},{"location":"api/#freetensor.core.stmt.MarkVersion","text":"Create an MarkVersion node (only for custom gradient) This node is only used for custom gradient. See UserGrad .","title":"MarkVersion()"},{"location":"api/#freetensor.core.stmt.VarDef","text":"A factory function that creates a VarDef or a series of nested VarDef s This scope is internally used by transformer and tests","title":"VarDef()"},{"location":"api/#freetensor.core.utils","text":"","title":"utils"},{"location":"api/#freetensor.core.utils.as_decorator","text":"Enable a multi-parameter function f to be used as a decorator Suppose g = as_decorator(f) , enable the following usages: @g def h(...): ... @g(a=a, b=b, c=c) def h(...): ... Formally, g will have the same parameters as f . f 's first parameter should be the function it decorate, say h , and may have other parameters with default values. If h is set when called, g will return the decorated function, just as f does. If h is not set, g will return an f 's partial function with all other parameters set, and the partial function can then be decorate another h again.","title":"as_decorator()"},{"location":"api/#freetensor.libop","text":"","title":"libop"},{"location":"api/#freetensor.libop.constant","text":"","title":"constant"},{"location":"api/#freetensor.libop.constant.zeros","text":"Create a zero tensor Parameters: shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used Returns: VarRef \u2013 The zero tensor","title":"zeros()"},{"location":"api/#freetensor.libop.constant.zeros_","text":"Fill zeros to a tensor Parameters: y ( VarRef ) \u2013 The tensor to fill","title":"zeros_()"},{"location":"api/#freetensor.libop.element_wise","text":"","title":"element_wise"},{"location":"api/#freetensor.libop.element_wise.binary_op","text":"(Broadcasted) any element-wise operation on two tensors and return the result Parameters: op ( Callable ) \u2013 The operation applied to each item a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"binary_op()"},{"location":"api/#freetensor.libop.element_wise.binary_op_","text":"(Broadcasted) any element-wise operation on two tensors. The result is written to another tensor Parameters: op ( Callable ) \u2013 The operation applied to each item a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"binary_op_()"},{"location":"api/#freetensor.libop.element_wise.unary_op","text":"Any element-wise operation on a tensor and return the result Parameters: op ( Callable ) \u2013 The operation applied to each item x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"unary_op()"},{"location":"api/#freetensor.libop.element_wise.unary_op_","text":"Any element-wise operation on a tensor. The result is written to another tensor Parameters: op ( Callable ) \u2013 The operation applied to each item x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor","title":"unary_op_()"},{"location":"api/#freetensor.libop.pooling","text":"","title":"pooling"},{"location":"api/#freetensor.libop.pooling.global_avg_pool","text":"Global averaging pooling. The result is returned Parameters follow ONNX convention. Currently only 2-D pooling is supported","title":"global_avg_pool()"},{"location":"api/#freetensor.libop.pooling.global_avg_pool_","text":"Global averaging pooling. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D pooling is supported","title":"global_avg_pool_()"},{"location":"api/#freetensor.libop.pooling.max_pool","text":"Maximum pooling. The result is returned Parameters follow ONNX convention. Currently only 2-D pooling is supported","title":"max_pool()"},{"location":"api/#freetensor.libop.pooling.max_pool_","text":"Maximum pooling. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D pooling is supported","title":"max_pool_()"},{"location":"api/#freetensor.libop.reduction","text":"","title":"reduction"},{"location":"api/#freetensor.libop.reduction.reduce_max","text":"Maximum of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence [ int ]( Optional ) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ( Optional ) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor","title":"reduce_max()"},{"location":"api/#freetensor.libop.reduction.reduce_max_","text":"Maximum of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence [ int ]( Optional ) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ( Optional ) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True","title":"reduce_max_()"},{"location":"api/#freetensor.libop.reduction.reduce_min","text":"Minimum of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence [ int ]( Optional ) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ( Optional ) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor","title":"reduce_min()"},{"location":"api/#freetensor.libop.reduction.reduce_min_","text":"Minimum of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence [ int ]( Optional ) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ( Optional ) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True","title":"reduce_min_()"},{"location":"about/contrib/","text":"Contributing \u00b6 Pull Requests are welcome! Please configure (or install some plugins for) your editor, to support clang-format , yapf and editorconfig , for code formating. And please note that we use different naming styles in Python and C++ parts.","title":"Contributing"},{"location":"about/pub/","text":"Publication \u00b6 Shizhi Tang, Jidong Zhai, Haojie Wang, Lin Jiang, Liyan Zheng, Zhenhao Yuan, and Chen Zhang. 2022. FreeTensor: A Free-Form DSL with Holistic Optimizations for Irregular Tensor Programs. In Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation (PLDI \u201922), June 13-17, 2022, San Diego, CA, USA . ACM, New York, NY, USA, 16 pages. https://doi.org/10.1145/3519939.3523448. ( Download ) @inproceedings{10.1145/3519939.3523448, author = {Tang, Shizhi and Zhai, Jidong and Wang, Haojie and Jiang, Lin and Zheng, Liyan and Yuan, Zhenhao and Zhang, Chen}, title = {FreeTensor: A Free-Form DSL with Holistic Optimizations for Irregular Tensor Programs}, year = {2022}, isbn = {9781450392655}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3519939.3523448}, doi = {10.1145/3519939.3523448}, booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation}, pages = {872\u2013887}, numpages = {16}, keywords = {tensor computing, optimizing compilers, DSL}, location = {San Diego, CA, USA}, series = {PLDI 2022} } Evaluation code can be found in this repository . NOTE: API of FreeTensor has been changed since submission. To reproduce the exact result in the paper, please consider the Artifact Evaluation version of FreeTensor, published here .","title":"Publication"},{"location":"guide/","text":"Get Started \u00b6 Build and Run Your First Program with FreeTenor Optimize a Program with Schedules Optimize a Program with Hints Running on a GPU Automatic Differentiation","title":"Get Started"},{"location":"guide/ad/","text":"Automatic Differentiation \u00b6 Reverse-Mode AD Providing Your Custom Gradients Why or When do We Need Custom Gradients How to Write Custom Gradients in FreeTensor Additional Descriptions on push_for_backward Automatic Differentiation (AD) transforms a program to another program that computes the original one's derivative or gradient. FreeTensor supports Reverse-Mode AD, and there is a plan to support Forward-Mode AD in the future. Reverse-Mode AD \u00b6 Suppose there is a program x -> y -> z -> w that computes an output w from intermediate variables z and y , and an input variable x . Reverse-Mode AD generates a gradient program dw/dw=1 -> dw/dz -> dw/dy -> dw/dx that computes dw/dx by Chain Rule. y , z and w may be saved in a \"tape\" when evaluation the original program, to be reused in the gradient one. If FreeTensor is built with WITH_PYTORCH=ON , you can skip this section and turn to the @optimize_to_pytorch integration , which integrates seamlessly with PyTorch's autograd mechanism, but will incur some runtime overhead. Here is an example of Reverse-Mode AD in FreeTensor: import freetensor as ft import numpy as np n = 4 @ft.optimize @ft.grad(requires=['a', 'b'], provides=[ft.Return()], attach_backward=True) def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.zeros((), \"float32\") for i in range(n): y[()] += a[i] * b[i] return y a = np.array([0, 1, 2, 3], dtype=\"float32\") b = np.array([3, 2, 1, 0], dtype=\"float32\") y = test(a, b) print(y.numpy()) dzdy = np.array(1, dtype='float32') input_grads = test.input_name_to_gradient_name output_grads = test.output_name_to_gradient_name dzda, dzdb = test.backward( **{output_grads[ft.Return()]: dzdy})[input_grads['a'], input_grads['b']] print(dzda.numpy()) print(dzdb.numpy()) You need to call ft.grad (or the inplace version ft.grad_ ) to generate a forward function and a backward function. In this example, the backward function is attached as the test.backward property because attach_backward is set to True . You can set it to False and ft.grad will return both functions. Please note that test is updated by ft.grad and becomes different than the original function, as it may save some intermediate tensors to a global tape , and it must be executed before the backward test.backward . Note on JIT JIT is only supported when attach_backward = True . After that, you call ft.optimize to optimize and compile the program just as in previous examples. This time it is done for both test and test.backward . Finally, you execute test and test.backward . The parameters and return values of test.backward are the gradients of a , b and y , which have their own names. To set and get these parameters and return values, you look up for them in two dictionaries test.input_name_to_gradient_name and test.output_name_to_gradient_name (in type ft.ParamRetDict . These two dictionaries accept either a name of a parameter, or a special ft.Return to specify a return value. When invoking test.backward , parameters can be set via keyword arguments, and return values can be collect via a bracket (from a special type ft.ReturnValuesPack ). These two maps are attached to test because attach_backward is True . Otherwise, they are returned as return values from ft.grad . Intermediate variables are not always have to be saved to the \"tape\" from the forward function. If a variable is need in the backward function but not saved, it will be re-computed, which is sometimes even faster than saving it due to better locality. By default, FreeTensor uses heuristics to determine which variable to save. To get better performance, you may want to control which intermediate variables should be saved by setting an optional tapes parameter in ft.grad . tapes can either be a different mode, or a explicit list of AST node IDs of all VarDef nodes of the variables you want to save. Providing Your Custom Gradients \u00b6 Why or When do We Need Custom Gradients \u00b6 Sometimes neither reverse-mode or forward-mode AD produces the most elegant form of gradients. FreeTensor allows you to provide your own gradients for part of the program. Take softmax as an example: The \\(\\mathbf{y} = softmax(\\mathbf{x})\\) function is mathematically defined by the following steps: \\[\\begin{align} e_i &= \\mathrm{e}^{x_i} \\label{eq:softmax-1} \\\\ s &= \\sum_i{e_i} \\label{eq:softmax-2} \\\\ y_i &= \\frac{e_i}{s} \\label{eq:softmax-3} \\end{align}\\] Suppose the final output of the program (the loss) is \\(z\\) . If using reverse-mode AD, the gradient of the input: \\(\\frac{\\partial z}{\\partial x}\\) can be computed by the following steps: \\[\\begin{align} \\frac{\\partial z}{\\partial s} &= -\\sum_i{\\frac{\\partial z}{\\partial y_i} \\frac{y_i}{s}} \\label{eq:softmax-grad-1} \\\\ \\frac{\\partial z}{\\partial e_i} &= \\frac{\\partial z}{\\partial y_i} \\frac{1}{s} + \\frac{\\partial z}{\\partial s} \\label{eq:softmax-grad-2} \\\\ \\frac{\\partial z}{\\partial x_i} &= \\frac{\\partial z}{\\partial e_i} e_i \\label{eq:softmax-grad-3} \\end{align}\\] However, usually we can NOT compute softmax by Equation \\(\\eqref{eq:softmax-1}\\eqref{eq:softmax-2}\\eqref{eq:softmax-3}\\) for numerical stability issues. Pratically, we compute softmax with additional normalization on \\(\\mathbf{x}\\) : \\[\\begin{align} m &= \\max_i{x_i} \\label{eq:softmax-norm-1} \\\\ e_i &= \\mathrm{e}^{x_i - m} \\label{eq:softmax-norm-2} \\\\ s &= \\sum_i{e_i} \\label{eq:softmax-norm-3} \\\\ y_i &= \\frac{e_i}{s} \\label{eq:softmax-norm-4} \\end{align}\\] If we directly apply reverse-mode AD on Equation \\(\\eqref{eq:softmax-norm-1}\\eqref{eq:softmax-norm-2}\\eqref{eq:softmax-norm-3}\\eqref{eq:softmax-norm-4}\\) , the backward program will be like: \\[\\begin{align} \\frac{\\partial z}{\\partial s} &= -\\sum_i{\\frac{\\partial z}{\\partial y_i} \\frac{y_i}{s}} \\\\ \\frac{\\partial z}{\\partial e_i} &= \\frac{\\partial z}{\\partial y_i} \\frac{1}{s} + \\frac{\\partial z}{\\partial s} \\\\ \\frac{\\partial z}{\\partial m} &= -\\sum_i{\\frac{\\partial z}{\\partial e_i}e_i} \\\\ \\frac{\\partial z}{\\partial x_i} &= \\frac{\\partial z}{\\partial e_i} e_i + \\begin{cases}\\frac{\\partial z}{\\partial m}, &i = \\arg\\max_j{x_j} \\\\ 0, &i \\neq \\arg\\max_j{x_j}\\end{cases} \\end{align}\\] You may have found that there is an extra \\(\\frac{\\partial z}{\\partial m}\\) involved. Apparently, the gradient should be the same no matter if we do the normalization. This is because \\(\\frac{\\partial z}{\\partial m}\\) actually always equals to \\(0\\) . FreeTensor can not dig out this mathematical property, so the computation on \\(\\frac{\\partial z}{\\partial m}\\) will remain and will be wasted. How to Write Custom Gradients in FreeTensor \u00b6 The following examples will demonstrate how to provide your own custom gradients, to override the default AD behaviour. Please note that this is only for demonstration. If you are just going to use softmax, call it from libop.softmax , which has already implemented the following code. First we show a softmax implementation with full AD: import freetensor as ft import torch n = 4 @ft.optimize # Set verbose=1 to see the code @ft.grad(requires=['x'], provides=[ft.Return()], attach_backward=True) def test(x: ft.Var[(n,), \"float32\"]): # Automatically decide gradients for this statement m = ft.reduce_max(x, axes=[-1]) e = ft.exp(x - m) s = ft.reduce_sum(e, axes=[-1]) y = e / s return y # Check forward result x = torch.rand(n, dtype=torch.float32) x.requires_grad = True y_ft = test(x).torch() y_torch = torch.softmax(x, axis=-1) assert torch.all(torch.isclose(y_ft, y_torch)) # Check backward result y_torch.grad = dzdy = torch.rand(n, dtype=torch.float32) input_grads = test.input_name_to_gradient_name output_grads = test.output_name_to_gradient_name dzdx_ft = test.backward(**{output_grads[ft.Return()]: dzdy}).torch() y_torch.backward(y_torch.grad) dzdx_torch = x.grad assert torch.all(torch.isclose(dzdx_ft, dzdx_torch, 1e-4, 1e-7)) Then, we add our own gradient to it: import freetensor as ft import torch n = 4 @ft.optimize # Set verbose=1 to see the code @ft.grad(requires=['x'], provides=[ft.Return()], attach_backward=True) def test(x: ft.Var[(n,), \"float32\"]): # Mark the range that you want to provide graident for, with `StmtRange` with ft.StmtRange() as rng: m = ft.reduce_max(x, axes=[-1]) e = ft.exp(x - m) s = ft.reduce_sum(e, axes=[-1]) y = e / s # Call `push_for_backward` so we can use forward values in backward e_now = ft.push_for_backward(e) s_now = ft.push_for_backward(s) y_now = ft.push_for_backward(y) # Define gradient in `UserGrad` with ft.UserGrad(x, y, stmt_range=rng) as (dzdx, dzdy): # Retrieve forward value from `y_now`, NOT `y` dzds = -ft.reduce_sum(dzdy * y_now, axes=[-1]) / s_now dzde = dzdy / s_now + dzds dzdx[...] += dzde * e_now # Use `+=` here return y # Check forward result x = torch.rand(n, dtype=torch.float32) x.requires_grad = True y_ft = test(x).torch() y_torch = torch.softmax(x, axis=-1) assert torch.all(torch.isclose(y_ft, y_torch)) # Check backward result y_torch.grad = dzdy = torch.rand(n, dtype=torch.float32) input_grads = test.input_name_to_gradient_name output_grads = test.output_name_to_gradient_name dzdx_ft = test.backward(**{output_grads[ft.Return()]: dzdy}).torch() y_torch.backward(y_torch.grad) dzdx_torch = x.grad assert torch.all(torch.isclose(dzdx_ft, dzdx_torch, 1e-4, 1e-7)) First, we mark the range of code that we want to provide gradient for, with ft.StmtRange , as a name rng . In the range, we write the code to compute softmax as usual. Additionaly, for the values that we want to reuse in the gradient, we call ft.push_for_backward to save it. push_for_backward returns a handle that you can use as a usual tensor in the gradient code. If your StmtRange is inside an outer loop, the handle will always reflect the correct iteration (see the next example). Besides, push_for_backward does not mean the value will be physically saved in tape: it only means the value will be logically reused in the backward, no matter by saving or by recomputing. push_for_backward is orthogonal with the tapes parameter in ft.grad . Next, we define our custom gradient with a ft.UserGrad scope. The scopes receives a special parameter stmt_range , which should be set to the StmtRange we have just defined. Beside stmt_range , UserGrand receives an arbitrary number of parameters, in this case, x and y , and returns the same number of variables, dzdx and dzdy , so we have the mapping between each variable and its gradient. What we are going to do is update dzdx from dzdy . We define our gradient code in the UserGrad code of Equation \\(\\eqref{eq:softmax-grad-1}\\eqref{eq:softmax-grad-2}\\eqref{eq:softmax-grad-3}\\) . We want to use the forward value y , s and e . But do NOT directly use its name, use the push_for_backward handler y_now , s_now and e_now instead. Finally, plase note that we update dzdx with += instead of = , because we may be only computing a partial derivative: there may be other functions of x other than y . And it is all done. Additional Descriptions on push_for_backward \u00b6 We have mentioned push_for_backward will automatically handle multiple versions of a variable. If you are familiar with PyTorch, you may have found the name is similar to PyTorch's save_for_backward . Here, versioning is the major difference: ft.push_for_backward can be called multiple times on a variable, to save multiple version (or snapshot of it), while the variable can keep changing. Here is an additional example: a softmax written in a loop form, where we receives a 2-d input, and apply softmax on the second dimension. Again, this is only for demonstration, and there are multiple ways to implement a softmax. import freetensor as ft import torch n = 4 @ft.optimize # Set verbose=1 to see the code @ft.grad(requires=['x'], provides=[ft.Return()], attach_backward=True) def test(x: ft.Var[(n, n), \"float32\"]): y = ft.empty((n, n), \"float32\") for i in range(n): # Mark the range that you want to provide graident for, with `StmtRange` with ft.StmtRange() as rng: # `m`, `e` and `s` are local to `i` m = ft.reduce_max(x[i], axes=[-1]) e = ft.exp(x[i] - m) s = ft.reduce_sum(e, axes=[-1]) y[i] = e / s # Call `push_for_backward` so we can use forward values in backward e_now = ft.push_for_backward(e) s_now = ft.push_for_backward(s) y_now = ft.push_for_backward(y) # Define gradient in `UserGrad` with ft.UserGrad(x, y, stmt_range=rng) as (dzdx, dzdy): # Retrieve forward value from `y_now`, NOT `y` dzds = -ft.reduce_sum(dzdy[i] * y_now[i], axes=[-1]) / s_now dzde = dzdy[i] / s_now + dzds dzdx[i] += dzde * e_now # Use `+=` here return y # Check forward result x = torch.rand(n, n, dtype=torch.float32) x.requires_grad = True y_ft = test(x).torch() y_torch = torch.softmax(x, axis=-1) assert torch.all(torch.isclose(y_ft, y_torch)) # Check backward result y_torch.grad = dzdy = torch.rand(n, n, dtype=torch.float32) input_grads = test.input_name_to_gradient_name output_grads = test.output_name_to_gradient_name dzdx_ft = test.backward(**{output_grads[ft.Return()]: dzdy}).torch() y_torch.backward(y_torch.grad) dzdx_torch = x.grad assert torch.all(torch.isclose(dzdx_ft, dzdx_torch, 1e-4, 1e-7)) Here our gradient scope is inside a loop, where m , e and s are local to the loop iteration. When we load the value from their push_for_backward handlers, we get the version of value at the exact iteration we need.","title":"Automatic Differentiation"},{"location":"guide/ad/#reverse-mode-ad","text":"Suppose there is a program x -> y -> z -> w that computes an output w from intermediate variables z and y , and an input variable x . Reverse-Mode AD generates a gradient program dw/dw=1 -> dw/dz -> dw/dy -> dw/dx that computes dw/dx by Chain Rule. y , z and w may be saved in a \"tape\" when evaluation the original program, to be reused in the gradient one. If FreeTensor is built with WITH_PYTORCH=ON , you can skip this section and turn to the @optimize_to_pytorch integration , which integrates seamlessly with PyTorch's autograd mechanism, but will incur some runtime overhead. Here is an example of Reverse-Mode AD in FreeTensor: import freetensor as ft import numpy as np n = 4 @ft.optimize @ft.grad(requires=['a', 'b'], provides=[ft.Return()], attach_backward=True) def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.zeros((), \"float32\") for i in range(n): y[()] += a[i] * b[i] return y a = np.array([0, 1, 2, 3], dtype=\"float32\") b = np.array([3, 2, 1, 0], dtype=\"float32\") y = test(a, b) print(y.numpy()) dzdy = np.array(1, dtype='float32') input_grads = test.input_name_to_gradient_name output_grads = test.output_name_to_gradient_name dzda, dzdb = test.backward( **{output_grads[ft.Return()]: dzdy})[input_grads['a'], input_grads['b']] print(dzda.numpy()) print(dzdb.numpy()) You need to call ft.grad (or the inplace version ft.grad_ ) to generate a forward function and a backward function. In this example, the backward function is attached as the test.backward property because attach_backward is set to True . You can set it to False and ft.grad will return both functions. Please note that test is updated by ft.grad and becomes different than the original function, as it may save some intermediate tensors to a global tape , and it must be executed before the backward test.backward . Note on JIT JIT is only supported when attach_backward = True . After that, you call ft.optimize to optimize and compile the program just as in previous examples. This time it is done for both test and test.backward . Finally, you execute test and test.backward . The parameters and return values of test.backward are the gradients of a , b and y , which have their own names. To set and get these parameters and return values, you look up for them in two dictionaries test.input_name_to_gradient_name and test.output_name_to_gradient_name (in type ft.ParamRetDict . These two dictionaries accept either a name of a parameter, or a special ft.Return to specify a return value. When invoking test.backward , parameters can be set via keyword arguments, and return values can be collect via a bracket (from a special type ft.ReturnValuesPack ). These two maps are attached to test because attach_backward is True . Otherwise, they are returned as return values from ft.grad . Intermediate variables are not always have to be saved to the \"tape\" from the forward function. If a variable is need in the backward function but not saved, it will be re-computed, which is sometimes even faster than saving it due to better locality. By default, FreeTensor uses heuristics to determine which variable to save. To get better performance, you may want to control which intermediate variables should be saved by setting an optional tapes parameter in ft.grad . tapes can either be a different mode, or a explicit list of AST node IDs of all VarDef nodes of the variables you want to save.","title":"Reverse-Mode AD"},{"location":"guide/ad/#providing-your-custom-gradients","text":"","title":"Providing Your Custom Gradients"},{"location":"guide/ad/#why-or-when-do-we-need-custom-gradients","text":"Sometimes neither reverse-mode or forward-mode AD produces the most elegant form of gradients. FreeTensor allows you to provide your own gradients for part of the program. Take softmax as an example: The \\(\\mathbf{y} = softmax(\\mathbf{x})\\) function is mathematically defined by the following steps: \\[\\begin{align} e_i &= \\mathrm{e}^{x_i} \\label{eq:softmax-1} \\\\ s &= \\sum_i{e_i} \\label{eq:softmax-2} \\\\ y_i &= \\frac{e_i}{s} \\label{eq:softmax-3} \\end{align}\\] Suppose the final output of the program (the loss) is \\(z\\) . If using reverse-mode AD, the gradient of the input: \\(\\frac{\\partial z}{\\partial x}\\) can be computed by the following steps: \\[\\begin{align} \\frac{\\partial z}{\\partial s} &= -\\sum_i{\\frac{\\partial z}{\\partial y_i} \\frac{y_i}{s}} \\label{eq:softmax-grad-1} \\\\ \\frac{\\partial z}{\\partial e_i} &= \\frac{\\partial z}{\\partial y_i} \\frac{1}{s} + \\frac{\\partial z}{\\partial s} \\label{eq:softmax-grad-2} \\\\ \\frac{\\partial z}{\\partial x_i} &= \\frac{\\partial z}{\\partial e_i} e_i \\label{eq:softmax-grad-3} \\end{align}\\] However, usually we can NOT compute softmax by Equation \\(\\eqref{eq:softmax-1}\\eqref{eq:softmax-2}\\eqref{eq:softmax-3}\\) for numerical stability issues. Pratically, we compute softmax with additional normalization on \\(\\mathbf{x}\\) : \\[\\begin{align} m &= \\max_i{x_i} \\label{eq:softmax-norm-1} \\\\ e_i &= \\mathrm{e}^{x_i - m} \\label{eq:softmax-norm-2} \\\\ s &= \\sum_i{e_i} \\label{eq:softmax-norm-3} \\\\ y_i &= \\frac{e_i}{s} \\label{eq:softmax-norm-4} \\end{align}\\] If we directly apply reverse-mode AD on Equation \\(\\eqref{eq:softmax-norm-1}\\eqref{eq:softmax-norm-2}\\eqref{eq:softmax-norm-3}\\eqref{eq:softmax-norm-4}\\) , the backward program will be like: \\[\\begin{align} \\frac{\\partial z}{\\partial s} &= -\\sum_i{\\frac{\\partial z}{\\partial y_i} \\frac{y_i}{s}} \\\\ \\frac{\\partial z}{\\partial e_i} &= \\frac{\\partial z}{\\partial y_i} \\frac{1}{s} + \\frac{\\partial z}{\\partial s} \\\\ \\frac{\\partial z}{\\partial m} &= -\\sum_i{\\frac{\\partial z}{\\partial e_i}e_i} \\\\ \\frac{\\partial z}{\\partial x_i} &= \\frac{\\partial z}{\\partial e_i} e_i + \\begin{cases}\\frac{\\partial z}{\\partial m}, &i = \\arg\\max_j{x_j} \\\\ 0, &i \\neq \\arg\\max_j{x_j}\\end{cases} \\end{align}\\] You may have found that there is an extra \\(\\frac{\\partial z}{\\partial m}\\) involved. Apparently, the gradient should be the same no matter if we do the normalization. This is because \\(\\frac{\\partial z}{\\partial m}\\) actually always equals to \\(0\\) . FreeTensor can not dig out this mathematical property, so the computation on \\(\\frac{\\partial z}{\\partial m}\\) will remain and will be wasted.","title":"Why or When do We Need Custom Gradients"},{"location":"guide/ad/#how-to-write-custom-gradients-in-freetensor","text":"The following examples will demonstrate how to provide your own custom gradients, to override the default AD behaviour. Please note that this is only for demonstration. If you are just going to use softmax, call it from libop.softmax , which has already implemented the following code. First we show a softmax implementation with full AD: import freetensor as ft import torch n = 4 @ft.optimize # Set verbose=1 to see the code @ft.grad(requires=['x'], provides=[ft.Return()], attach_backward=True) def test(x: ft.Var[(n,), \"float32\"]): # Automatically decide gradients for this statement m = ft.reduce_max(x, axes=[-1]) e = ft.exp(x - m) s = ft.reduce_sum(e, axes=[-1]) y = e / s return y # Check forward result x = torch.rand(n, dtype=torch.float32) x.requires_grad = True y_ft = test(x).torch() y_torch = torch.softmax(x, axis=-1) assert torch.all(torch.isclose(y_ft, y_torch)) # Check backward result y_torch.grad = dzdy = torch.rand(n, dtype=torch.float32) input_grads = test.input_name_to_gradient_name output_grads = test.output_name_to_gradient_name dzdx_ft = test.backward(**{output_grads[ft.Return()]: dzdy}).torch() y_torch.backward(y_torch.grad) dzdx_torch = x.grad assert torch.all(torch.isclose(dzdx_ft, dzdx_torch, 1e-4, 1e-7)) Then, we add our own gradient to it: import freetensor as ft import torch n = 4 @ft.optimize # Set verbose=1 to see the code @ft.grad(requires=['x'], provides=[ft.Return()], attach_backward=True) def test(x: ft.Var[(n,), \"float32\"]): # Mark the range that you want to provide graident for, with `StmtRange` with ft.StmtRange() as rng: m = ft.reduce_max(x, axes=[-1]) e = ft.exp(x - m) s = ft.reduce_sum(e, axes=[-1]) y = e / s # Call `push_for_backward` so we can use forward values in backward e_now = ft.push_for_backward(e) s_now = ft.push_for_backward(s) y_now = ft.push_for_backward(y) # Define gradient in `UserGrad` with ft.UserGrad(x, y, stmt_range=rng) as (dzdx, dzdy): # Retrieve forward value from `y_now`, NOT `y` dzds = -ft.reduce_sum(dzdy * y_now, axes=[-1]) / s_now dzde = dzdy / s_now + dzds dzdx[...] += dzde * e_now # Use `+=` here return y # Check forward result x = torch.rand(n, dtype=torch.float32) x.requires_grad = True y_ft = test(x).torch() y_torch = torch.softmax(x, axis=-1) assert torch.all(torch.isclose(y_ft, y_torch)) # Check backward result y_torch.grad = dzdy = torch.rand(n, dtype=torch.float32) input_grads = test.input_name_to_gradient_name output_grads = test.output_name_to_gradient_name dzdx_ft = test.backward(**{output_grads[ft.Return()]: dzdy}).torch() y_torch.backward(y_torch.grad) dzdx_torch = x.grad assert torch.all(torch.isclose(dzdx_ft, dzdx_torch, 1e-4, 1e-7)) First, we mark the range of code that we want to provide gradient for, with ft.StmtRange , as a name rng . In the range, we write the code to compute softmax as usual. Additionaly, for the values that we want to reuse in the gradient, we call ft.push_for_backward to save it. push_for_backward returns a handle that you can use as a usual tensor in the gradient code. If your StmtRange is inside an outer loop, the handle will always reflect the correct iteration (see the next example). Besides, push_for_backward does not mean the value will be physically saved in tape: it only means the value will be logically reused in the backward, no matter by saving or by recomputing. push_for_backward is orthogonal with the tapes parameter in ft.grad . Next, we define our custom gradient with a ft.UserGrad scope. The scopes receives a special parameter stmt_range , which should be set to the StmtRange we have just defined. Beside stmt_range , UserGrand receives an arbitrary number of parameters, in this case, x and y , and returns the same number of variables, dzdx and dzdy , so we have the mapping between each variable and its gradient. What we are going to do is update dzdx from dzdy . We define our gradient code in the UserGrad code of Equation \\(\\eqref{eq:softmax-grad-1}\\eqref{eq:softmax-grad-2}\\eqref{eq:softmax-grad-3}\\) . We want to use the forward value y , s and e . But do NOT directly use its name, use the push_for_backward handler y_now , s_now and e_now instead. Finally, plase note that we update dzdx with += instead of = , because we may be only computing a partial derivative: there may be other functions of x other than y . And it is all done.","title":"How to Write Custom Gradients in FreeTensor"},{"location":"guide/ad/#additional-descriptions-on-push_for_backward","text":"We have mentioned push_for_backward will automatically handle multiple versions of a variable. If you are familiar with PyTorch, you may have found the name is similar to PyTorch's save_for_backward . Here, versioning is the major difference: ft.push_for_backward can be called multiple times on a variable, to save multiple version (or snapshot of it), while the variable can keep changing. Here is an additional example: a softmax written in a loop form, where we receives a 2-d input, and apply softmax on the second dimension. Again, this is only for demonstration, and there are multiple ways to implement a softmax. import freetensor as ft import torch n = 4 @ft.optimize # Set verbose=1 to see the code @ft.grad(requires=['x'], provides=[ft.Return()], attach_backward=True) def test(x: ft.Var[(n, n), \"float32\"]): y = ft.empty((n, n), \"float32\") for i in range(n): # Mark the range that you want to provide graident for, with `StmtRange` with ft.StmtRange() as rng: # `m`, `e` and `s` are local to `i` m = ft.reduce_max(x[i], axes=[-1]) e = ft.exp(x[i] - m) s = ft.reduce_sum(e, axes=[-1]) y[i] = e / s # Call `push_for_backward` so we can use forward values in backward e_now = ft.push_for_backward(e) s_now = ft.push_for_backward(s) y_now = ft.push_for_backward(y) # Define gradient in `UserGrad` with ft.UserGrad(x, y, stmt_range=rng) as (dzdx, dzdy): # Retrieve forward value from `y_now`, NOT `y` dzds = -ft.reduce_sum(dzdy[i] * y_now[i], axes=[-1]) / s_now dzde = dzdy[i] / s_now + dzds dzdx[i] += dzde * e_now # Use `+=` here return y # Check forward result x = torch.rand(n, n, dtype=torch.float32) x.requires_grad = True y_ft = test(x).torch() y_torch = torch.softmax(x, axis=-1) assert torch.all(torch.isclose(y_ft, y_torch)) # Check backward result y_torch.grad = dzdy = torch.rand(n, n, dtype=torch.float32) input_grads = test.input_name_to_gradient_name output_grads = test.output_name_to_gradient_name dzdx_ft = test.backward(**{output_grads[ft.Return()]: dzdy}).torch() y_torch.backward(y_torch.grad) dzdx_torch = x.grad assert torch.all(torch.isclose(dzdx_ft, dzdx_torch, 1e-4, 1e-7)) Here our gradient scope is inside a loop, where m , e and s are local to the loop iteration. When we load the value from their push_for_backward handlers, we get the version of value at the exact iteration we need.","title":"Additional Descriptions on push_for_backward"},{"location":"guide/build-and-run/","text":"Build and Run \u00b6 Dependencies Build Run a Program with FreeTensor Global Configurations Run the Tests Build this Document Dependencies \u00b6 Linux Python (>= 3.8, for the Python frontend) GCC (>= 10, to support C++20 and the \"unroll\" pragma) CUDA (>= 11.4.1, to support GCC 10, Optional) MKL (Optional) PyTorch (Optional, see below) Java (= 11, Build-time dependency only) Other Python dependencies: pip3 install --user numpy sourceinspect astor Pygments Note on Python version Because we are analyzing Python AST, which is sensitive to Python version, there may be potential bugs for Python strictly later than 3.8. Please file an issue if something goes wrong PyTorch support FreeTensor can optionally link PyTorch to support a copy-free interface between FreeTensor and PyTorch. Please note that, if you are using CUDA, FreeTensor and PyTorch should link CUDA of the same version . PyTorch can be installed in any way you like, see PyTorch's guide . If you are installing a CUDA-supporting release of PyTorch via pip , you need to tell pip where to find the release, for example by a -i <url-to-some-pypi-index> argument, or a -f https://download.pytorch.org/whl/torch_stable.html argument. Tested python dependencies You can also install Python dependencies of the versions we have tested, instead of the latest, by pip3 install --user -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html . This also includes optional dependencies and dependencies only for development. Build \u00b6 First, clone this repo. Don't forget there are some submodules. git clone --recursive <path/to/this/repo> Then, build. mkdir build cd build cmake .. make -j # Or use Ninja There are some options to cmake : -DFT_WITH_CUDA=ON/OFF : build with/without CUDA (defaults to ON ). -DFT_WITH_MKL=<path/to/mkl/root> : build with MKL (path to MKL is required, defaults to building without it). The path accepts by CMake should be a raw unescaped path; i.e. -DFT_WITH_MKL=\"/some path\" is good since the quotes are resolved by the shell but -DFT_WITH_MKL=\\\"/some\\ path\\\" is not. -DFT_WITH_PYTORCH=ON/OFF : build with/without copy-free interface from/to PyTorch, requring PyTorch installed on the system (defaults to OFF ). -DFT_DEBUG_LOG_NODE=ON (for developers): enables tracing to tell by which pass a specific AST node is modified. -DFT_DEBUG_PROFILE=ON (for developers): profiles some heavy functions in the compiler. -DFT_DEBUG_SANITIZE=<sanitizer_name> (for developers): build with GCC sanitizer (set it to a sanitizer name to use, e.g. address). It will build a shared library with a name like freetensor_ffi.cpython-37m-x86_64-linux-gnu.so , which can be used in Python via import freetensor . Run a Program with FreeTensor \u00b6 To run any program with FreeTensor, one should add the python/ and build/ directory to PYTHONPATH first. E.g. to run a python program a.py with FreeTensor in the build/ directory, PYTHONPATH=../python:../build:$PYTHONPATH python3 a.py Global Configurations \u00b6 There are serveral global configurations can be set via environment variables: FT_PRETTY_PRINT=ON/OFF . Enable/disable colored printing. FT_PRINT_ALL_ID=ON/OFF . Print (or not) IDs of all statements in an AST. FT_WERROR=ON/OFF . Treat warnings as errors (or not). FT_BACKEND_COMPILER_CXX=<path/to/compiler> . The C++ compiler used to compiler the optimized program. Default to the same compiler found when building FreeTensor itself, and compilers found in the PATH enviroment variable. This environment variable should be set to a colon-separated list of paths, in which the paths are searched from left to right. FT_BACKEND_COMPILER_NVCC=<path/to/compiler> . The CUDA compiler used to compiler the optimized program (if built with CUDA). Default to the same compiler found when building FreeTensor itself, and compilers found in the PATH enviroment variable. This environment variable should be set to a colon-separated list of paths, in which the paths are searched from left to right. FT_DEBUG_RUNTIME_CHECK . Check out-of-bound access and integer overflow at the generated code at runtime. This option is only for debugging, and will introduce significant runtime overhead. Currently the checker cannot print the error site, please also enable FT_DEBUG_BINARY and then use GDB to locate the error site. FT_DEBUG_BINARY=ON (for developers). Compile with -g at backend. Do not delete the binary file after loaded. FT_DEBUG_CUDA_WITH_UM . Allocate CUDA buffers on Unified Memory, for faster (debugging) access of GPU Array from CPU, but with slower Array allocations and more synchronizations. No performance effect on normal in-kernel computations. This configurations can also set at runtime in ft.config . Run the Tests \u00b6 To run the test, first change into the test/ directory, then PYTHONPATH=../python:../build:$PYTHONPATH pytest To run a single test case, specify the test case name, and optionally use pytest -s to display the standard output. E.g, PYTHONPATH=../python:../build:$PYTHONPATH pytest -s 00.hello_world/test_basic.py::test_hello_world Debugging (for developers) If using GDB, one should invoke PyTest with python3 -m : PYTHONPATH=../python:../build:$PYTHONPATH gdb --args python3 -m pytest If using Valgrind, one should set Python to use the system malloc: PYTHONPATH=../python:../build:$PYTHONPATH PYTHONMALLOC=malloc valgrind python3 -m pytest Sometimes Valgrind is not enough to detect some errors. An alternative is to use the sanitizer from GCC. For example, if you are using the \"address\" sanitizer, first set -DFT_DEBUG_SANITIZE=address to cmake , and then: PYTHONPATH=../python:../build:$PYTHONPATH LD_PRELOAD=`gcc -print-file-name=libasan.so` pytest -s If you are using another sanitizer, change the string set to FT_DEBUG_SANITIZE and the library's name. For example, -DFT_DEBUG_SANITIZE=undefined and libubsan.so . Build this Document \u00b6 First install some dependencies: pip3 install --user mkdocs \"mkdocstrings[python]\" From the root directory of FreeTensor, run a HTTP server to serve the document (recommended, but without document on C++ interface due to a limitation ): PYTHONPATH=./python:./build:$PYTHONPATH mkdocs serve Or build and save the pages (with document on C++ interface, requiring Doxygen and Graphviz): doxygen Doxyfile && PYTHONPATH=./python:./build:$PYTHONPATH mkdocs build Publish the documents to GitHub Pages (for developers) doxygen Doxyfile && PYTHONPATH=./python:./build:$PYTHONPATH mkdocs gh-deploy","title":"Build and Run"},{"location":"guide/build-and-run/#dependencies","text":"Linux Python (>= 3.8, for the Python frontend) GCC (>= 10, to support C++20 and the \"unroll\" pragma) CUDA (>= 11.4.1, to support GCC 10, Optional) MKL (Optional) PyTorch (Optional, see below) Java (= 11, Build-time dependency only) Other Python dependencies: pip3 install --user numpy sourceinspect astor Pygments Note on Python version Because we are analyzing Python AST, which is sensitive to Python version, there may be potential bugs for Python strictly later than 3.8. Please file an issue if something goes wrong PyTorch support FreeTensor can optionally link PyTorch to support a copy-free interface between FreeTensor and PyTorch. Please note that, if you are using CUDA, FreeTensor and PyTorch should link CUDA of the same version . PyTorch can be installed in any way you like, see PyTorch's guide . If you are installing a CUDA-supporting release of PyTorch via pip , you need to tell pip where to find the release, for example by a -i <url-to-some-pypi-index> argument, or a -f https://download.pytorch.org/whl/torch_stable.html argument. Tested python dependencies You can also install Python dependencies of the versions we have tested, instead of the latest, by pip3 install --user -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html . This also includes optional dependencies and dependencies only for development.","title":"Dependencies"},{"location":"guide/build-and-run/#build","text":"First, clone this repo. Don't forget there are some submodules. git clone --recursive <path/to/this/repo> Then, build. mkdir build cd build cmake .. make -j # Or use Ninja There are some options to cmake : -DFT_WITH_CUDA=ON/OFF : build with/without CUDA (defaults to ON ). -DFT_WITH_MKL=<path/to/mkl/root> : build with MKL (path to MKL is required, defaults to building without it). The path accepts by CMake should be a raw unescaped path; i.e. -DFT_WITH_MKL=\"/some path\" is good since the quotes are resolved by the shell but -DFT_WITH_MKL=\\\"/some\\ path\\\" is not. -DFT_WITH_PYTORCH=ON/OFF : build with/without copy-free interface from/to PyTorch, requring PyTorch installed on the system (defaults to OFF ). -DFT_DEBUG_LOG_NODE=ON (for developers): enables tracing to tell by which pass a specific AST node is modified. -DFT_DEBUG_PROFILE=ON (for developers): profiles some heavy functions in the compiler. -DFT_DEBUG_SANITIZE=<sanitizer_name> (for developers): build with GCC sanitizer (set it to a sanitizer name to use, e.g. address). It will build a shared library with a name like freetensor_ffi.cpython-37m-x86_64-linux-gnu.so , which can be used in Python via import freetensor .","title":"Build"},{"location":"guide/build-and-run/#run-a-program-with-freetensor","text":"To run any program with FreeTensor, one should add the python/ and build/ directory to PYTHONPATH first. E.g. to run a python program a.py with FreeTensor in the build/ directory, PYTHONPATH=../python:../build:$PYTHONPATH python3 a.py","title":"Run a Program with FreeTensor"},{"location":"guide/build-and-run/#global-configurations","text":"There are serveral global configurations can be set via environment variables: FT_PRETTY_PRINT=ON/OFF . Enable/disable colored printing. FT_PRINT_ALL_ID=ON/OFF . Print (or not) IDs of all statements in an AST. FT_WERROR=ON/OFF . Treat warnings as errors (or not). FT_BACKEND_COMPILER_CXX=<path/to/compiler> . The C++ compiler used to compiler the optimized program. Default to the same compiler found when building FreeTensor itself, and compilers found in the PATH enviroment variable. This environment variable should be set to a colon-separated list of paths, in which the paths are searched from left to right. FT_BACKEND_COMPILER_NVCC=<path/to/compiler> . The CUDA compiler used to compiler the optimized program (if built with CUDA). Default to the same compiler found when building FreeTensor itself, and compilers found in the PATH enviroment variable. This environment variable should be set to a colon-separated list of paths, in which the paths are searched from left to right. FT_DEBUG_RUNTIME_CHECK . Check out-of-bound access and integer overflow at the generated code at runtime. This option is only for debugging, and will introduce significant runtime overhead. Currently the checker cannot print the error site, please also enable FT_DEBUG_BINARY and then use GDB to locate the error site. FT_DEBUG_BINARY=ON (for developers). Compile with -g at backend. Do not delete the binary file after loaded. FT_DEBUG_CUDA_WITH_UM . Allocate CUDA buffers on Unified Memory, for faster (debugging) access of GPU Array from CPU, but with slower Array allocations and more synchronizations. No performance effect on normal in-kernel computations. This configurations can also set at runtime in ft.config .","title":"Global Configurations"},{"location":"guide/build-and-run/#run-the-tests","text":"To run the test, first change into the test/ directory, then PYTHONPATH=../python:../build:$PYTHONPATH pytest To run a single test case, specify the test case name, and optionally use pytest -s to display the standard output. E.g, PYTHONPATH=../python:../build:$PYTHONPATH pytest -s 00.hello_world/test_basic.py::test_hello_world Debugging (for developers) If using GDB, one should invoke PyTest with python3 -m : PYTHONPATH=../python:../build:$PYTHONPATH gdb --args python3 -m pytest If using Valgrind, one should set Python to use the system malloc: PYTHONPATH=../python:../build:$PYTHONPATH PYTHONMALLOC=malloc valgrind python3 -m pytest Sometimes Valgrind is not enough to detect some errors. An alternative is to use the sanitizer from GCC. For example, if you are using the \"address\" sanitizer, first set -DFT_DEBUG_SANITIZE=address to cmake , and then: PYTHONPATH=../python:../build:$PYTHONPATH LD_PRELOAD=`gcc -print-file-name=libasan.so` pytest -s If you are using another sanitizer, change the string set to FT_DEBUG_SANITIZE and the library's name. For example, -DFT_DEBUG_SANITIZE=undefined and libubsan.so .","title":"Run the Tests"},{"location":"guide/build-and-run/#build-this-document","text":"First install some dependencies: pip3 install --user mkdocs \"mkdocstrings[python]\" From the root directory of FreeTensor, run a HTTP server to serve the document (recommended, but without document on C++ interface due to a limitation ): PYTHONPATH=./python:./build:$PYTHONPATH mkdocs serve Or build and save the pages (with document on C++ interface, requiring Doxygen and Graphviz): doxygen Doxyfile && PYTHONPATH=./python:./build:$PYTHONPATH mkdocs build Publish the documents to GitHub Pages (for developers) doxygen Doxyfile && PYTHONPATH=./python:./build:$PYTHONPATH mkdocs gh-deploy","title":"Build this Document"},{"location":"guide/first-program/","text":"Your First Program with FreeTenor \u00b6 Example: Vector addition Declare and Define Tensors Manipulating Tensors Dynamic or Static Just-in-Time (JIT) Compilation Dynamic Tensor Shapes Copy-free interface from/to PyTorch In this page, we introduce some basic concepts of FreeTensor. Example: Vector addition \u00b6 import freetensor as ft import numpy as np n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Here is a basic example program in FreeTensor. You write a Python function that manipulates FreeTensor's tensor type ft.Var , decorate the function with ft.optimize , and finally invoke the decorated function. FreeTensor will generate C++ code for this vector addition, compile it using a native compiler, and finally load it back to Python. Set verbose = 1 to optimize if you are interested in the generated native code. To write such a function, you need to follow some basic concept described in this page. Declare and Define Tensors \u00b6 All tensors, including function parameters, intermediate tensors and return values should be properly declared or defined. Scalars are 0-D tensors in FreeTensor. Declare or define a tensor with an empty shape, and you will get a scalar. Function parameters should be declared like x : ft.Var[shape, data_type] . Declaring a parameter either in the function signature or as a stand-alone statment is acceptable. If your parameter uses another parameter as shape, you will need the latter manner. An optional parameter atype can be set to \"output\" or \"inout\" if you want to mutate a function argument. Intermediate and returning tensors can be created by ft.empty , ft.var or ft.zeros . If you are using FreeTensor for GPU computing, an optional parameter mtype can be set to specify where to store the tensor. It defaults to the main memory of your currently chosen computing device. All tensors and their slices are implemented by an internal ft.VarRef type. If you are looking for a tensor's API, ft.VarRef is the right place. Manipulating Tensors \u00b6 To read or write tensors in a function, just write for ... in range(...) loops that iterate through elements in the tensors, and do arithmetic operations on them. We also provide some functions that operates on a whole tensor or a tensor slice in libop . Special note on tensor assignments We follow Python convention for tensor assignments, but sometimes it is a little counterintuitive. Suppose you have two list s in Python: a and b . a = b replaces the object a with the object b , while a[...] = b assigns data in b to a . FreeTensor does not support replacing a tensor object with another one. It supports assignments only. Therefore, we need to write a[...] = b to assign tensor. a[:] = b (for non-scalars), a[None] = b and a[()] = b is also supported. Dynamic or Static \u00b6 Another concept is that statements and expressions in your program are divided into two categories: dynamic and static . Dynamic statements or expressions are restricted to a small subset of Python, and are compiled to native code. Static statements or expressions can be any Python statements or expressions, and are executed before compilation. In other words, static statements or expressions are like macros or templates in C++, while dynamic ones are actually quotations in Multi-Stage Programming . The following statements and expressions are considered dynamic: Declarations, definitions and operations of FreeTensor's tensor type ft.Var (or its internal implementation ft.VarRef ). if statements, for ... in range(...) and assert statements that have a ft.Var condition or range. All other statements and expressions are considered static. With the help of dynamic and static categories, you can utilize complex Python functions as the static part, while still generate high-performance native code using dynamic loops. For example, the following code combines static and dynamic code to sum multiple vectors together: import freetensor as ft import numpy as np n = 4 @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"], c: ft.Var[(n,), \"int32\"]): inputs = [a, b, c] # Static y = ft.empty((n,), \"int32\") # Dynamic for i in range(n): # Dyanmic y[i] = 0 # Dynamic for item in inputs: # Static y[i] += item[i] # Dynamic return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\"), np.array([3, 4, 5, 6], dtype=\"int32\")).numpy() print(y) However, there might be some counterintuitive behaviours when using static statments or expressions. Please remember that static static statements or expressions are executed before compilation, so the following piece of code will result in a list containing only one item: the expression i , instead of 10 numbers: lst = [] for i in range(10): # Dynamic lst.append(i) # Static. Appends only once Just-in-Time (JIT) Compilation \u00b6 In the example of vector addition above, we support any vector length, but only in a static way. This means each time you change the vector length n , you need to recompile (run optimize again) the function. You may find it inconvenient because you need to write your own code to control whether or when to do the re-compilation, and the compilation code is entangled with the computation code. Therefore, FreeTensor supports automating this recompilation procedure, which can be considered as Just-in-Time (JIT) compilation. To enable this feature, just declare n as an additional parameter, with the type ft.JIT : import freetensor as ft import numpy as np @ft.optimize def test(n: ft.JIT, a, b): # Or `n: ft.JIT[int]` if you like, but it is only for documentation a: ft.Var[(n,), \"int32\"] # After the function signature to use `n` b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(4, np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) For each different n you pass, test will be automatically recompiled. The compiled test for the same n will be memoized, so the same instance will not be repelated compiled. You can also custom the memoization by setting the jit_cache parameter of ft.optimize . Note on parameter declaration You may have note that it is non-trivial to include a parameter n in other parameters a and b 's type annotation. Python actually forbids such a declaration if a and b 's type annotation is inside the function signature. To cope with this restriction, FreeTensor allows declaring a and b 's type AFTER the function sigature as statements. But this off-sigature annoation is only supported for ft.Var types, NOT ft.JIT . Dynamic Tensor Shapes \u00b6 Frequent recompilation does not meet many requirements, so FreeTensor also supports defining tensors with dynamic shapes, just by setting their shapes to a dynamic values. The following code shows an example: import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] # After the function signature to use `n` b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) assert np.array_equal(y, [3, 5, 7, 9]) In this way, in only have to compile your program once. You will expect a single but longer compiling time, and some optimizations are not possible with dynamic shapes. Copy-free interface from/to PyTorch \u00b6 If FreeTensor is built with WITH_PYTORCH=ON , you can directly pass PyTorch tensors to or get them from FreeTensor. For example, import freetensor as ft import torch n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(torch.tensor([1, 2, 3, 4], dtype=torch.int32), torch.tensor([2, 3, 4, 5], dtype=torch.int32)).torch() print(y) FreeTensor also supports integration with PyTorch's \"function\" interface. You can use @ft.optimize_to_pytorch to directly generate a PyTorch \"function\" (specifically, a function wrapper around PyTorch's Function.invoke , just like usual PyTorch functions). This approach seamlessly integrates with PyTorch's autograd mechanism, but incurs some more runtime overhead. Please also note that, because we do not know whether we need to do autograd and which input tensors need gradients until we first run a function, compiling of the FreeTensor code will be delayed to run time. The compiled binary code will be cached and reused if following runs requires the same set of inputs to be derived. The following code shows an example of this approach: import freetensor as ft import torch n = 4 # Change this line to ft.optimize_to_pytorch(verbose=1) to see the resulting # native code @ft.optimize_to_pytorch def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.empty((n,), \"float32\") for i in range(n): y[i] = a[i] * b[i] return y # Forward a = torch.tensor([1, 2, 3, 4], requires_grad=True, dtype=torch.float32) b = torch.tensor([2, 3, 4, 5], requires_grad=True, dtype=torch.float32) y = test(a, b) print(\"y = \", y) # Backward y.grad = torch.tensor([1, 1, 1, 1], dtype=torch.float32) y.backward(y.grad) print(\"a.grad = \", a.grad) print(\"b.grad = \", b.grad)","title":"Your First Program with FreeTenor"},{"location":"guide/first-program/#example-vector-addition","text":"import freetensor as ft import numpy as np n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Here is a basic example program in FreeTensor. You write a Python function that manipulates FreeTensor's tensor type ft.Var , decorate the function with ft.optimize , and finally invoke the decorated function. FreeTensor will generate C++ code for this vector addition, compile it using a native compiler, and finally load it back to Python. Set verbose = 1 to optimize if you are interested in the generated native code. To write such a function, you need to follow some basic concept described in this page.","title":"Example: Vector addition"},{"location":"guide/first-program/#declare-and-define-tensors","text":"All tensors, including function parameters, intermediate tensors and return values should be properly declared or defined. Scalars are 0-D tensors in FreeTensor. Declare or define a tensor with an empty shape, and you will get a scalar. Function parameters should be declared like x : ft.Var[shape, data_type] . Declaring a parameter either in the function signature or as a stand-alone statment is acceptable. If your parameter uses another parameter as shape, you will need the latter manner. An optional parameter atype can be set to \"output\" or \"inout\" if you want to mutate a function argument. Intermediate and returning tensors can be created by ft.empty , ft.var or ft.zeros . If you are using FreeTensor for GPU computing, an optional parameter mtype can be set to specify where to store the tensor. It defaults to the main memory of your currently chosen computing device. All tensors and their slices are implemented by an internal ft.VarRef type. If you are looking for a tensor's API, ft.VarRef is the right place.","title":"Declare and Define Tensors"},{"location":"guide/first-program/#manipulating-tensors","text":"To read or write tensors in a function, just write for ... in range(...) loops that iterate through elements in the tensors, and do arithmetic operations on them. We also provide some functions that operates on a whole tensor or a tensor slice in libop . Special note on tensor assignments We follow Python convention for tensor assignments, but sometimes it is a little counterintuitive. Suppose you have two list s in Python: a and b . a = b replaces the object a with the object b , while a[...] = b assigns data in b to a . FreeTensor does not support replacing a tensor object with another one. It supports assignments only. Therefore, we need to write a[...] = b to assign tensor. a[:] = b (for non-scalars), a[None] = b and a[()] = b is also supported.","title":"Manipulating Tensors"},{"location":"guide/first-program/#dynamic-or-static","text":"Another concept is that statements and expressions in your program are divided into two categories: dynamic and static . Dynamic statements or expressions are restricted to a small subset of Python, and are compiled to native code. Static statements or expressions can be any Python statements or expressions, and are executed before compilation. In other words, static statements or expressions are like macros or templates in C++, while dynamic ones are actually quotations in Multi-Stage Programming . The following statements and expressions are considered dynamic: Declarations, definitions and operations of FreeTensor's tensor type ft.Var (or its internal implementation ft.VarRef ). if statements, for ... in range(...) and assert statements that have a ft.Var condition or range. All other statements and expressions are considered static. With the help of dynamic and static categories, you can utilize complex Python functions as the static part, while still generate high-performance native code using dynamic loops. For example, the following code combines static and dynamic code to sum multiple vectors together: import freetensor as ft import numpy as np n = 4 @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"], c: ft.Var[(n,), \"int32\"]): inputs = [a, b, c] # Static y = ft.empty((n,), \"int32\") # Dynamic for i in range(n): # Dyanmic y[i] = 0 # Dynamic for item in inputs: # Static y[i] += item[i] # Dynamic return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\"), np.array([3, 4, 5, 6], dtype=\"int32\")).numpy() print(y) However, there might be some counterintuitive behaviours when using static statments or expressions. Please remember that static static statements or expressions are executed before compilation, so the following piece of code will result in a list containing only one item: the expression i , instead of 10 numbers: lst = [] for i in range(10): # Dynamic lst.append(i) # Static. Appends only once","title":"Dynamic or Static"},{"location":"guide/first-program/#just-in-time-jit-compilation","text":"In the example of vector addition above, we support any vector length, but only in a static way. This means each time you change the vector length n , you need to recompile (run optimize again) the function. You may find it inconvenient because you need to write your own code to control whether or when to do the re-compilation, and the compilation code is entangled with the computation code. Therefore, FreeTensor supports automating this recompilation procedure, which can be considered as Just-in-Time (JIT) compilation. To enable this feature, just declare n as an additional parameter, with the type ft.JIT : import freetensor as ft import numpy as np @ft.optimize def test(n: ft.JIT, a, b): # Or `n: ft.JIT[int]` if you like, but it is only for documentation a: ft.Var[(n,), \"int32\"] # After the function signature to use `n` b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(4, np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) For each different n you pass, test will be automatically recompiled. The compiled test for the same n will be memoized, so the same instance will not be repelated compiled. You can also custom the memoization by setting the jit_cache parameter of ft.optimize . Note on parameter declaration You may have note that it is non-trivial to include a parameter n in other parameters a and b 's type annotation. Python actually forbids such a declaration if a and b 's type annotation is inside the function signature. To cope with this restriction, FreeTensor allows declaring a and b 's type AFTER the function sigature as statements. But this off-sigature annoation is only supported for ft.Var types, NOT ft.JIT .","title":"Just-in-Time (JIT) Compilation"},{"location":"guide/first-program/#dynamic-tensor-shapes","text":"Frequent recompilation does not meet many requirements, so FreeTensor also supports defining tensors with dynamic shapes, just by setting their shapes to a dynamic values. The following code shows an example: import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] # After the function signature to use `n` b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) assert np.array_equal(y, [3, 5, 7, 9]) In this way, in only have to compile your program once. You will expect a single but longer compiling time, and some optimizations are not possible with dynamic shapes.","title":"Dynamic Tensor Shapes"},{"location":"guide/first-program/#copy-free-interface-fromto-pytorch","text":"If FreeTensor is built with WITH_PYTORCH=ON , you can directly pass PyTorch tensors to or get them from FreeTensor. For example, import freetensor as ft import torch n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(torch.tensor([1, 2, 3, 4], dtype=torch.int32), torch.tensor([2, 3, 4, 5], dtype=torch.int32)).torch() print(y) FreeTensor also supports integration with PyTorch's \"function\" interface. You can use @ft.optimize_to_pytorch to directly generate a PyTorch \"function\" (specifically, a function wrapper around PyTorch's Function.invoke , just like usual PyTorch functions). This approach seamlessly integrates with PyTorch's autograd mechanism, but incurs some more runtime overhead. Please also note that, because we do not know whether we need to do autograd and which input tensors need gradients until we first run a function, compiling of the FreeTensor code will be delayed to run time. The compiled binary code will be cached and reused if following runs requires the same set of inputs to be derived. The following code shows an example of this approach: import freetensor as ft import torch n = 4 # Change this line to ft.optimize_to_pytorch(verbose=1) to see the resulting # native code @ft.optimize_to_pytorch def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.empty((n,), \"float32\") for i in range(n): y[i] = a[i] * b[i] return y # Forward a = torch.tensor([1, 2, 3, 4], requires_grad=True, dtype=torch.float32) b = torch.tensor([2, 3, 4, 5], requires_grad=True, dtype=torch.float32) y = test(a, b) print(\"y = \", y) # Backward y.grad = torch.tensor([1, 1, 1, 1], dtype=torch.float32) y.backward(y.grad) print(\"a.grad = \", a.grad) print(\"b.grad = \", b.grad)","title":"Copy-free interface from/to PyTorch"},{"location":"guide/gpu/","text":"Running on a GPU \u00b6 Example: Vector addition on a GPU mtype=\"byvalue\" for Dynamic Tensor Shapes Example: Vector addition on a GPU \u00b6 If FreeTensor is built with a CUDA backend, you can compile your program to a GPU. We still take a vector addition as an example: import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): n = 4 # Add verbose=1 to see the resulting native code @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize('Li', 'threadIdx.x')) def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li # Label the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Similar to parallelizing to OpenMP threads , in this example, we parallelize Loop Li to the threadIdx.x dimension of CUDA. There are two major differences: You are now calling parallelize schedule with a threadIdx.x parameter, instead of openmp . All the code are enclosed by a with ft.GPU(0) scope. Usually, you not only parallelize your loops to threadIdx.x , but also other CUDA dimensions like blockIdx.x . To achieve this, you either parallelize different loops in a loop nests to different CUDA dimensions, or split your loops before parallelizing them. As for the with ft.GPU(0) scope, ft.GPU(0) specifies a Device (a specific hardware device of GPU). By calling with on a device, default values of several classes and functions are set, but currently you only need to be aware of two things: It sets the Device of optimize . It sets the default mtype of all tensors in the program, which is an optional parameter of ft.Var , ft.empty , etc. mtype refers to memory type. It controls where a tensor is stored. It defaults to \"cpu\" for a CPU program, and \"gpu/global\" for a GPU program. You probably GPU requires putting each variable to a right place (global memory, shared memory, registers, etc.), and this can be done by setting mtype s of each tensor. There are several ways to set mtype s: (Recommended) Leave them to the default \"gpu/global\" first, and modify them with the set_mem_type schedule. In this way, you write some architecture-dependent schedules, but keep your function architecture-independent. (Experimental) Leave them to the default \"gpu/global\" first, and modify them automatically using auto_schedule , or the auto_set_mem_type schedule (which is a part of auto_schedule ). Set them explicitly in the program by setting an optional mtype parameter of ft.Var , ft.empty , etc. mtype=\"byvalue\" for Dynamic Tensor Shapes \u00b6 Tensors with normal mtypes ( \"cpu\" , \"gpu/global\" , etc.) are passed by references, which means a \"cpu\" tensor can only be accessed from a CPU, and a \"gpu/global\" tensor can only be accessed from a GPU. However, sometimes, and especially for dynamic tensor shapes, we want the shapes to be passed by values, and accessible from both CPUs and GPUs (remember we need tensor's shape both when launching a kernel from the CPU side, and during actual computatoin on the GPU side). In this case, we can set the shape-related tensors a \"byvalue\" mtype , and here is an example: import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize(\"Li\", \"threadIdx.x\")) # Use \"byvalue\" for `n` so it can be used both during kernel launching # and inside a kernel def test(n: ft.Var[(), \"int32\", \"input\", \"byvalue\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li # Label the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y)","title":"Running on a GPU"},{"location":"guide/gpu/#example-vector-addition-on-a-gpu","text":"If FreeTensor is built with a CUDA backend, you can compile your program to a GPU. We still take a vector addition as an example: import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): n = 4 # Add verbose=1 to see the resulting native code @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize('Li', 'threadIdx.x')) def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li # Label the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Similar to parallelizing to OpenMP threads , in this example, we parallelize Loop Li to the threadIdx.x dimension of CUDA. There are two major differences: You are now calling parallelize schedule with a threadIdx.x parameter, instead of openmp . All the code are enclosed by a with ft.GPU(0) scope. Usually, you not only parallelize your loops to threadIdx.x , but also other CUDA dimensions like blockIdx.x . To achieve this, you either parallelize different loops in a loop nests to different CUDA dimensions, or split your loops before parallelizing them. As for the with ft.GPU(0) scope, ft.GPU(0) specifies a Device (a specific hardware device of GPU). By calling with on a device, default values of several classes and functions are set, but currently you only need to be aware of two things: It sets the Device of optimize . It sets the default mtype of all tensors in the program, which is an optional parameter of ft.Var , ft.empty , etc. mtype refers to memory type. It controls where a tensor is stored. It defaults to \"cpu\" for a CPU program, and \"gpu/global\" for a GPU program. You probably GPU requires putting each variable to a right place (global memory, shared memory, registers, etc.), and this can be done by setting mtype s of each tensor. There are several ways to set mtype s: (Recommended) Leave them to the default \"gpu/global\" first, and modify them with the set_mem_type schedule. In this way, you write some architecture-dependent schedules, but keep your function architecture-independent. (Experimental) Leave them to the default \"gpu/global\" first, and modify them automatically using auto_schedule , or the auto_set_mem_type schedule (which is a part of auto_schedule ). Set them explicitly in the program by setting an optional mtype parameter of ft.Var , ft.empty , etc.","title":"Example: Vector addition on a GPU"},{"location":"guide/gpu/#mtypebyvalue-for-dynamic-tensor-shapes","text":"Tensors with normal mtypes ( \"cpu\" , \"gpu/global\" , etc.) are passed by references, which means a \"cpu\" tensor can only be accessed from a CPU, and a \"gpu/global\" tensor can only be accessed from a GPU. However, sometimes, and especially for dynamic tensor shapes, we want the shapes to be passed by values, and accessible from both CPUs and GPUs (remember we need tensor's shape both when launching a kernel from the CPU side, and during actual computatoin on the GPU side). In this case, we can set the shape-related tensors a \"byvalue\" mtype , and here is an example: import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize(\"Li\", \"threadIdx.x\")) # Use \"byvalue\" for `n` so it can be used both during kernel launching # and inside a kernel def test(n: ft.Var[(), \"int32\", \"input\", \"byvalue\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li # Label the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y)","title":"mtype=\"byvalue\" for Dynamic Tensor Shapes"},{"location":"guide/hint/","text":"Optimize a Program with Hints \u00b6 Types with Sign Information Provide Information by Assertions Expressions and statements in a program do not always provide enough mathematical information for the compiler. Since the compiler must ensure safety for all possible cases, optimizations might be missed. In FreeTensor, you may provide additional information in some ways to guide the compiler, in other words, guide FreeTensor. Types with Sign Information \u00b6 Suppose you are filling a n by m matrix from 0 to n * m - 1 in row-major order, you may loop from 0 to n * m - 1 with a single loop, and the iterator i by m to find each element in the i // m -th row and i % m -th column: y = ft.empty((n, m), \"int32\") for i in range(n * m): y[i // m, i % m] = i Definitely there are other solutions, for example using two loops, but we are going to use the single-loop program to show a common but unobvious performance pitfall: Integer division in Python (including in FreeTensor) rounds to negative infinity, but integer division in most target instructions and target languages like C++ or CUDA rounds to 0. There is only a difference when dividend is negative, but compiling a general Python division to target architectures has to involve an extra branch to check it. In our example, n and m refer to the shape of a matrix, so it cannot be negative. If we can hint FreeTensor, we can avoid the redundant branch. FreeTensor supports adding a suffix to the data type string to show the sign of a number. Simply changing \"int32\" to \"int32>=0\" will make a difference. All supported suffices are \">0\" , >=0 , <0 , <=0 , !=0 and ==0 . A complete example is below: import freetensor as ft print(\"Without hint\") @ft.optimize(verbose=1) # `verbose=1` prints the code def test_no_hint(n: ft.Var[(), \"int32\"], m: ft.Var[(), \"int32\"]): y = ft.empty((n, m), \"int32\") for i in range(n * m): y[i // m, i % m] = i return y # You will find `runtime_mod` in the code, which involves additional branching assert \"runtime_mod\" in test_no_hint.native_code() assert \"%\" not in test_no_hint.native_code() print(\"With hint\") @ft.optimize(verbose=1) # `verbose=1` prints the code def test_hint(n: ft.Var[(), \"int32\"], m: ft.Var[(), \"int32>=0\"]): y = ft.empty((n, m), \"int32\") for i in range(n * m): y[i // m, i % m] = i return y # You will find native C++ `%` in the code, which compiles directly to mod # instructions assert \"runtime_mod\" not in test_hint.native_code() assert \"%\" in test_hint.native_code() The sign hint also works for other optimizations. One example is ft.sqrt(x * x) can be automatically optimized to x if x is non-negative. Another example is ft.min(a, b) can be automatically optimized to a if a is negative while b is positive. Provide Information by Assertions \u00b6 Another way to hint FreeTensor is to add some assert statements in the program. In this way, you can add some more precise hints, which reveals mathematical properties among specifc elements, instead of the whole tensor. Here is an example of adding two n -length vectors, and the program is scheduled to execute in parallel. def sch(s): outer, inner = s.split('Li', 32) s.parallelize(outer, 'openmp') @ft.optimize(schedule_callback=sch, verbose=1) def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li for i in range(n): y[i] = a[i] + b[i] return y The algorithm is simple: we use (at most) n // 32 threads, each computing 32 elements. However, if you look the code, you will find the length of the serial loop is not as simple as 32. Instead, it is a complex expression that results in 31 or 32. This is because n is not always divisible by 32 . Suppose in our case, n is really divisible by 32, we can add an assert statement to hint FreeTensor: assert n % 32 == 0 , and the serial loop will have a neat length 32. A complete example is below: import freetensor as ft import re def sch(s): outer, inner = s.split('Li', 32) s.parallelize(outer, 'openmp') @ft.optimize(schedule_callback=sch, verbose=1) def test_no_hint(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li for i in range(n): y[i] = a[i] + b[i] return y # You will not find a 32-length loop assert not re.search(r\".* = 0; .* < 32; .*\\+\\+\", test_no_hint.native_code()) @ft.optimize(schedule_callback=sch, verbose=1) def test_hint(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") assert n % 32 == 0 #! label: Li for i in range(n): y[i] = a[i] + b[i] return y # You will find a 32-length loop assert re.search(r\".* = 0; .* < 32; .*\\+\\+\", test_hint.native_code())","title":"Optimize a Program with Hints"},{"location":"guide/hint/#types-with-sign-information","text":"Suppose you are filling a n by m matrix from 0 to n * m - 1 in row-major order, you may loop from 0 to n * m - 1 with a single loop, and the iterator i by m to find each element in the i // m -th row and i % m -th column: y = ft.empty((n, m), \"int32\") for i in range(n * m): y[i // m, i % m] = i Definitely there are other solutions, for example using two loops, but we are going to use the single-loop program to show a common but unobvious performance pitfall: Integer division in Python (including in FreeTensor) rounds to negative infinity, but integer division in most target instructions and target languages like C++ or CUDA rounds to 0. There is only a difference when dividend is negative, but compiling a general Python division to target architectures has to involve an extra branch to check it. In our example, n and m refer to the shape of a matrix, so it cannot be negative. If we can hint FreeTensor, we can avoid the redundant branch. FreeTensor supports adding a suffix to the data type string to show the sign of a number. Simply changing \"int32\" to \"int32>=0\" will make a difference. All supported suffices are \">0\" , >=0 , <0 , <=0 , !=0 and ==0 . A complete example is below: import freetensor as ft print(\"Without hint\") @ft.optimize(verbose=1) # `verbose=1` prints the code def test_no_hint(n: ft.Var[(), \"int32\"], m: ft.Var[(), \"int32\"]): y = ft.empty((n, m), \"int32\") for i in range(n * m): y[i // m, i % m] = i return y # You will find `runtime_mod` in the code, which involves additional branching assert \"runtime_mod\" in test_no_hint.native_code() assert \"%\" not in test_no_hint.native_code() print(\"With hint\") @ft.optimize(verbose=1) # `verbose=1` prints the code def test_hint(n: ft.Var[(), \"int32\"], m: ft.Var[(), \"int32>=0\"]): y = ft.empty((n, m), \"int32\") for i in range(n * m): y[i // m, i % m] = i return y # You will find native C++ `%` in the code, which compiles directly to mod # instructions assert \"runtime_mod\" not in test_hint.native_code() assert \"%\" in test_hint.native_code() The sign hint also works for other optimizations. One example is ft.sqrt(x * x) can be automatically optimized to x if x is non-negative. Another example is ft.min(a, b) can be automatically optimized to a if a is negative while b is positive.","title":"Types with Sign Information"},{"location":"guide/hint/#provide-information-by-assertions","text":"Another way to hint FreeTensor is to add some assert statements in the program. In this way, you can add some more precise hints, which reveals mathematical properties among specifc elements, instead of the whole tensor. Here is an example of adding two n -length vectors, and the program is scheduled to execute in parallel. def sch(s): outer, inner = s.split('Li', 32) s.parallelize(outer, 'openmp') @ft.optimize(schedule_callback=sch, verbose=1) def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li for i in range(n): y[i] = a[i] + b[i] return y The algorithm is simple: we use (at most) n // 32 threads, each computing 32 elements. However, if you look the code, you will find the length of the serial loop is not as simple as 32. Instead, it is a complex expression that results in 31 or 32. This is because n is not always divisible by 32 . Suppose in our case, n is really divisible by 32, we can add an assert statement to hint FreeTensor: assert n % 32 == 0 , and the serial loop will have a neat length 32. A complete example is below: import freetensor as ft import re def sch(s): outer, inner = s.split('Li', 32) s.parallelize(outer, 'openmp') @ft.optimize(schedule_callback=sch, verbose=1) def test_no_hint(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li for i in range(n): y[i] = a[i] + b[i] return y # You will not find a 32-length loop assert not re.search(r\".* = 0; .* < 32; .*\\+\\+\", test_no_hint.native_code()) @ft.optimize(schedule_callback=sch, verbose=1) def test_hint(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") assert n % 32 == 0 #! label: Li for i in range(n): y[i] = a[i] + b[i] return y # You will find a 32-length loop assert re.search(r\".* = 0; .* < 32; .*\\+\\+\", test_hint.native_code())","title":"Provide Information by Assertions"},{"location":"guide/schedules/","text":"Optimize a Program with Schedules \u00b6 Example: Parallel Vector addition Combining Multiple Schdules Specify What to Schedule by Selectors Auto Scheduling (Experimental) Oftentimes, only compiling your programs to native code is not enough, and you need further optimizations. This can be done by applying \"schedules\" (explicit program transformations) to you program. Example: Parallel Vector addition \u00b6 import freetensor as ft import numpy as np n = 4 # Add verbose=1 to see the resulting native code @ft.optimize(schedule_callback=lambda s: s.parallelize('Li', 'openmp') ) # <-- 2. Apply the schedule def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li # <-- 1. Label the loop as Li for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Here is an example of a parallel vector addition executed with OpenMP multithreading. Each element is computed by one thread. To achieve this, there are two steps: Label the loop to be parallelized with a #! label: comment. Here label refers to label of an AST node, which is not required to be unique. Apply a parallelize schedule to Li in the schedule_callback argument to optimize ; since the Li label is unambiguous here, the only Li loop is selectd and parallelized. And you are done. You can have a look at the generated OpenMP multithreaded code by setting verbose=1 . Parameter s in schedule_callback is a Schedule object. Besides parallelize , there are more supported scheduling primitives. If you are using the @optimize_to_pytorch integration , you need to set schedules for the forward pass and the backward pass separately. Combining Multiple Schdules \u00b6 Some optimizations can be done by applying multiple schedules. For example, a tiled matrix-multiplication can be done by first split the loops, then reorder them, and finally apply cache s to create tile tensors. In order to demonstrate the idea, we show a simplier example here: still a vector addtion, but with the loop split and only the outer one parallelize d. Please note that this is an example only for demonstration. Usually you do not need it because OpenMP has its own \"schedule(static)\" for parallelized loops. import freetensor as ft import numpy as np n = 1024 def sch(s): outer, inner = s.split('Li', 32) s.parallelize(outer, 'openmp') # Set verbose=1 to see the resulting native code # Set verbose=2 to see the code after EVERY schedule @ft.optimize(schedule_callback=sch) def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(np.arange(1024), dtype=\"int32\"), np.array(np.arange(1024), dtype=\"int32\")).numpy() print(y) One important thing is to track labels of the loops, because the labels will change after schedules. You get labels (to be precise, IDs, which is can be looked-up by labels) of new loops generated from one schedule from its return values ( outer and inner in this case), and pass them to a next schedule. Specify What to Schedule by Selectors \u00b6 In the example above, we label a loop Li and apply schedules on it. It is straight-forward in a tiny example, but as programs grow, it often gets hard to track each statement by a unique label, especially there are inlined function calls. To make things easy, FreeTensor supports specifying a statement by a selector, written in the following rules: A label is a selector. E.g., Li matches a statement with a label Li . (For debugging only) A numerical ID is also a selector. E.g., #31 . A node type surrounded in angle brackets ( <> ) is also a selector. E.g., <For> matches for-loop statements. A selector can be extended to match a new statement produced by a previous schedule. E.g., $split.0{Li} matches the outer loop split from the loop Li . This is useful when return values from schedules are hard to track. Please refer the API document for detailed grammar. Selectors can be combined to match a statement by nesting order. A<-B matches a statement A DIRECTLY NESTED IN another statement B . A<<-B matches a statement DIRECTLY or INDIRECTLY nested in another statement B . A<-(B<-)*C matches a statement A DIRECTLY or INDIRECTLY nested in another statement C with intermedaite nesting statements satisfying the condition in B . B->A matches a statement B directly OUT OF another statement A . B->>A and C->(B->)*A are alike. ( A , B , C can be nested selectors.) Use <-| for the root node, and ->| for a leaf node. Selectors can be combined to match a statement by DFS order. A<:B matches a statement A DIRECTLY BEFORE another statement B . A<<:B matches a statement A DIRECTLY or INDIRECTLY before another statement B . B:>A matches a statment B directly AFTER another statement A . B:>>A matches a statement B directly or indirectly after another statement A . Selectors can be combined to match a statement in a function call. A<~B matches a statement A DIRECTLY called by a call site B . A<<~B matches a statement DIRECTLY or INDIRECTLY called by a call site B . A<~(B<~)*C matches a statement A DIRECTLY or INDIRECTLY called by a call site C with intermediate call sites satisfying the condition in B . ( A , B , C can be nested selectors.) Use <~| for the root function. All the arrow-like selectors ( <- , <~ , <: , etc.) are right-associated. For example, A<-B<-C matches A nested in B , where B is nested in C . All the arrow-like selectors can be used with the first argument omitted. For example, <-B matches ALL statements nested in B . Selectors can be combined with logical \"and\" ( & ), \"or\" ( | ), \"not\" ( ! ) and parentheses. E.g., Li|Lj matches a statement labeled Li OR Lj . Li&Lj matches a statement labeled Li&Lj . All schedules support passing selectors. Auto Scheduling (Experimental) \u00b6 Manually scheduling a program requires a lot of efforts. We provide an experimental automatic scheduling functions in Schedule . You can call s.auto_schedule to pick schedules fully automatically. s.auto_schedule calls other s.auto_xxxxxx functions internally, you can also call one or some of them instead. Please note that these auto-scheduling functions are experimental, and their API is subject to changes.","title":"Optimize a Program with Schedules"},{"location":"guide/schedules/#example-parallel-vector-addition","text":"import freetensor as ft import numpy as np n = 4 # Add verbose=1 to see the resulting native code @ft.optimize(schedule_callback=lambda s: s.parallelize('Li', 'openmp') ) # <-- 2. Apply the schedule def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li # <-- 1. Label the loop as Li for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Here is an example of a parallel vector addition executed with OpenMP multithreading. Each element is computed by one thread. To achieve this, there are two steps: Label the loop to be parallelized with a #! label: comment. Here label refers to label of an AST node, which is not required to be unique. Apply a parallelize schedule to Li in the schedule_callback argument to optimize ; since the Li label is unambiguous here, the only Li loop is selectd and parallelized. And you are done. You can have a look at the generated OpenMP multithreaded code by setting verbose=1 . Parameter s in schedule_callback is a Schedule object. Besides parallelize , there are more supported scheduling primitives. If you are using the @optimize_to_pytorch integration , you need to set schedules for the forward pass and the backward pass separately.","title":"Example: Parallel Vector addition"},{"location":"guide/schedules/#combining-multiple-schdules","text":"Some optimizations can be done by applying multiple schedules. For example, a tiled matrix-multiplication can be done by first split the loops, then reorder them, and finally apply cache s to create tile tensors. In order to demonstrate the idea, we show a simplier example here: still a vector addtion, but with the loop split and only the outer one parallelize d. Please note that this is an example only for demonstration. Usually you do not need it because OpenMP has its own \"schedule(static)\" for parallelized loops. import freetensor as ft import numpy as np n = 1024 def sch(s): outer, inner = s.split('Li', 32) s.parallelize(outer, 'openmp') # Set verbose=1 to see the resulting native code # Set verbose=2 to see the code after EVERY schedule @ft.optimize(schedule_callback=sch) def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(np.arange(1024), dtype=\"int32\"), np.array(np.arange(1024), dtype=\"int32\")).numpy() print(y) One important thing is to track labels of the loops, because the labels will change after schedules. You get labels (to be precise, IDs, which is can be looked-up by labels) of new loops generated from one schedule from its return values ( outer and inner in this case), and pass them to a next schedule.","title":"Combining Multiple Schdules"},{"location":"guide/schedules/#specify-what-to-schedule-by-selectors","text":"In the example above, we label a loop Li and apply schedules on it. It is straight-forward in a tiny example, but as programs grow, it often gets hard to track each statement by a unique label, especially there are inlined function calls. To make things easy, FreeTensor supports specifying a statement by a selector, written in the following rules: A label is a selector. E.g., Li matches a statement with a label Li . (For debugging only) A numerical ID is also a selector. E.g., #31 . A node type surrounded in angle brackets ( <> ) is also a selector. E.g., <For> matches for-loop statements. A selector can be extended to match a new statement produced by a previous schedule. E.g., $split.0{Li} matches the outer loop split from the loop Li . This is useful when return values from schedules are hard to track. Please refer the API document for detailed grammar. Selectors can be combined to match a statement by nesting order. A<-B matches a statement A DIRECTLY NESTED IN another statement B . A<<-B matches a statement DIRECTLY or INDIRECTLY nested in another statement B . A<-(B<-)*C matches a statement A DIRECTLY or INDIRECTLY nested in another statement C with intermedaite nesting statements satisfying the condition in B . B->A matches a statement B directly OUT OF another statement A . B->>A and C->(B->)*A are alike. ( A , B , C can be nested selectors.) Use <-| for the root node, and ->| for a leaf node. Selectors can be combined to match a statement by DFS order. A<:B matches a statement A DIRECTLY BEFORE another statement B . A<<:B matches a statement A DIRECTLY or INDIRECTLY before another statement B . B:>A matches a statment B directly AFTER another statement A . B:>>A matches a statement B directly or indirectly after another statement A . Selectors can be combined to match a statement in a function call. A<~B matches a statement A DIRECTLY called by a call site B . A<<~B matches a statement DIRECTLY or INDIRECTLY called by a call site B . A<~(B<~)*C matches a statement A DIRECTLY or INDIRECTLY called by a call site C with intermediate call sites satisfying the condition in B . ( A , B , C can be nested selectors.) Use <~| for the root function. All the arrow-like selectors ( <- , <~ , <: , etc.) are right-associated. For example, A<-B<-C matches A nested in B , where B is nested in C . All the arrow-like selectors can be used with the first argument omitted. For example, <-B matches ALL statements nested in B . Selectors can be combined with logical \"and\" ( & ), \"or\" ( | ), \"not\" ( ! ) and parentheses. E.g., Li|Lj matches a statement labeled Li OR Lj . Li&Lj matches a statement labeled Li&Lj . All schedules support passing selectors.","title":"Specify What to Schedule by Selectors"},{"location":"guide/schedules/#auto-scheduling-experimental","text":"Manually scheduling a program requires a lot of efforts. We provide an experimental automatic scheduling functions in Schedule . You can call s.auto_schedule to pick schedules fully automatically. s.auto_schedule calls other s.auto_xxxxxx functions internally, you can also call one or some of them instead. Please note that these auto-scheduling functions are experimental, and their API is subject to changes.","title":"Auto Scheduling (Experimental)"}]}