{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FreeTensor \u00b6 A language and compiler for irregular tensor programs. GitHub User Guide API Reference Publication License Features by Example \u00b6 Write a simple vector addition with loops that compiles to native code: import freetensor as ft import numpy as np n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) If you are not willing to compile the program once for each different n , you can set n as another function argument (but you may lose some performance). In FreeTensor, all variables are tensors, where scalars are 0-D tensors. import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) assert np.array_equal(y, [3, 5, 7, 9]) If building with CUDA, you can also run the program on a GPU. This time, a \" schedule \" (an explicit program transformation) is needed, and memory types of variables should be properly set. import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize(\"Li\", \"threadIdx.x\")) # Use \"byvalue\" for `n` so it can be used both during kernel launching # and inside a kernel def test(n: ft.Var[(), \"int32\", \"input\", \"byvalue\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li # Name the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Some common tensor operations, including tensor addition (broadcasting is supported), are pre-defined functions in FreeTensor. They are defiend in freetensor.libop , and they can also be invoked using operator overloading. These functions are pure Python functions, which will be inlined into your code, and will enjoy a joint optimization. import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = a + b # Or y = ft.add(a, b) return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) FreeTensor also supports reverse-mode Automatic Differentiation: import freetensor as ft import numpy as np n = 4 def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.zeros((), \"float32\") for i in range(n): y[()] += a[i] * b[i] return y fwd, bwd, input_grads, output_grads = ft.grad(test, ['a', 'b'], [ft.Return()]) fwd = ft.optimize(fwd) bwd = ft.optimize(bwd) a = np.array([0, 1, 2, 3], dtype=\"float32\") b = np.array([3, 2, 1, 0], dtype=\"float32\") y = fwd(a, b) print(y.numpy()) dzdy = np.array(1, dtype='float32') dzda, dzdb = bwd(**{output_grads[ft.Return()]: dzdy})[input_grads['a'], input_grads['b']] print(dzda.numpy()) print(dzdb.numpy())","title":"Home"},{"location":"#freetensor","text":"A language and compiler for irregular tensor programs. GitHub User Guide API Reference Publication License","title":"FreeTensor"},{"location":"#features-by-example","text":"Write a simple vector addition with loops that compiles to native code: import freetensor as ft import numpy as np n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) If you are not willing to compile the program once for each different n , you can set n as another function argument (but you may lose some performance). In FreeTensor, all variables are tensors, where scalars are 0-D tensors. import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) assert np.array_equal(y, [3, 5, 7, 9]) If building with CUDA, you can also run the program on a GPU. This time, a \" schedule \" (an explicit program transformation) is needed, and memory types of variables should be properly set. import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize(\"Li\", \"threadIdx.x\")) # Use \"byvalue\" for `n` so it can be used both during kernel launching # and inside a kernel def test(n: ft.Var[(), \"int32\", \"input\", \"byvalue\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li # Name the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Some common tensor operations, including tensor addition (broadcasting is supported), are pre-defined functions in FreeTensor. They are defiend in freetensor.libop , and they can also be invoked using operator overloading. These functions are pure Python functions, which will be inlined into your code, and will enjoy a joint optimization. import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = a + b # Or y = ft.add(a, b) return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) FreeTensor also supports reverse-mode Automatic Differentiation: import freetensor as ft import numpy as np n = 4 def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.zeros((), \"float32\") for i in range(n): y[()] += a[i] * b[i] return y fwd, bwd, input_grads, output_grads = ft.grad(test, ['a', 'b'], [ft.Return()]) fwd = ft.optimize(fwd) bwd = ft.optimize(bwd) a = np.array([0, 1, 2, 3], dtype=\"float32\") b = np.array([3, 2, 1, 0], dtype=\"float32\") y = fwd(a, b) print(y.numpy()) dzdy = np.array(1, dtype='float32') dzda, dzdb = bwd(**{output_grads[ft.Return()]: dzdy})[input_grads['a'], input_grads['b']] print(dzda.numpy()) print(dzdb.numpy())","title":"Features by Example"},{"location":"api/","text":"Python API \u00b6 core special \u00b6 auto_schedule \u00b6 AutoSchedule ( AutoSchedule ) \u00b6 Source code in freetensor/core/auto_schedule.py class AutoSchedule ( ffi . AutoSchedule ): def __init__ ( self , schedule , target , device , * , population = 64 , explore_ratio = 0.1 , tag = \"\" , min_block_size = 0 , continue_training = False , random_seed = None , rule_set = None , verbose = 0 ): ''' Automatic scheduler Parameters ---------- schedule : Schedule A Schedule object to apply schedules onto target : Target The type of devices to compile to population : int How many programs to test in each iteration explore_ratio : float Portion of random programs in the population. Higher ratio focuses on exploration, while lower ratio focuses on exploitation continue_trianing : bool Continue to train an existing XGBoost model file if found random_seed : Optional[int] Random seed. Setting a deterministic random seed and using a fixed OpenMP thread count (since we are using thread-local random number generators) resulting deterministic pseudo random numbers, but please note that the whole auto-scheduling procedure is still non-deterministic, becuase it measures real performance. Default to a non-deterministic seed rule_set : Optional[set] Explicitly control over what rules to use. None for defualt rules verbose : int Verbosity level. 0 = print nothing, 1 = print tuning progress, 2 = print extra info mation of each rule ''' self . population = population self . n_explore = int ( population * explore_ratio ) self . n_exploit = population - self . n_explore self . model = None self . xgb_params = {} self . save_file_name = tag + \"_xgb.model\" if continue_training and os . path . isfile ( self . save_file_name ): self . model = xgb . Booster () self . model . load_model ( self . save_file_name ) self . verbose = verbose def predict_func ( features ): return self . predict ( features ) def update_func ( features , times ): return self . update ( features , times ) super ( AutoSchedule , self ) . __init__ ( schedule , target , device , predict_func , update_func , tag , min_block_size , random_seed , rule_set , verbose ) def set_params ( self , * args , ** kws ): super ( AutoSchedule , self ) . set_params ( args , kws ) def run ( self , iteration ): for i in range ( iteration ): if self . verbose >= 1 : print ( \"Iteration\" , i ) self . search_one_round ( self . population , self . n_exploit , self . n_explore ) return self . get_best_schedule () def predict ( self , features ): if not self . model : return [ 1 ] * len ( features ) return self . model . predict ( xgb . DMatrix ( np . array ( features ), missing =- 1 )) def update ( self , features , times ): dtrain = xgb . DMatrix ( np . array ( features ), np . array ( times ), missing =- 1 ) self . model = xgb . train ( self . xgb_params , dtrain , xgb_model = self . model ) self . model . save_model ( self . save_file_name ) __init__ ( self , schedule , target , device , * , population = 64 , explore_ratio = 0.1 , tag = '' , min_block_size = 0 , continue_training = False , random_seed = None , rule_set = None , verbose = 0 ) special \u00b6 Automatic scheduler Parameters: schedule ( Schedule ) \u2013 A Schedule object to apply schedules onto target ( Target ) \u2013 The type of devices to compile to population ( int ) \u2013 How many programs to test in each iteration explore_ratio ( float ) \u2013 Portion of random programs in the population. Higher ratio focuses on exploration, while lower ratio focuses on exploitation continue_trianing ( bool ) \u2013 Continue to train an existing XGBoost model file if found random_seed ( Optional[int] ) \u2013 Random seed. Setting a deterministic random seed and using a fixed OpenMP thread count (since we are using thread-local random number generators) resulting deterministic pseudo random numbers, but please note that the whole auto-scheduling procedure is still non-deterministic, becuase it measures real performance. Default to a non-deterministic seed rule_set ( Optional[set] ) \u2013 Explicitly control over what rules to use. None for defualt rules verbose ( int ) \u2013 Verbosity level. 0 = print nothing, 1 = print tuning progress, 2 = print extra info mation of each rule Source code in freetensor/core/auto_schedule.py def __init__ ( self , schedule , target , device , * , population = 64 , explore_ratio = 0.1 , tag = \"\" , min_block_size = 0 , continue_training = False , random_seed = None , rule_set = None , verbose = 0 ): ''' Automatic scheduler Parameters ---------- schedule : Schedule A Schedule object to apply schedules onto target : Target The type of devices to compile to population : int How many programs to test in each iteration explore_ratio : float Portion of random programs in the population. Higher ratio focuses on exploration, while lower ratio focuses on exploitation continue_trianing : bool Continue to train an existing XGBoost model file if found random_seed : Optional[int] Random seed. Setting a deterministic random seed and using a fixed OpenMP thread count (since we are using thread-local random number generators) resulting deterministic pseudo random numbers, but please note that the whole auto-scheduling procedure is still non-deterministic, becuase it measures real performance. Default to a non-deterministic seed rule_set : Optional[set] Explicitly control over what rules to use. None for defualt rules verbose : int Verbosity level. 0 = print nothing, 1 = print tuning progress, 2 = print extra info mation of each rule ''' self . population = population self . n_explore = int ( population * explore_ratio ) self . n_exploit = population - self . n_explore self . model = None self . xgb_params = {} self . save_file_name = tag + \"_xgb.model\" if continue_training and os . path . isfile ( self . save_file_name ): self . model = xgb . Booster () self . model . load_model ( self . save_file_name ) self . verbose = verbose def predict_func ( features ): return self . predict ( features ) def update_func ( features , times ): return self . update ( features , times ) super ( AutoSchedule , self ) . __init__ ( schedule , target , device , predict_func , update_func , tag , min_block_size , random_seed , rule_set , verbose ) set_params ( self , * args , ** kws ) \u00b6 set_params(self: freetensor_ffi.AutoSchedule, args: List[freetensor_ffi.Array], kws: Dict[str, freetensor_ffi.Array] = {}) -> None Source code in freetensor/core/auto_schedule.py def set_params ( self , * args , ** kws ): super ( AutoSchedule , self ) . set_params ( args , kws ) autograd \u00b6 ArgRetDict \u00b6 Look an object using either a function argument or return value's name or its position Source code in freetensor/core/autograd.py class ArgRetDict : ''' Look an object using either a function argument or return value's name or its position ''' def __init__ ( self , func , d ): self . func = func self . d = d def __getitem__ ( self , key ): if type ( key ) is Return : key = key . get_name ( self . func ) return self . d [ key ] def __contains__ ( self , key ): # Python's auto fallback from __getitem__ to __contains__ only works for # integer index if type ( key ) is Return : key = key . get_name ( self . func ) return key in self . d def __str__ ( self ): return str ( self . d ) Return \u00b6 Alias of a return value of a function Return(n) represents the n-th return value (counted from 0) Return() can be used if there is only one return value Source code in freetensor/core/autograd.py class Return : ''' Alias of a return value of a function `Return(n)` represents the n-th return value (counted from 0) `Return()` can be used if there is only one return value ''' def __init__ ( self , n : Optional [ int ] = None ): self . n = n def get_name ( self , func ): assert len ( func . returns ) > 0 , f \" { func . name } has no return value\" if self . n is not None : return func . returns [ self . n ] . name else : assert len ( func . returns ) == 1 , f \" { func . name } has more than one return value, and you need to specify the number of a return value\" return func . returns [ 0 ] . name def __str__ ( self ): return f \"Return( { self . n } )\" grad ( func , requires , provides , tapes =< GradTapeMode . NoReuseOnly : 2 > , tape_in_closure = True , verbose = None ) \u00b6 Reverse mode automatic differentiation It returns a forward function and a backward function. The forward has the same interface of the original function, but it will store some intermediate tensors (the tape) to be reused by the backward function in some global states. The backward function computes the gradients. grad is an out-of-place version. The resulting gradient are returned from the backward function. Parameters: func ( Func ) \u2013 The original function requires ( Sequence[str] ) \u2013 Name of input variables that need gradients provides ( Sequence[Union[str, freetensor.core.autograd.Return]] ) \u2013 Name of output variables whose gradients are known. A return value of a function can be specified with a Return object tapes ( Union[Sequence, freetensor_ffi.GradTapeMode] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure ( bool ) \u2013 True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True Returns: tuple \u2013 ( 0. Forward AST. 1. Backward AST. 2. Mapping from names in requries to its gradient name. 3. Mapping from names in provides to its gradient name. ) Source code in freetensor/core/autograd.py def grad ( func : ffi . Func , requires : Sequence [ str ], provides : Sequence [ Union [ str , Return ]], tapes : Union [ Sequence , GradTapeMode ] = GradTapeMode . NoReuseOnly , tape_in_closure : bool = True , verbose : Optional [ int ] = None ): ''' Reverse mode automatic differentiation It returns a forward function and a backward function. The forward has the same interface of the original function, but it will store some intermediate tensors (the tape) to be reused by the backward function in some global states. The backward function computes the gradients. `grad` is an out-of-place version. The resulting gradient are returned from the backward function. Parameters ---------- func : AST The original function requires : Sequence[str] Name of input variables that need gradients provides : Sequence[Union[str, Return]] Name of output variables whose gradients are known. A return value of a function can be specified with a `Return` object tapes : Union[Sequence, GradTapeMode] Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a `GradTapeMode`, then it will determine which intermediate variables to be stored by heuristics. Avail `GradTapeMode`s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure : bool True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True Returns ------- tuple ( 0. Forward AST. 1. Backward AST. 2. Mapping from names in requries to its gradient name. 3. Mapping from names in provides to its gradient name. ) ''' return _grad_func ( ffi . grad , func , requires , provides , tapes , tape_in_closure , verbose = verbose ) grad_ ( func , requires , provides , tapes =< GradTapeMode . NoReuseOnly : 2 > , tape_in_closure = True , verbose = None ) \u00b6 Reverse mode automatic differentiation It returns a forward function and a backward function. The forward has the same interface of the original function, but it will store some intermediate tensors (the tape) to be reused by the backward function in some global states. The backward function computes the gradients. grad_ is an inplace version. The resulting gradient are mutable arguments of the backward function. Parameters: func ( Func ) \u2013 The original function requires ( Sequence[str] ) \u2013 Name of input variables that need gradients provides ( Sequence[Union[str, freetensor.core.autograd.Return]] ) \u2013 Name of output variables whose gradients are known. A return value of a function can be specified with a Return object tapes ( Union[Sequence, freetensor_ffi.GradTapeMode] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure ( bool ) \u2013 True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True Returns: tuple \u2013 ( 0. Forward AST. 1. Backward AST. 2. Mapping from names in requries to its gradient name. 3. Mapping from names in provides to its gradient name. ) Source code in freetensor/core/autograd.py def grad_ ( func : ffi . Func , requires : Sequence [ str ], provides : Sequence [ Union [ str , Return ]], tapes : Union [ Sequence , GradTapeMode ] = GradTapeMode . NoReuseOnly , tape_in_closure : bool = True , verbose : Optional [ int ] = None ): ''' Reverse mode automatic differentiation It returns a forward function and a backward function. The forward has the same interface of the original function, but it will store some intermediate tensors (the tape) to be reused by the backward function in some global states. The backward function computes the gradients. `grad_` is an inplace version. The resulting gradient are mutable arguments of the backward function. Parameters ---------- func : AST The original function requires : Sequence[str] Name of input variables that need gradients provides : Sequence[Union[str, Return]] Name of output variables whose gradients are known. A return value of a function can be specified with a `Return` object tapes : Union[Sequence, GradTapeMode] Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a `GradTapeMode`, then it will determine which intermediate variables to be stored by heuristics. Avail `GradTapeMode`s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure : bool True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True Returns ------- tuple ( 0. Forward AST. 1. Backward AST. 2. Mapping from names in requries to its gradient name. 3. Mapping from names in provides to its gradient name. ) ''' return _grad_func ( ffi . grad_ , func , requires , provides , tapes , tape_in_closure , verbose = verbose ) grad_body ( stmt , requires , provides , tapes =< GradTapeMode . NoReuseOnly : 2 > ) \u00b6 grad or grad_ on a function body (for internal tests only) Source code in freetensor/core/autograd.py def grad_body ( stmt : ffi . Stmt , requires : Sequence [ Union [ str , Return ]], provides : Sequence [ Union [ str , Return ]], tapes : Union [ Sequence , GradTapeMode ] = GradTapeMode . NoReuseOnly ): ''' `grad` or `grad_` on a function body (for internal tests only) ''' req = set ( requires ) prov = set ( provides ) if type ( tapes ) is not GradTapeMode : tapes = { find_stmt ( stmt , t ) . id for t in tapes } return ffi . grad_body ( stmt , req , prov , tapes ) codegen \u00b6 codegen ( ast = None , target = None , verbose = None ) \u00b6 Generate native code Parameters: ast ( AST ) \u2013 The AST to be lowered. It must includes function signature to determine parameters and return values. If not specified, a partial function is returned, which can be used as a decorator target ( Optional[freetensor_ffi.Target] ) \u2013 The target architecture. If omitted, use the default one in config Source code in freetensor/core/codegen.py def codegen ( ast = None , target : Optional [ ffi . Target ] = None , verbose : Optional [ bool ] = None ) -> NativeCode : ''' Generate native code Parameters ---------- ast : AST The AST to be lowered. It must includes function signature to determine parameters and return values. If not specified, a partial function is returned, which can be used as a decorator target : Target (Optional) The target architecture. If omitted, use the default one in config ''' if ast is not None : if target is None : target = config . default_target () raw_code = ffi . code_gen ( ast , target ) if verbose : print ( debug . with_line_no ( raw_code ), file = sys . stderr ) return NativeCode ( ast , raw_code , target ) else : f = codegen if target is not None : f = functools . partial ( f , target = target ) if verbose is not None : f = functools . partial ( f , verbose = verbose ) return f config \u00b6 Global configurations backend_compiler_cxx ( * args , ** kvs ) \u00b6 backend_compiler_cxx() -> List[str] Backend compiler used to compile generated C++ code Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) backend_compiler_nvcc ( * args , ** kvs ) \u00b6 backend_compiler_nvcc() -> List[str] Backend compiler used to compile generated CUDA code Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) debug_binary ( * args , ** kvs ) \u00b6 debug_binary() -> bool Check if compiling binary in debug mode Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) default_device ( * args , ** kvs ) \u00b6 default_device() -> freetensor_ffi.Device Check current default device Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) default_target ( * args , ** kvs ) \u00b6 default_target() -> freetensor_ffi.Target Check current default target Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) pretty_print ( * args , ** kvs ) \u00b6 pretty_print() -> bool Check if colored printing enabled Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) print_all_id ( * args , ** kvs ) \u00b6 pretty_print() -> bool Check if colored printing enabled Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) set_backend_compiler_cxx ( * args , ** kvs ) \u00b6 set_backend_compiler_cxx(path: List[str]) -> None Set backend compiler used to compile generated C++ code, unescaped raw path expected Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) set_backend_compiler_nvcc ( * args , ** kvs ) \u00b6 set_backend_compiler_nvcc(path: List[str]) -> None Set backend compiler used to compile generated CUDA code, unescaped raw path expected Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) set_debug_binary ( * args , ** kvs ) \u00b6 set_debug_binary(flag: bool = True) -> None Compile with -g at backend. Do not delete the binary file after loaded Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) set_default_device ( * args , ** kvs ) \u00b6 set_default_device(device: freetensor_ffi.Device) -> None Set default device (internal implementation of with Device ) Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) set_default_target ( * args , ** kvs ) \u00b6 set_default_target(target: freetensor_ffi.Target) -> None Set default target (internal implementation of with Target ) Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) set_pretty_print ( * args , ** kvs ) \u00b6 set_pretty_print(flag: bool = True) -> None Set colored printing Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) set_print_all_id ( * args , ** kvs ) \u00b6 set_pretty_print(flag: bool = True) -> None Set colored printing Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) set_werror ( * args , ** kvs ) \u00b6 set_werror(flag: bool = True) -> None Error on warning Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) werror ( * args , ** kvs ) \u00b6 werror() -> bool Check if error-on-warning enabled Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) with_cuda ( * args , ** kvs ) \u00b6 with_cuda() -> bool Check if FreeTensor is built with CUDA Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) with_mkl ( * args , ** kvs ) \u00b6 with_mkl() -> str Check if FreeTensor is built with MKL Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) with_pytorch ( * args , ** kvs ) \u00b6 with_pytorch() -> bool Check if FreeTensor is built with PyTorch interface Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs ) context \u00b6 Facility to pick statements to build an AST Classes and functions in this module are internally used by transformer to construct ASTs. They are also used by some internal tests. API of these classes and functions are subject to changes. End users are encouraged to use transformer , instead of this module. pop_ast ( verbose = False ) \u00b6 Get AST and reset context Internally used by transformer and tests Source code in freetensor/core/context.py def pop_ast ( verbose : bool = False ): \"\"\" Get AST and reset context Internally used by `transformer` and tests \"\"\" ret = ctx_stack . pop () . make_stmt () ctx_stack . reset () if verbose : print ( \"The popped AST is:\" , file = sys . stderr ) print ( ret , file = sys . stderr ) print ( file = sys . stderr ) return ret driver \u00b6 Device ( Device ) \u00b6 A computing device can be constructed from (TargetType, DeviceNumber) (TargetType, getDeviceByName): cuda uses best matches criteria. (TargetType, FullName, nth): get nth(from 0) device named Fullname . E.g. Device(TargetType::GPU, 0) means the 0-th GPU (device) Device(TargetType::GPU, \"V100\") means a GPU which best matches \"V100\" Device(TargetType::GPU, \"NVIDIA GeForce RTX 3060 Laptop GPU\", 0) A Device can be used as a \"with\" scope, then all the Array s and Driver s will use it by default. In this style, it also sets the default Target. E.g: with Device(...): ast = lower(ast) # Use the Target of the Device above by default a = Array(...) # Use the Device above by default Source code in freetensor/core/driver.py class Device ( ffi . Device ): ''' A computing device can be constructed from 1. (TargetType, DeviceNumber) 2. (TargetType, getDeviceByName): cuda uses best matches criteria. 3. (TargetType, FullName, nth): get nth(from 0) device named `Fullname`. E.g. Device(TargetType::GPU, 0) means the 0-th GPU (device) Device(TargetType::GPU, \"V100\") means a GPU which best matches \"V100\" Device(TargetType::GPU, \"NVIDIA GeForce RTX 3060 Laptop GPU\", 0) A Device can be used as a \"with\" scope, then all the `Array`s and `Driver`s will use it by default. In this style, it also sets the default Target. E.g: ``` with Device(...): ast = lower(ast) # Use the Target of the Device above by default a = Array(...) # Use the Device above by default ``` ''' def __enter__ ( self ): _old_target_device_stack . append ( ( config . default_target (), config . default_device ())) config . set_default_target ( self . target ()) config . set_default_device ( self ) return self def __exit__ ( self , exc_type , exc_value , traceback ): old_target , old_device = _old_target_device_stack . pop () config . set_default_target ( old_target ) config . set_default_device ( old_device ) Driver ( Driver ) \u00b6 Source code in freetensor/core/driver.py class Driver ( ffi . Driver ): def __init__ ( self , func : ffi . Func , src : str , device : Optional [ Device ] = None , host_device : Optional [ Device ] = None , verbose : Optional [ bool ] = None ): ''' Compile a program using a backend compiler and load it into memory This class is for internal use. Please consider using `build_binary` Parameters ---------- func : ffi.Func AST of the function, where the function signature is needed to determine the parameters and return values src : str Native code generated from codegen device : Device (Optional) The device to run the program. If omitted, use the default device in config verbose : bool (Optional) True to print extra infomation ''' src = str ( src ) if device is None : device = config . default_device () if verbose is None : verbose = False if host_device is None : super ( Driver , self ) . __init__ ( func , src , device , verbose ) else : super ( Driver , self ) . __init__ ( func , src , device , host_device , verbose ) self . func = func # When we pass numpy or pytorch tensors to `set_args`, they are # converted to `Array` objects by reference. In `Array`'s FFI, we # keep these tensors alive whenever the `Array`'s PYTHON objects # alive. We need to also keep the `Array`'s PYTHON objects here. # Please note that we cannot hold the reference count in `Driver`'s # C++ implementation, where we can only hold the `Array`'s C++ # objects alive. self . args_ref_cnt_holder = [] def set_args ( self , * args , ** kws ): ''' Set argument for an invocation ''' # No need to hold reference of the last run any more self . args_ref_cnt_holder = [] args = list ( args ) kws = dict ( kws ) for i in range ( len ( args )): args [ i ] = array ( args [ i ]) for key in kws : kws [ key ] = array ( kws [ key ]) for arg in args : self . args_ref_cnt_holder . append ( arg ) for key in kws : self . args_ref_cnt_holder . append ( kws [ key ]) super ( Driver , self ) . set_args ( args , kws ) def collect_returns ( self , always_return_pack : bool = False ): ''' Collect return values from an invocation Return values must be collect. Otherwise there will be memory leaks If there is only one return value, it is returned directly. Otherwise, or if `always_return_pack` is set, the return values are packed in a ReturnValuesPack ''' values = super ( Driver , self ) . collect_returns () if len ( values ) == 0 and not always_return_pack : return None elif len ( values ) == 1 and not always_return_pack : return values [ 0 ] else : return ReturnValuesPack ( map ( lambda r : r . name , filter ( lambda r : not r . is_in_closure or r . return_closure , self . func . returns )), values ) def __call__ ( self , * args , ** kws ): ''' Set argument, execute the binary code, and collect the returns If there is only one return value, it is returned directly. Otherwise, the return values are packed in a ReturnValuesPack This function will introduce some overhaed handling arguments and return values. For an accurate execution time measurement, plase call `self.set_args` first, then `self.time`, and finally `self.collect_returns` ''' self . set_args ( * args , ** kws ) self . run () return self . collect_returns () __call__ ( self , * args , ** kws ) special \u00b6 Set argument, execute the binary code, and collect the returns If there is only one return value, it is returned directly. Otherwise, the return values are packed in a ReturnValuesPack This function will introduce some overhaed handling arguments and return values. For an accurate execution time measurement, plase call self.set_args first, then self.time , and finally self.collect_returns Source code in freetensor/core/driver.py def __call__ ( self , * args , ** kws ): ''' Set argument, execute the binary code, and collect the returns If there is only one return value, it is returned directly. Otherwise, the return values are packed in a ReturnValuesPack This function will introduce some overhaed handling arguments and return values. For an accurate execution time measurement, plase call `self.set_args` first, then `self.time`, and finally `self.collect_returns` ''' self . set_args ( * args , ** kws ) self . run () return self . collect_returns () __init__ ( self , func , src , device = None , host_device = None , verbose = None ) special \u00b6 Compile a program using a backend compiler and load it into memory This class is for internal use. Please consider using build_binary Parameters: func ( Func ) \u2013 AST of the function, where the function signature is needed to determine the parameters and return values src ( str ) \u2013 Native code generated from codegen device ( Optional[freetensor.core.driver.Device] ) \u2013 The device to run the program. If omitted, use the default device in config verbose ( Optional[bool] ) \u2013 True to print extra infomation Source code in freetensor/core/driver.py def __init__ ( self , func : ffi . Func , src : str , device : Optional [ Device ] = None , host_device : Optional [ Device ] = None , verbose : Optional [ bool ] = None ): ''' Compile a program using a backend compiler and load it into memory This class is for internal use. Please consider using `build_binary` Parameters ---------- func : ffi.Func AST of the function, where the function signature is needed to determine the parameters and return values src : str Native code generated from codegen device : Device (Optional) The device to run the program. If omitted, use the default device in config verbose : bool (Optional) True to print extra infomation ''' src = str ( src ) if device is None : device = config . default_device () if verbose is None : verbose = False if host_device is None : super ( Driver , self ) . __init__ ( func , src , device , verbose ) else : super ( Driver , self ) . __init__ ( func , src , device , host_device , verbose ) self . func = func # When we pass numpy or pytorch tensors to `set_args`, they are # converted to `Array` objects by reference. In `Array`'s FFI, we # keep these tensors alive whenever the `Array`'s PYTHON objects # alive. We need to also keep the `Array`'s PYTHON objects here. # Please note that we cannot hold the reference count in `Driver`'s # C++ implementation, where we can only hold the `Array`'s C++ # objects alive. self . args_ref_cnt_holder = [] collect_returns ( self , always_return_pack = False ) \u00b6 Collect return values from an invocation Return values must be collect. Otherwise there will be memory leaks If there is only one return value, it is returned directly. Otherwise, or if always_return_pack is set, the return values are packed in a ReturnValuesPack Source code in freetensor/core/driver.py def collect_returns ( self , always_return_pack : bool = False ): ''' Collect return values from an invocation Return values must be collect. Otherwise there will be memory leaks If there is only one return value, it is returned directly. Otherwise, or if `always_return_pack` is set, the return values are packed in a ReturnValuesPack ''' values = super ( Driver , self ) . collect_returns () if len ( values ) == 0 and not always_return_pack : return None elif len ( values ) == 1 and not always_return_pack : return values [ 0 ] else : return ReturnValuesPack ( map ( lambda r : r . name , filter ( lambda r : not r . is_in_closure or r . return_closure , self . func . returns )), values ) set_args ( self , * args , ** kws ) \u00b6 Set argument for an invocation Source code in freetensor/core/driver.py def set_args ( self , * args , ** kws ): ''' Set argument for an invocation ''' # No need to hold reference of the last run any more self . args_ref_cnt_holder = [] args = list ( args ) kws = dict ( kws ) for i in range ( len ( args )): args [ i ] = array ( args [ i ]) for key in kws : kws [ key ] = array ( kws [ key ]) for arg in args : self . args_ref_cnt_holder . append ( arg ) for key in kws : self . args_ref_cnt_holder . append ( kws [ key ]) super ( Driver , self ) . set_args ( args , kws ) ReturnValuesPack \u00b6 Hold return values from a Driver invocation Return values can be retrieved in an anonymous manner: x, y, z = pack , or in a named manner: pack['x'] Please note that a ReturnValuesPack is different from a OrderedDict, as OrderedDict unpacks to keys rather than values Source code in freetensor/core/driver.py class ReturnValuesPack : ''' Hold return values from a Driver invocation Return values can be retrieved in an anonymous manner: `x, y, z = pack`, or in a named manner: `pack['x']` Please note that a ReturnValuesPack is different from a OrderedDict, as OrderedDict unpacks to keys rather than values ''' def __init__ ( self , keys : Sequence [ str ], values : Sequence [ Array ]): keys = list ( keys ) values = list ( values ) assert len ( keys ) == len ( values ) self . keys = keys self . values = values def __iter__ ( self ): ''' Get all return values in the order declared in Func ''' yield from self . values def __getitem__ ( self , key ) -> Array : ''' Get a return value with a name. Tuple is supported for multiple values ''' if type ( key ) is tuple or type ( key ) is list : ret = [] for k in key : ret . append ( self [ k ]) return ret for k , v in zip ( self . keys , self . values ): if k == key : return v raise ffi . DriverError ( \"No such return value named \" + key ) def __contains__ ( self , key ): ''' Test if a return value exists ''' for k , v in zip ( self . keys , self . values ): if k == key : return True return False __contains__ ( self , key ) special \u00b6 Test if a return value exists Source code in freetensor/core/driver.py def __contains__ ( self , key ): ''' Test if a return value exists ''' for k , v in zip ( self . keys , self . values ): if k == key : return True return False __getitem__ ( self , key ) special \u00b6 Get a return value with a name. Tuple is supported for multiple values Source code in freetensor/core/driver.py def __getitem__ ( self , key ) -> Array : ''' Get a return value with a name. Tuple is supported for multiple values ''' if type ( key ) is tuple or type ( key ) is list : ret = [] for k in key : ret . append ( self [ k ]) return ret for k , v in zip ( self . keys , self . values ): if k == key : return v raise ffi . DriverError ( \"No such return value named \" + key ) __iter__ ( self ) special \u00b6 Get all return values in the order declared in Func Source code in freetensor/core/driver.py def __iter__ ( self ): ''' Get all return values in the order declared in Func ''' yield from self . values array ( data ) \u00b6 Factory function for Array It converts more data format to Array Source code in freetensor/core/driver.py def array ( data ): ''' Factory function for Array It converts more data format to Array ''' if type ( data ) is Array : return data # For NumPy, Although Pybind11's `array_t` type provides a flag `forcecast` to # cast from a strided array to a contiguous one. But it always casts to a specific # type, e.g. float64. I have no idea how to support multiple types. Therfore, # we have to call NumPy's `.copy(order='C')` to make a new NumPy array. This # function can only be called from Python side (not from PyBind11's `py::array` # type). if type ( data ) is np . ndarray : if not data . flags [ 'C_CONTIGUOUS' ]: data = data . copy ( order = 'C' ) return Array ( data ) if data . __class__ . __module__ == 'torch' : import torch if type ( data ) is torch . Tensor : if not config . with_pytorch (): raise ffi . DriverError ( \"FreeTensor should be built with WITH_PYTORCH to accept a PyTorch tensor\" ) if not data . is_contiguous (): data = data . contiguous () return Array ( data ) raise ffi . DriverError ( f \"Unsupported data type { type ( data ) } for Array\" ) build_binary ( code = None , device = None , host_device = None , verbose = None ) \u00b6 Compile a program using a backend compiler and load it into memory Parameters: code ( Optional[freetensor.core.codegen.NativeCode] ) \u2013 Native code generated by codegen . If not specified, a partial function is returned, which can be used as a decorator device ( Optional[freetensor.core.driver.Device] ) \u2013 The device to run the program. If omitted, use the default device in config Source code in freetensor/core/driver.py def build_binary ( code : Optional [ NativeCode ] = None , device : Optional [ Device ] = None , host_device : Optional [ Device ] = None , verbose : Optional [ bool ] = None ): ''' Compile a program using a backend compiler and load it into memory Parameters ---------- code : NativeCode Native code generated by `codegen`. If not specified, a partial function is returned, which can be used as a decorator device : Device (Optional) The device to run the program. If omitted, use the default device in config ''' if code is not None : if device is None : device = config . default_device () if device . target () != code . target : raise ffi . DriverError ( f \"Codegen target ( { code . target } ) is inconsistent with device target ( { device . target () } )\" ) return Driver ( code . func , code . code , device , host_device , verbose ) else : f = build_binary if device is not None : f = functools . partial ( f , device = device ) if host_device is not None : f = functools . partial ( f , host_device = host_device ) if verbose is not None : f = functools . partial ( f , verbose = verbose ) return f expr \u00b6 Facility to build AST expressions Classes and functions in this module are not only used internally for constructing AST nodes, and also exposed to users via multi-stage programming AlreadyMadeReduceTo \u00b6 A single-value type that marks a ReduceTo node is already made, and there is no need to make another Store node In standard Python data model, functions like iadd returns the modified self, and setitem does a self-assignment. We do the augmenting assignment directly in iadd and return AlreadyMadeReduceTo, so we do not have to Store it again Source code in freetensor/core/expr.py class AlreadyMadeReduceTo : \"\"\" A single-value type that marks a ReduceTo node is already made, and there is no need to make another Store node In standard Python data model, functions like __iadd__ returns the modified self, and __setitem__ does a self-assignment. We do the augmenting assignment directly in __iadd__ and return AlreadyMadeReduceTo, so we do not have to Store it again \"\"\" pass VarRef ( FrontendVar ) \u00b6 Variable of FreeTensor All variables in FreeTensor DSL (declared via Var , created by empty or var , returned by libop , etc.), and their slices, are VarRef objects. Operations on VarRef objects generates AST nodes Source code in freetensor/core/expr.py class VarRef ( ffi . FrontendVar ): ''' Variable of FreeTensor All variables in FreeTensor DSL (declared via `Var`, created by `empty` or `var`, returned by `libop`, etc.), and their slices, are `VarRef` objects. Operations on `VarRef` objects generates AST nodes ''' def __init__ ( self , name : str , vardef , full_shape : Sequence , dtype : ffi . DataType , mtype : ffi . MemType , indices : Sequence = []): super ( VarRef , self ) . __init__ ( name , full_shape , dtype , mtype , indices ) self . vardef = vardef from .stmt import find_borrowed_vardefs self . borrowed_vardefs = find_borrowed_vardefs ( indices ) for item in self . borrowed_vardefs : item . lend_out () def __del__ ( self ): for item in self . borrowed_vardefs : item . reclaim () def __getitem__ ( self , key ): return VarRef ( self . name , self . vardef , self . full_shape , self . dtype , self . mtype , self . chain_indices ( self . _parse_key ( key ))) def __setitem__ ( self , key , value ): var = VarRef ( self . name , self . vardef , self . full_shape , self . dtype , self . mtype , self . chain_indices ( self . _parse_key ( key ))) if var . ndim > 0 : if value is AlreadyMadeReduceTo : return from .. import libop libop . assign ( var , value ) return if var . vardef . atype == ffi . AccessType ( \"input\" ): raise ffi . InvalidProgram ( \"Cannot modify an \\\" input \\\" tensor `\" + self . name + \"`\" ) if var . vardef . borrower_cnt > 0 : raise ffi . InvalidProgram ( \"Cannot modify tensor `\" + self . name + \"` becuase it has been borrowed in another tensor's shape, \" \"a tensor slice, or a range of a loop\" ) if value is AlreadyMadeReduceTo : # Following the checks above return top = ctx_stack . top () top . append_stmt ( var . as_store ( top . get_metadata (), value )) def as_store ( self , metadata , value ): if ( not isinstance ( value , ffi . AnyExpr ) and ffi . up_cast ( dtype ( value ), self . vardef . dtype ) != self . vardef . dtype ): # Add explicit cast node, to avoid confusion after propagation value = cast ( value , self . vardef . dtype ) return super ( VarRef , self ) . as_store ( metadata , value ) def as_reduce_to ( self , reduce_op , metadata , value , atomic = False ): if ( not isinstance ( value , ffi . AnyExpr ) and ffi . up_cast ( dtype ( value ), self . vardef . dtype ) != self . vardef . dtype ): # Add explicit cast node, to avoid confusion after propagation value = cast ( value , self . vardef . dtype ) return super ( VarRef , self ) . as_reduce_to ( reduce_op , metadata , value , atomic ) def select ( self , idx , dim ): assert isinstance ( dim , int ) assert dim >= 0 and dim < self . ndim indices = [ slice ( None , None ) if d != dim else idx for d in range ( self . ndim ) ] return self [ indices ] def shape ( self , dim = None ): ''' Return lengths of all dimensions or the length of one dimension `.shape()` -> list of lengths of all dimensions `.shape(dim)` -> length of dimension `dim`, where `dim` can be `int` or `Expr` All lengths can be `Expr` (if the length is dynamically decided) or `int` (if statically decided) ''' intOrExpr = lambda x : x . val if isinstance ( x , ffi . IntConst ) else x if dim is None : return [ intOrExpr ( d ) for d in super ( VarRef , self ) . shape ()] else : return intOrExpr ( super ( VarRef , self ) . shape ( dim )) def _parse_key ( self , key ): if key is None or key is ... : key = () if not isinstance ( key , collections . abc . Sequence ): key = ( key ,) ffiIdx = [] for idx , length in zip ( key , self . shape ()): if isinstance ( idx , slice ): start = idx . start if idx . start is not None else 0 stop = idx . stop if idx . stop is not None else length assert idx . step is None or idx . step == 1 ffiIdx . append ( ffi . FrontendVarIdx ( start , stop )) elif isinstance ( idx , VarRef ): if len ( idx . full_shape ) == len ( idx . indices ): ffiIdx . append ( ffi . FrontendVarIdx ( idx . as_load ())) else : assert len ( key ) == 1 , f \"Shape of an index of { self . name } should be 1-D, instead of { idx . name } \" assert type ( idx . full_shape [ 0 ] ) is ffi . IntConst , \"Dynamic number of dimensions is not supported\" ndim = idx . full_shape [ 0 ] . val ffiIdx += [ ffi . FrontendVarIdx ( idx [ i ] . as_load ()) for i in range ( ndim ) ] else : ffiIdx . append ( ffi . FrontendVarIdx ( idx )) return ffiIdx def __add__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . add ( self , other ) return self . as_load () + other def __radd__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . add ( other , self ) return other + self . as_load () def __iadd__ ( self , other ): if self . ndim > 0 : from .. import libop libop . add_to ( self , other ) return AlreadyMadeReduceTo top = ctx_stack . top () top . append_stmt ( self . as_reduce_to ( ffi . ReduceOp . Add , top . get_metadata (), other )) return AlreadyMadeReduceTo def __sub__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . sub ( self , other ) return self . as_load () - other def __rsub__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . sub ( other , self ) return other - self . as_load () def __isub__ ( self , other ): if self . ndim > 0 : from .. import libop libop . sub_to ( self , other ) return AlreadyMadeReduceTo top = ctx_stack . top () top . append_stmt ( self . as_reduce_to ( ffi . ReduceOp . Sub , top . get_metadata (), other )) return AlreadyMadeReduceTo def __mul__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . mul ( self , other ) return self . as_load () * other def __rmul__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . mul ( other , self ) return other * self . as_load () def __imul__ ( self , other ): if self . ndim > 0 : from .. import libop libop . mul_to ( self , other ) return AlreadyMadeReduceTo top = ctx_stack . top () top . append_stmt ( self . as_reduce_to ( ffi . ReduceOp . Mul , top . get_metadata (), other )) return AlreadyMadeReduceTo def __truediv__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . truediv ( self , other ) return self . as_load () / other def __rtruediv__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . truediv ( other , self ) return other / self . as_load () def __itruediv__ ( self , other ): if self . ndim > 0 : from .. import libop libop . truediv_to ( self , other ) return AlreadyMadeReduceTo return NotImplemented # Fallback to x = x / y def __floordiv__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . floordiv ( self , other ) return self . as_load () // other def __rfloordiv__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . floordiv ( other , self ) return other // self . as_load () def __ifloordiv__ ( self , other ): if self . ndim > 0 : from .. import libop libop . floordiv_to ( self , other ) return AlreadyMadeReduceTo return NotImplemented # Fallback to x = x // y def __mod__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . mod ( self , other ) return self . as_load () % other def __rmod__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . mod ( other , self ) return other % self . as_load () def __imod__ ( self , other ): if self . ndim > 0 : from .. import libop libop . mod_to ( self , other ) return AlreadyMadeReduceTo return NotImplemented # Fallback to x = x % y def __lt__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . lt ( self , other ) return self . as_load () < other def __le__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . le ( self , other ) return self . as_load () <= other def __gt__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . gt ( self , other ) return self . as_load () > other def __ge__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . ge ( self , other ) return self . as_load () >= other def __eq__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . eq ( self , other ) return self . as_load () == other def __ne__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . ne ( self , other ) return self . as_load () != other def __neg__ ( self ): if self . ndim > 0 : from .. import libop return libop . neg ( self ) return 0 - self . as_load () def __matmul__ ( self , other ): from .. import libop return libop . matmul ( self , other ) def __rmatmul__ ( self , other ): from .. import libop return libop . matmul ( other , self ) as_reduce_to ( self , reduce_op , metadata , value , atomic = False ) \u00b6 as_reduce_to(self: freetensor_ffi.FrontendVar, op: freetensor_ffi.ReduceOp, metadata: freetensor_ffi.Metadata, value: freetensor_ffi.Expr, atomic: bool = False) -> freetensor_ffi.Stmt Source code in freetensor/core/expr.py def as_reduce_to ( self , reduce_op , metadata , value , atomic = False ): if ( not isinstance ( value , ffi . AnyExpr ) and ffi . up_cast ( dtype ( value ), self . vardef . dtype ) != self . vardef . dtype ): # Add explicit cast node, to avoid confusion after propagation value = cast ( value , self . vardef . dtype ) return super ( VarRef , self ) . as_reduce_to ( reduce_op , metadata , value , atomic ) as_store ( self , metadata , value ) \u00b6 as_store(self: freetensor_ffi.FrontendVar, metadata: freetensor_ffi.Metadata, value: freetensor_ffi.Expr) -> freetensor_ffi.Stmt Source code in freetensor/core/expr.py def as_store ( self , metadata , value ): if ( not isinstance ( value , ffi . AnyExpr ) and ffi . up_cast ( dtype ( value ), self . vardef . dtype ) != self . vardef . dtype ): # Add explicit cast node, to avoid confusion after propagation value = cast ( value , self . vardef . dtype ) return super ( VarRef , self ) . as_store ( metadata , value ) shape ( self , dim = None ) \u00b6 Return lengths of all dimensions or the length of one dimension .shape() -> list of lengths of all dimensions .shape(dim) -> length of dimension dim , where dim can be int or Expr All lengths can be Expr (if the length is dynamically decided) or int (if statically decided) Source code in freetensor/core/expr.py def shape ( self , dim = None ): ''' Return lengths of all dimensions or the length of one dimension `.shape()` -> list of lengths of all dimensions `.shape(dim)` -> length of dimension `dim`, where `dim` can be `int` or `Expr` All lengths can be `Expr` (if the length is dynamically decided) or `int` (if statically decided) ''' intOrExpr = lambda x : x . val if isinstance ( x , ffi . IntConst ) else x if dim is None : return [ intOrExpr ( d ) for d in super ( VarRef , self ) . shape ()] else : return intOrExpr ( super ( VarRef , self ) . shape ( dim )) abs ( expr ) \u00b6 Absolute value For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.abs Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The absolute value Source code in freetensor/core/expr.py def abs ( expr ): ''' Absolute value For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.abs Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The absolute value ''' if _istensor ( expr ): from .. import libop return libop . abs ( expr ) if isinstance ( expr , Number ): return builtins . abs ( expr ) return ffi . makeAbs ( expr ) add ( lhs , rhs ) \u00b6 lhs + rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.add Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The sum Source code in freetensor/core/expr.py def add ( lhs , rhs ): ''' `lhs + rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.add Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The sum ''' return lhs + rhs any () \u00b6 Create an AnyExpr node (only for testing) Any nodes matches any expression nodes in ast.match Source code in freetensor/core/expr.py def any (): ''' Create an AnyExpr node (only for testing) Any nodes matches any expression nodes in `ast.match` ''' return ffi . makeAnyExpr () cast ( expr , dtype ) \u00b6 Cast to another type Parameters: expr ( VarRef or Number ) \u2013 The operand dtype ( DataTypr or str ) \u2013 The target data type Returns: VarRef or Number \u2013 The result Source code in freetensor/core/expr.py def cast ( expr , dtype ): ''' Cast to another type Parameters ---------- expr : VarRef or Number The operand dtype : DataTypr or str The target data type Returns ------- VarRef or Number The result ''' return ffi . makeCast ( expr , ffi . DataType ( dtype )) ceil ( expr ) \u00b6 Round a float up to an interger (towards +inf) For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ceil Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result Source code in freetensor/core/expr.py def ceil ( expr ): ''' Round a float up to an interger (towards +inf) For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ceil Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The result ''' if _istensor ( expr ): from .. import libop return libop . ceil ( expr ) return ffi . makeCeil ( expr ) ceildiv ( lhs , rhs ) \u00b6 Ceiling integer division of lhs dividing by rhs The result rounds towards positive infinity For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ceildiv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient Source code in freetensor/core/expr.py def ceildiv ( lhs , rhs ): ''' Ceiling integer division of `lhs` dividing by `rhs` The result rounds towards positive infinity For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ceildiv Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The quotient ''' if _istensor ( lhs ) or _istensor ( rhs ): from .. import libop return libop . ceildiv ( lhs , rhs ) if type ( lhs ) is int and type ( rhs ) is int : return lhs // rhs + ( lhs % rhs > 0 ) return ffi . makeCeilDiv ( lhs , rhs ) dtype ( var ) \u00b6 Get element data type of a variable Source code in freetensor/core/expr.py def dtype ( var ): ''' Get element data type of a variable ''' if isinstance ( var , VarRef ): return var . dtype elif isinstance ( var , ffi . Expr ): return var . dtype else : # TODO: Config default type if isinstance ( var , bool ): # NOTE: before int, because bool in Python is a sub-class of int return ffi . DataType ( \"bool\" ) elif isinstance ( var , float ): return ffi . DataType ( \"float32\" ) elif isinstance ( var , int ): return ffi . DataType ( \"int32\" ) else : raise Exception ( 'Unknown scalar type: ' + str ( type ( var ))) eq ( lhs , rhs ) \u00b6 lhs == rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.eq Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison Source code in freetensor/core/expr.py def eq ( lhs , rhs ): ''' `lhs == rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.eq Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The comparison ''' return lhs == rhs exp ( expr ) \u00b6 Natural exponent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.exp Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The exponent Source code in freetensor/core/expr.py def exp ( expr ): ''' Natural exponent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.exp Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The exponent ''' if _istensor ( expr ): from .. import libop return libop . exp ( expr ) if isinstance ( expr , Number ): return math . exp ( expr ) return ffi . makeExp ( expr ) floor ( expr ) \u00b6 Round a float down to an interger (towards -inf) For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.floor Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result Source code in freetensor/core/expr.py def floor ( expr ): ''' Round a float down to an interger (towards -inf) For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.floor Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The result ''' if _istensor ( expr ): from .. import libop return libop . floor ( expr ) return ffi . makeFloor ( expr ) floordiv ( lhs , rhs ) \u00b6 Floored integer division of lhs dividing by rhs The result rounds towards negative infinity (following Python convention, instead of C) This function is recommended over round_towards_0_div , as it enjoys more optimizations For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.floordiv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient Source code in freetensor/core/expr.py def floordiv ( lhs , rhs ): ''' Floored integer division of `lhs` dividing by `rhs` The result rounds towards negative infinity (following Python convention, instead of C) This function is recommended over `round_towards_0_div`, as it enjoys more optimizations For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.floordiv Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The quotient ''' return lhs // rhs ge ( lhs , rhs ) \u00b6 lhs >= rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ge Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison Source code in freetensor/core/expr.py def ge ( lhs , rhs ): ''' `lhs >= rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ge Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The comparison ''' return lhs >= rhs gt ( lhs , rhs ) \u00b6 lhs > rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.gt Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison Source code in freetensor/core/expr.py def gt ( lhs , rhs ): ''' `lhs > rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.gt Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The comparison ''' return lhs > rhs if_then_else ( cond , then_case , else_case ) \u00b6 Similar to then_case if cond else else_case NOTE: there is NO guarantee that only one branch will be executed. In some cases, both branches will be executed and the result of one of them will be picked. Therefore, please do NOT use if_then_else to guard an out-of-bound array indexing Parameters: cond ( VarRef of Number ) \u2013 Condition lhs ( VarRef or Number ) \u2013 Then-case experssion rhs ( VarRef or Number ) \u2013 Else-case expression Returns: VarRef or Number \u2013 The result Source code in freetensor/core/expr.py def if_then_else ( cond , then_case , else_case ): ''' Similar to `then_case if cond else else_case` NOTE: there is NO guarantee that only one branch will be executed. In some cases, both branches will be executed and the result of one of them will be picked. Therefore, please do NOT use `if_then_else` to guard an out-of-bound array indexing Parameters ---------- cond : VarRef of Number Condition lhs : VarRef or Number Then-case experssion rhs : VarRef or Number Else-case expression Returns ------- VarRef or Number The result ''' if type ( cond ) is bool : return then_case if cond else else_case return ffi . makeIfExpr ( cond , then_case , else_case ) intrinsic ( fmt , * params , ** kws ) \u00b6 Invoke whatever target code Parameters: fmt ( str ) \u2013 What to run. \"%\" is filled by parameters one by one. E.g. sinf(%) The following variadic arguments ( Expr ) \u2013 Parameters to fmt ret_type ( DataType or str ) \u2013 (Keyword argument only) The return type. Void for no return type. Defaults to Void has_side_effect ( bool ) \u2013 (Keyword argument only) True to indicate the intrinsic modifes something other than the return value. Defaults to false Source code in freetensor/core/expr.py def intrinsic ( fmt , * params , ** kws ): \"\"\" Invoke whatever target code Parameters ---------- fmt : str What to run. \"%\" is filled by parameters one by one. E.g. sinf(%) The following variadic arguments : Expr Parameters to `fmt` ret_type : DataType or str (Keyword argument only) The return type. Void for no return type. Defaults to Void has_side_effect: bool (Keyword argument only) True to indicate the intrinsic modifes something other than the return value. Defaults to false \"\"\" ret_type = ffi . DataType ( \"void\" ) has_side_effect = False if \"ret_type\" in kws : ret_type = ffi . DataType ( kws [ \"ret_type\" ]) del kws [ \"ret_type\" ] if \"has_side_effect\" in kws : has_side_effect = kws [ \"has_side_effect\" ] del kws [ \"has_side_effect\" ] assert len ( kws ) == 0 , \"Unrecognized keyword arguments: %s \" % kws return ffi . makeIntrinsic ( fmt , params , ret_type , has_side_effect ) l_and ( lhs , rhs ) \u00b6 Logical and of lhs and rhs NOTE: Short-circuit evaluation is NOT supported For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_and Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The logical and Source code in freetensor/core/expr.py def l_and ( lhs , rhs ): ''' Logical and of `lhs` and `rhs` NOTE: Short-circuit evaluation is NOT supported For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_and Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The logical and ''' if _istensor ( lhs ) or _istensor ( rhs ): from .. import libop return libop . l_and ( lhs , rhs ) if type ( lhs ) is bool and type ( rhs ) is bool : return lhs and rhs else : return ffi . makeLAnd ( lhs , rhs ) l_not ( expr ) \u00b6 Logical not For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_not Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The logical not Source code in freetensor/core/expr.py def l_not ( expr ): ''' Logical not For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_not Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The logical not ''' if _istensor ( expr ): from .. import libop return libop . l_not ( expr ) if type ( expr ) is bool : return not expr else : return ffi . makeLNot ( expr ) l_or ( lhs , rhs ) \u00b6 Logical or of lhs and rhs NOTE: Short-circuit evaluation is NOT supported For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_or Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The logical or Source code in freetensor/core/expr.py def l_or ( lhs , rhs ): ''' Logical or of `lhs` and `rhs` NOTE: Short-circuit evaluation is NOT supported For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_or Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The logical or ''' if _istensor ( lhs ) or _istensor ( rhs ): from .. import libop return libop . l_or ( lhs , rhs ) if type ( lhs ) is bool and type ( rhs ) is bool : return lhs or rhs else : return ffi . makeLOr ( lhs , rhs ) le ( lhs , rhs ) \u00b6 lhs <= rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.le Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison Source code in freetensor/core/expr.py def le ( lhs , rhs ): ''' `lhs <= rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.le Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The comparison ''' return lhs <= rhs lt ( lhs , rhs ) \u00b6 lhs < rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.lt Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison Source code in freetensor/core/expr.py def lt ( lhs , rhs ): ''' `lhs < rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.lt Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The comparison ''' return lhs < rhs max ( lhs , rhs ) \u00b6 Maximum of lhs and rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.max Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The maximum Source code in freetensor/core/expr.py def max ( lhs , rhs ): ''' Maximum of `lhs` and `rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.max Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The maximum ''' if _istensor ( lhs ) or _istensor ( rhs ): from .. import libop return libop . max ( lhs , rhs ) if isinstance ( lhs , Number ) and isinstance ( rhs , Number ): return builtins . max ( lhs , rhs ) return ffi . makeMax ( lhs , rhs ) min ( lhs , rhs ) \u00b6 Minimum of lhs and rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.min Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The minimum Source code in freetensor/core/expr.py def min ( lhs , rhs ): ''' Minimum of `lhs` and `rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.min Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The minimum ''' if _istensor ( lhs ) or _istensor ( rhs ): from .. import libop return libop . min ( lhs , rhs ) if isinstance ( lhs , Number ) and isinstance ( rhs , Number ): return builtins . min ( lhs , rhs ) return ffi . makeMin ( lhs , rhs ) mod ( lhs , rhs ) \u00b6 lhs modulus rhs The result is always non-negative (following Python convention, instead of C). This function is recommended over remainder , as it enjoys more optimizations For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.mod Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The modulo Source code in freetensor/core/expr.py def mod ( lhs , rhs ): ''' `lhs` modulus `rhs` The result is always non-negative (following Python convention, instead of C). This function is recommended over `remainder`, as it enjoys more optimizations For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.mod Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The modulo ''' return lhs % rhs mtype ( var ) \u00b6 Get memory type of a variable Source code in freetensor/core/expr.py def mtype ( var ): ''' Get memory type of a variable ''' if isinstance ( var , VarRef ): return var . mtype else : return 'byvalue' mul ( lhs , rhs ) \u00b6 lhs * rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.mul Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The product Source code in freetensor/core/expr.py def mul ( lhs , rhs ): ''' `lhs * rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.mul Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The product ''' return lhs * rhs ndim ( var ) \u00b6 Get the number of dimensions of a variable Source code in freetensor/core/expr.py def ndim ( var ): ''' Get the number of dimensions of a variable ''' if isinstance ( var , VarRef ): return var . ndim else : return 0 ne ( lhs , rhs ) \u00b6 lhs != rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ne Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison Source code in freetensor/core/expr.py def ne ( lhs , rhs ): ''' `lhs != rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ne Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The comparison ''' return lhs != rhs remainder ( lhs , rhs ) \u00b6 Remainder of lhs dividing rhs The result can be positive or negative (following C convention, instead of Python). End users are encouraged to use lhs % rhs instead, which follows Python convetion, and enjoys better optimization in FreeTensor For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.remainder Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The remainder Source code in freetensor/core/expr.py def remainder ( lhs , rhs ): ''' Remainder of `lhs` dividing `rhs` The result can be positive or negative (following C convention, instead of Python). End users are encouraged to use `lhs % rhs` instead, which follows Python convetion, and enjoys better optimization in FreeTensor For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.remainder Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The remainder ''' if _istensor ( lhs ) or _istensor ( rhs ): from .. import libop return libop . remainder ( lhs , rhs ) return ffi . makeRemainder ( lhs , rhs ) round_towards_0_div ( lhs , rhs ) \u00b6 C-style integer division of lhs dividing by rhs The result rounds towards 0 (following C convention, instead of Python) End users are encouraged to use lhs // rhs instead, which follows Python convetion, and enjoys better optimization in FreeTensor For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.round_towards_0_div Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient Source code in freetensor/core/expr.py def round_towards_0_div ( lhs , rhs ): ''' C-style integer division of `lhs` dividing by `rhs` The result rounds towards 0 (following C convention, instead of Python) End users are encouraged to use `lhs // rhs` instead, which follows Python convetion, and enjoys better optimization in FreeTensor For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.round_towards_0_div Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The quotient ''' if _istensor ( lhs ) or _istensor ( rhs ): from .. import libop return libop . round_towards_0_div ( lhs , rhs ) return ffi . makeRoundTowards0Div ( lhs , rhs ) shape ( var , i = None ) \u00b6 shape(var, i): Get size of specified dimension of a variable shape(var): Get sizes of all dimensions of a variable Source code in freetensor/core/expr.py def shape ( var , i = None ): ''' shape(var, i): Get size of specified dimension of a variable shape(var): Get sizes of all dimensions of a variable ''' if isinstance ( var , VarRef ): return var . shape ( i ) else : if i is None : return () else : raise Exception ( f 'Getting size of dimension { i } of scalar { var } ' ) sigmoid ( expr ) \u00b6 Sigmoid For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sigmoid Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result Source code in freetensor/core/expr.py def sigmoid ( expr ): ''' Sigmoid For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sigmoid Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The result ''' if _istensor ( expr ): from .. import libop return libop . sigmoid ( expr ) return ffi . makeSigmoid ( expr ) sqrt ( expr ) \u00b6 Square root For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sqrt Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The square root Source code in freetensor/core/expr.py def sqrt ( expr ): ''' Square root For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sqrt Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The square root ''' if _istensor ( expr ): from .. import libop return libop . sqrt ( expr ) if isinstance ( expr , Number ): return math . sqrt ( expr ) return ffi . makeSqrt ( expr ) square ( expr ) \u00b6 Square For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.square Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The square Source code in freetensor/core/expr.py def square ( expr ): ''' Square For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.square Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The square ''' if _istensor ( expr ): from .. import libop return libop . square ( expr ) if isinstance ( expr , Number ): return expr * expr return ffi . makeSquare ( expr ) sub ( lhs , rhs ) \u00b6 lhs - rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sub Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The difference Source code in freetensor/core/expr.py def sub ( lhs , rhs ): ''' `lhs - rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sub Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The difference ''' return lhs - rhs tanh ( expr ) \u00b6 Hyperbolic tangent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.tanh Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result Source code in freetensor/core/expr.py def tanh ( expr ): ''' Hyperbolic tangent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.tanh Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The result ''' if _istensor ( expr ): from .. import libop return libop . tanh ( expr ) if isinstance ( expr , Number ): return math . tanh ( expr ) return ffi . makeTanh ( expr ) truediv ( lhs , rhs ) \u00b6 Floating point division of lhs dividing by rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.truediv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient Source code in freetensor/core/expr.py def truediv ( lhs , rhs ): ''' Floating point division of `lhs` dividing by `rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.truediv Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The quotient ''' return lhs / rhs frontend \u00b6 A frontend transforming user Python functions to ASTs via staging. FreeTensorOverload ( StagingOverload ) \u00b6 Helper class managing context in IR staging. Source code in freetensor/core/frontend.py class FreeTensorOverload ( StagingOverload ): '''Helper class managing context in IR staging.''' def __init__ ( self ): super () . __init__ () self . lifetime_stack : List [ LifetimeScope ] = [] self . closure : Dict [ str , Any ] = {} self . name_dict : Dict [ str , int ] = {} def register_vardef ( self , name , shape , dtype , atype , mtype = None , capture = None ): fullname = self . fullname ( name ) if capture : self . closure [ fullname ] = capture return self . lifetime_stack [ - 1 ] . register_inner_scope ( _VarDef ( fullname , shape , dtype , atype , mtype )) def register_inlined_invoke ( self , ret_names : Sequence [ str ], func : ffi . Func , args , kvs ): ret_names = [ self . fullname ( name ) for name in ret_names ] return self . lifetime_stack [ - 1 ] . register_inner_scope ( Invoke ( ret_names , func , args , kvs )) def register_assert ( self , pred ): self . lifetime_stack [ - 1 ] . register_inner_scope ( Assert ( pred )) def fullname ( self , name : str ) -> str : '''Get distinct name.''' if name in self . name_dict : self . name_dict [ name ] += 1 return f ' { name } _ { self . name_dict [ name ] } ' else : self . name_dict [ name ] = 0 return name def in_staging ( self ,): return len ( self . lifetime_stack ) > 0 def custom_attr ( self , obj : Any , attr : str ) -> Any : if attr == \"ndim\" : return ndim ( obj ) if attr == \"shape\" : return lambda i = None : shape ( obj , i ) if attr == \"dtype\" : return dtype ( obj ) if attr == \"mtype\" : return mtype ( obj ) raise AttributeError () def functiondef_wrapper ( self , filename : str , func ): basic_wrapped = super () . functiondef_wrapper ( filename , func ) def wrapped ( * args , __freetensor_transform_outermost__ = False , ** kwargs ): if __freetensor_transform_outermost__ : call_metadata = None else : call_metadata = ctx_stack . top () . get_metadata () ctx_stack . top () . clear_metadata () prev = ctx_stack . top () . caller_metadata ctx_stack . top () . set_caller_metadata ( call_metadata ) result = basic_wrapped ( * args , ** kwargs ) ctx_stack . top () . set_caller_metadata ( prev ) return result return wrapped def metadata ( self , entry : str ) -> None : parts = entry . split () if len ( parts ) == 0 : return key = parts [ 0 ] if len ( parts ) > 1 : key = key [: - 1 ] val = parts [ 1 ] if key == 'label' : ctx_stack . top () . add_label ( val ) elif key == 'no_deps' : back = inspect . currentframe () . f_back if val in back . f_locals : var = back . f_locals [ val ] elif val in back . f_globals : var = back . f_globals [ val ] else : raise self . error ( f 'Variable { val } not found for annotating comment ( { key } : { val } )' ) if not isinstance ( var , VarRef ): raise self . error ( f 'Variable { val } = { var } is not a VarRef, which is required by annotating comment ( { key } : { val } )' ) ctx_stack . top () . add_next_no_deps ( var . name ) elif key == 'prefer_libs' : ctx_stack . top () . set_next_prefer_libs () def at_position ( self , filename : str , lineno : int ) -> None : ctx_stack . top () . set_next_location ( filename , lineno ) allow_shortcut_scope ( self , allow ) inherited \u00b6 Opens a scope that allows shortcut control flows in a statically deterministic context. Need to be closed by with statement. Source code in freetensor/core/frontend.py def allow_shortcut_scope ( self , allow : bool ): '''Opens a scope that allows shortcut control flows in a statically deterministic context. Need to be closed by `with` statement.''' return AllowShortcutScope ( self , allow ) assert_stmt ( self , test ) inherited \u00b6 Assert staging tool. Source code in freetensor/core/frontend.py def assert_stmt ( self , test ): '''Assert staging tool.''' if isinstance ( test , StagedPredicate ): test . assert_stmt () else : assert test assign_stmt ( self , name , value ) inherited \u00b6 Customized assign wrapper. If value is instance of StagedAssignable , it's regarded as a customized assign behavior and gets executed with the assigned target variable name. This wrapper is used for initializing a variable. Source code in freetensor/core/frontend.py def assign_stmt ( self , name : str , value ): '''Customized assign wrapper. If `value` is instance of `StagedAssignable`, it's regarded as a customized assign behavior and gets executed with the assigned target variable name. This wrapper is used for initializing a variable. ''' if isinstance ( value , StagedAssignable ): return value . assign ( name ) else : return value at_position ( self , filename , lineno ) \u00b6 Code position handler. Defaults to a no-op. Can be overridden by subclasses. Parameters: filename ( str ) \u2013 Name of the file containing code for the next statement. lineno ( int ) \u2013 Line number of the next statement. Source code in freetensor/core/frontend.py def at_position ( self , filename : str , lineno : int ) -> None : ctx_stack . top () . set_next_location ( filename , lineno ) break_stmt ( self ) inherited \u00b6 Break staging tool. Only allow break in static control flow. Source code in freetensor/core/frontend.py def break_stmt ( self ): '''Break staging tool. Only allow break in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Break is only allowed in statically deterministic control flow.' ) raise BreakException () continue_stmt ( self ) inherited \u00b6 Continue staging tool. Only allow continue in static control flow. Source code in freetensor/core/frontend.py def continue_stmt ( self ): '''Continue staging tool. Only allow continue in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Continue is only allowed in statically deterministic control flow.' ) raise ContinueException () custom_attr ( self , obj , attr ) \u00b6 Customized attribute accessor. The framework first looks for a Python native attribute. If not found, it looks for this overloaded custom attribute resolver. The default implementation provides no custom attribute. Can be overridden by subclasses. Parameters: obj ( Any ) \u2013 Object to access attribute. attr ( str ) \u2013 Attribute name. Returns: Any \u2013 The attribute value. Source code in freetensor/core/frontend.py def custom_attr ( self , obj : Any , attr : str ) -> Any : if attr == \"ndim\" : return ndim ( obj ) if attr == \"shape\" : return lambda i = None : shape ( obj , i ) if attr == \"dtype\" : return dtype ( obj ) if attr == \"mtype\" : return mtype ( obj ) raise AttributeError () foreach ( self , names , iter , body ) inherited \u00b6 Customized foreach wrapper. If value is instance of StagedIterable , its regarded as a customized foreach behavior and used to generate code for the python for loop. Otherwise, we try to execute the loop as usual. Source code in freetensor/core/frontend.py def foreach ( self , names , iter , body : Callable [[ Any ], None ]) -> None : '''Customized foreach wrapper. If `value` is instance of `StagedIterable`, its regarded as a customized foreach behavior and used to generate code for the python for loop. Otherwise, we try to execute the loop as usual. ''' if isinstance ( iter , StagedIterable ): iter . foreach ( names , body ) else : for iter_var in iter : try : body ( iter_var ) except BreakException : break except ContinueException : continue fullname ( self , name ) \u00b6 Get distinct name. Source code in freetensor/core/frontend.py def fullname ( self , name : str ) -> str : '''Get distinct name.''' if name in self . name_dict : self . name_dict [ name ] += 1 return f ' { name } _ { self . name_dict [ name ] } ' else : self . name_dict [ name ] = 0 return name functiondef_wrapper ( self , filename , func ) \u00b6 Function definition wrapper. This wrapper performs extra initialization and cleanup for function definition. Source code in freetensor/core/frontend.py def functiondef_wrapper ( self , filename : str , func ): basic_wrapped = super () . functiondef_wrapper ( filename , func ) def wrapped ( * args , __freetensor_transform_outermost__ = False , ** kwargs ): if __freetensor_transform_outermost__ : call_metadata = None else : call_metadata = ctx_stack . top () . get_metadata () ctx_stack . top () . clear_metadata () prev = ctx_stack . top () . caller_metadata ctx_stack . top () . set_caller_metadata ( call_metadata ) result = basic_wrapped ( * args , ** kwargs ) ctx_stack . top () . set_caller_metadata ( prev ) return result return wrapped if_then_else_expr ( self , predicate , then_expr , else_expr ) inherited \u00b6 If-then-else expression staging tool. Source code in freetensor/core/frontend.py def if_then_else_expr ( self , predicate , then_expr , else_expr ): '''If-then-else expression staging tool.''' if isinstance ( predicate , StagedPredicate ): return predicate . if_then_else_expr ( then_expr , else_expr ) else : if predicate : return then_expr () else : return else_expr () if_then_else_stmt ( self , predicate , then_body , else_body = None ) inherited \u00b6 If-then-else statement staging tool. When predicate is deterministic in staging, only one branch is generated. Otherwise, a If node in IR is generated. Source code in freetensor/core/frontend.py def if_then_else_stmt ( self , predicate , then_body , else_body = None ): '''If-then-else statement staging tool. When predicate is deterministic in staging, only one branch is generated. Otherwise, a If node in IR is generated. ''' if isinstance ( predicate , StagedPredicate ): predicate . if_then_else_stmt ( then_body , else_body ) else : if predicate : then_body () elif else_body : else_body () load_attr ( self , obj , attr ) inherited \u00b6 Load attribute staging tool. Allows customization of reading attributes. Source code in freetensor/core/frontend.py def load_attr ( self , obj , attr : str ): '''Load attribute staging tool. Allows customization of reading attributes.''' try : return getattr ( obj , attr ) except AttributeError : try : # Have to use AttributeError again, since a custom attribute might have # a None value result = self . custom_attr ( obj , attr ) successful = True except AttributeError : successful = False if successful : return result else : raise metadata ( self , entry ) \u00b6 Metadata handler. A metadata line is a comment starting with #! and followed by a metadata, represented as a string parameter. Defaults to a no-op. Can be overridden by subclasses. Parameters: content ( str ) \u2013 The metadata content. Source code in freetensor/core/frontend.py def metadata ( self , entry : str ) -> None : parts = entry . split () if len ( parts ) == 0 : return key = parts [ 0 ] if len ( parts ) > 1 : key = key [: - 1 ] val = parts [ 1 ] if key == 'label' : ctx_stack . top () . add_label ( val ) elif key == 'no_deps' : back = inspect . currentframe () . f_back if val in back . f_locals : var = back . f_locals [ val ] elif val in back . f_globals : var = back . f_globals [ val ] else : raise self . error ( f 'Variable { val } not found for annotating comment ( { key } : { val } )' ) if not isinstance ( var , VarRef ): raise self . error ( f 'Variable { val } = { var } is not a VarRef, which is required by annotating comment ( { key } : { val } )' ) ctx_stack . top () . add_next_no_deps ( var . name ) elif key == 'prefer_libs' : ctx_stack . top () . set_next_prefer_libs () return_stmt ( self , value , funcname ) inherited \u00b6 Return staging tool. Only allow return in static control flow. Source code in freetensor/core/frontend.py def return_stmt ( self , value , funcname ): '''Return staging tool. Only allow return in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Return is only allowed in statically deterministic control flow.' ) if isinstance ( value , StagedUnpackAssignable ): # We don't know how many items are there, so no unpacking value = value . assign ( funcname ) if isinstance ( value , StagedAssignable ): value = value . assign ( funcname ) raise ReturnException ( value ) unpack_assign_stmt ( self , names , values ) inherited \u00b6 Customized assign wrapper for one or more targets. If values is instance of StagedUnpackAssignable , it's regarded as a customized assign behavior and gets executed with all the assigned targets' names. Otherwise, it calls assign_stmt with each sub-assignments. Please note that names can be nested tuples like (\"a\", (\"b\", \"c\")) . Please also note that names can also be a single string like \"a\" even if values is a tuple. There is no unpacking in this case Source code in freetensor/core/frontend.py def unpack_assign_stmt ( self , names , values ): '''Customized assign wrapper for one or more targets. If `values` is instance of `StagedUnpackAssignable`, it's regarded as a customized assign behavior and gets executed with all the assigned targets' names. Otherwise, it calls `assign_stmt` with each sub-assignments. Please note that `names` can be nested tuples like `(\"a\", (\"b\", \"c\"))`. Please also note that `names` can also be a single string like \"a\" even if `values` is a tuple. There is no unpacking in this case ''' if isinstance ( values , StagedUnpackAssignable ): return values . assign ( names ) elif isinstance ( names , str ): return self . assign_stmt ( names , values ) else : assert isinstance ( names , Sequence ) values = tuple ( values ) if len ( names ) != len ( values ): raise self . error ( \"Number of return values does not match when unpacking\" ) returns = [] for name , value in zip ( names , values ): returns . append ( self . unpack_assign_stmt ( name , value )) return tuple ( returns ) while_stmt ( self , fpred , body ) inherited \u00b6 While statement staging tool. Source code in freetensor/core/frontend.py def while_stmt ( self , fpred , body ): '''While statement staging tool.''' first_pred = fpred () if isinstance ( first_pred , StagedPredicate ): first_pred . while_stmt ( body ) else : if first_pred : try : body () except BreakException : return except ContinueException : pass while fpred (): try : body () except BreakException : break except ContinueException : continue LifetimeScope \u00b6 This scope is used to register multiple scopes inside a single lifetime scope. The inner scopes might be used to register variables, etc. They will be exited in reverse order of their registration. Source code in freetensor/core/frontend.py class LifetimeScope : '''This scope is used to register multiple scopes inside a single lifetime scope. The inner scopes might be used to register variables, etc. They will be exited in reverse order of their registration. ''' def __init__ ( self ): self . inner_scopes = [] def __enter__ ( self ): _overload . lifetime_stack . append ( self ) def __exit__ ( self , exc_type , exc_val , exc_tb ): for scope in reversed ( self . inner_scopes ): scope . __exit__ ( exc_type , exc_val , exc_tb ) popped = _overload . lifetime_stack . pop () if popped != self : raise _overload . error ( 'LifetimeScope enter/exit not match, must be FILO' ) def register_inner_scope ( self , scope ): self . inner_scopes . append ( scope ) return scope . __enter__ () PredefinedVarCreator ( VarCreator ) dataclass \u00b6 Source code in freetensor/core/frontend.py class PredefinedVarCreator ( VarCreator ): def __init__ ( self , initializer : List [ Any ], dtype : str , mtype : str ): def get_shape ( lst ): if not isinstance ( lst , list ): assert ndim ( lst ) == 0 return () if len ( lst ) == 0 : return ( 0 ,) shape_ = get_shape ( lst [ 0 ]) for x in lst [ 1 :]: assert shape_ == get_shape ( x ) return ( len ( lst ),) + shape_ super () . __init__ ( get_shape ( initializer ), dtype , mtype ) self . initializer = initializer def assign ( self , name : str ) -> VarRef : var = super () . assign ( name ) def impl ( var_slice , init_slice ): if not isinstance ( init_slice , list ): var_slice [()] = init_slice else : for i , x in enumerate ( init_slice ): impl ( var_slice [ i ], x ) impl ( var , self . initializer ) return var __class__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in freetensor/core/frontend.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls ) __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). Source code in freetensor/core/frontend.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. Source code in freetensor/core/frontend.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). Source code in freetensor/core/frontend.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/frontend.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , initializer , dtype , mtype ) special \u00b6 Initialize self. See help(type(self)) for accurate signature. Source code in freetensor/core/frontend.py def __init__ ( self , initializer : List [ Any ], dtype : str , mtype : str ): def get_shape ( lst ): if not isinstance ( lst , list ): assert ndim ( lst ) == 0 return () if len ( lst ) == 0 : return ( 0 ,) shape_ = get_shape ( lst [ 0 ]) for x in lst [ 1 :]: assert shape_ == get_shape ( x ) return ( len ( lst ),) + shape_ super () . __init__ ( get_shape ( initializer ), dtype , mtype ) self . initializer = initializer assign ( self , name ) \u00b6 Customized assign behavior. Creates a VarDef with its full name. Source code in freetensor/core/frontend.py def assign ( self , name : str ) -> VarRef : var = super () . assign ( name ) def impl ( var_slice , init_slice ): if not isinstance ( init_slice , list ): var_slice [()] = init_slice else : for i , x in enumerate ( init_slice ): impl ( var_slice [ i ], x ) impl ( var , self . initializer ) return var Var ( StagedTypeAnnotation ) \u00b6 Source code in freetensor/core/frontend.py class Var ( StagedTypeAnnotation ): def __init__ ( self , shape , dtype , atype = \"input\" , mtype = None ): ''' Declare a variable Parameters ---------- name : str Name of the variable shape : Sequence[Expr] or Var Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype : str or DataType Data type of the variable atype : str or AccessType Access type of the variable. It specifies whether (and how) the variable is an I/O variable of the function it belongs to. Defaults to \"input\" mtype : str or MemType (Optional) Memory type of the variable. If omitted, the main memory type of the default Target in config will be used ''' self . shape , self . dtype , self . atype , self . mtype = shape , dtype , atype , mtype def annotate ( self , name : str ) -> VarRef : return _overload . register_vardef ( name , self . shape , self . dtype , self . atype , self . mtype ) __class__ ( ABCMeta ) inherited \u00b6 Source code in freetensor/core/frontend.py class StagedTypeAnnotationMeta ( abc . ABCMeta ): def __getitem__ ( self , args ): return self ( * args ) __base__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in freetensor/core/frontend.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls ) __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). Source code in freetensor/core/frontend.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. Source code in freetensor/core/frontend.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). Source code in freetensor/core/frontend.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/frontend.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/frontend.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , shape , dtype , atype = 'input' , mtype = None ) special \u00b6 Declare a variable Parameters: name ( str ) \u2013 Name of the variable shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable atype ( str or AccessType ) \u2013 Access type of the variable. It specifies whether (and how) the variable is an I/O variable of the function it belongs to. Defaults to \"input\" mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used Source code in freetensor/core/frontend.py def __init__ ( self , shape , dtype , atype = \"input\" , mtype = None ): ''' Declare a variable Parameters ---------- name : str Name of the variable shape : Sequence[Expr] or Var Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype : str or DataType Data type of the variable atype : str or AccessType Access type of the variable. It specifies whether (and how) the variable is an I/O variable of the function it belongs to. Defaults to \"input\" mtype : str or MemType (Optional) Memory type of the variable. If omitted, the main memory type of the default Target in config will be used ''' self . shape , self . dtype , self . atype , self . mtype = shape , dtype , atype , mtype VarCreator ( StagedAssignable ) dataclass \u00b6 VarCreator(shape: Union[Sequence, freetensor.core.expr.VarRef], dtype: str, mtype: str, assigned: bool = False) Source code in freetensor/core/frontend.py @dataclass class VarCreator ( StagedAssignable ): shape : Union [ Sequence , VarRef ] dtype : str mtype : str assigned : bool = False def assign ( self , name : str ) -> VarRef : '''Customized assign behavior. Creates a VarDef with its full name.''' if not self . assigned : self . assigned = True return _overload . register_vardef ( name , self . shape , self . dtype , 'cache' , self . mtype ) else : raise _overload . error ( \"Create new tensors in an `a = b = c`-like multi-assignment \" \"is not supported\" ) __class__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in freetensor/core/frontend.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls ) __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). Source code in freetensor/core/frontend.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. Source code in freetensor/core/frontend.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). Source code in freetensor/core/frontend.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/frontend.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) assign ( self , name ) \u00b6 Customized assign behavior. Creates a VarDef with its full name. Source code in freetensor/core/frontend.py def assign ( self , name : str ) -> VarRef : '''Customized assign behavior. Creates a VarDef with its full name.''' if not self . assigned : self . assigned = True return _overload . register_vardef ( name , self . shape , self . dtype , 'cache' , self . mtype ) else : raise _overload . error ( \"Create new tensors in an `a = b = c`-like multi-assignment \" \"is not supported\" ) dynamic_range ( StagedIterable ) \u00b6 Dynamic range that generates For loop in IR tree. Source code in freetensor/core/frontend.py class dynamic_range ( StagedIterable ): '''Dynamic range that generates For loop in IR tree.''' def __init__ ( self , start , stop = None , step = 1 ) -> None : '''Initialize a dynamic range. Arguments semantic identical to builtin `range`.''' if stop : self . start = start self . stop = stop else : self . start = 0 self . stop = start self . step = step def foreach ( self , name , body : Callable [[ Any ], None ]) -> None : '''Customized foreach behavior. Creates a For loop.''' if not isinstance ( name , str ): raise _overload . error ( 'dynamic_range only supports exactly one target variable' ) # Early optimizations if isinstance ( self . start , Number ) and isinstance ( self . stop , Number ) and isinstance ( self . step , Number ): if not range ( self . start , self . stop , self . step ): return if len ( range ( self . start , self . stop , self . step )) == 1 : with LifetimeScope (): body ( self . start ) return with _overload . allow_shortcut_scope ( False ): with For ( _overload . fullname ( name ), self . start , self . stop , self . step ) as iter_var : with LifetimeScope (): body ( iter_var ) __init__ ( self , start , stop = None , step = 1 ) special \u00b6 Initialize a dynamic range. Arguments semantic identical to builtin range . Source code in freetensor/core/frontend.py def __init__ ( self , start , stop = None , step = 1 ) -> None : '''Initialize a dynamic range. Arguments semantic identical to builtin `range`.''' if stop : self . start = start self . stop = stop else : self . start = 0 self . stop = start self . step = step foreach ( self , name , body ) \u00b6 Customized foreach behavior. Creates a For loop. Source code in freetensor/core/frontend.py def foreach ( self , name , body : Callable [[ Any ], None ]) -> None : '''Customized foreach behavior. Creates a For loop.''' if not isinstance ( name , str ): raise _overload . error ( 'dynamic_range only supports exactly one target variable' ) # Early optimizations if isinstance ( self . start , Number ) and isinstance ( self . stop , Number ) and isinstance ( self . step , Number ): if not range ( self . start , self . stop , self . step ): return if len ( range ( self . start , self . stop , self . step )) == 1 : with LifetimeScope (): body ( self . start ) return with _overload . allow_shortcut_scope ( False ): with For ( _overload . fullname ( name ), self . start , self . stop , self . step ) as iter_var : with LifetimeScope (): body ( iter_var ) capture_var ( * args , ** kwargs ) \u00b6 Capture external array as tensor variable. Source code in freetensor/core/frontend.py def impl ( * args , ** kwargs ): if _overload . in_staging (): return staging ( * args , ** kwargs ) else : return original ( * args , ** kwargs ) empty ( * args , ** kwargs ) \u00b6 Create an empty variable Parameters: shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used Source code in freetensor/core/frontend.py def impl ( * args , ** kwargs ): if _overload . in_staging (): return staging ( * args , ** kwargs ) else : return original ( * args , ** kwargs ) inline ( func = None , src = None , fallback = None , default_dynamic_range = True , verbose = False ) \u00b6 Enable a user function to be called by a transformed function at run time Parameters: func ( Python function ) \u2013 The user function src ( str (Optional) ) \u2013 The source code of func . This parameter is only required if the source code cannot be get automatically, e.g., if func is generated from a exec default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( bool ) \u2013 True to print the generated Python code that is used for transforming Source code in freetensor/core/frontend.py def inline ( func = None , src = None , fallback = None , default_dynamic_range = True , verbose = False ): ''' Enable a user function to be called by a transformed function at run time Parameters ---------- func : Python function The user function src : str (Optional) The source code of `func`. This parameter is only required if the source code cannot be get automatically, e.g., if `func` is generated from a `exec` default_dynamic_range : bool If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose : bool True to print the generated Python code that is used for transforming ''' if func is None : return functools . partial ( inline , src = src , fallback = fallback , default_dynamic_range = default_dynamic_range , verbose = verbose ) extra_locals = _prepare_extra_locals ( default_dynamic_range ) # Do not initialize _overload here, since `into_staging` does not use the context. # Keep the context as-is to support adding new inline functions during transforming. # Such a case occurs when a transformed function dynamically imports a new inline. transformed = _overload . into_staging ( func , extra_locals , src , verbose = verbose ) return functools . wraps ( func )( staged_callable ( transformed , fallback or func )) transform ( func = None , default_dynamic_range = True , verbose = 0 ) \u00b6 Transform a user function to an AST Parameters: func ( Python function ) \u2013 The user function to transform. If not specified, a partial function will be returend, which can be used as a decorator default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( int ) \u2013 0 = print nothing. 1 = print the resulting AST. 2 = 1 + print the generated Python code that is used for transforming Source code in freetensor/core/frontend.py def transform ( func = None , default_dynamic_range = True , verbose : int = 0 ): ''' Transform a user function to an AST Parameters ---------- func : Python function The user function to transform. If not specified, a partial function will be returend, which can be used as a decorator default_dynamic_range : bool If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose : int 0 = print nothing. 1 = print the resulting AST. 2 = 1 + print the generated Python code that is used for transforming ''' if func is None : return functools . partial ( transform , default_dynamic_range = default_dynamic_range , verbose = verbose ) if verbose is None : verbose = 0 extra_locals = _prepare_extra_locals ( default_dynamic_range ) params = list ( inspect . signature ( func ) . parameters ) staging_func = _overload . into_staging ( func , extra_locals , verbose = verbose >= 2 ) try : # Initialize _overload to prepare for staging. _overload . __init__ () # Create a new scope for the function with LifetimeScope (): # Run staging function with the tensor program arguments' names as parameters returns = staging_func ( * params , __freetensor_transform_outermost__ = True ) # Check returned vardefs (if any) if isinstance ( returns , VarRef ): returns = [ returns ] elif isinstance ( returns , tuple ): for ret in returns : if not isinstance ( ret , VarRef ): raise _overload . error ( 'Illegal return at top level, need to be a `VarRef` or a tuple of `VarRef`s' ) returns = list ( returns ) elif returns is None : returns = [] else : raise _overload . error ( 'Illegal return at top level, need to be a `VarRef` or a tuple of `VarRef`s' ) # Set returned vardefs' access type to inout/output according to whether it was an input for ret in returns : if ret . vardef . atype == 'input' or ret . vardef . atype == 'inout' : ret . vardef . set_atype ( 'inout' ) else : ret . vardef . set_atype ( 'output' ) returns = [( ret . vardef . name , ret . vardef . dtype ) for ret in returns ] # Set closure; they are from captured Arrays. closure = _overload . closure except StagingError : raise except TransformError : raise except Exception as e : raise _overload . error ( 'Exception occurred in staging' ) from e finally : # Despite whether the exception is raised, we need to clean up the ctx_stack staged_ast = pop_ast () staged = None # Enable invoking a transformed AST in another function being transformed, # via `inlined_invoke` def prepare_inlined_invoke ( * args , ** kvs ): nonlocal staged if _overload . in_staging (): if len ( returns ) == 1 : names = ( func . __name__ ,) else : names = tuple ( f \" { func . __name__ } . { i } \" for i in range ( len ( returns ))) return _overload . register_inlined_invoke ( names , staged , args , kvs ) else : raise _overload . error ( 'Unexpected call on a transformed AST. A transformed AST can only ' 'be called in the following two ways: 1) called with actual data ' 'after `@optimize`, and 2) called from another function to be ' '`@transform`ed' ) staged = Func ( func . __name__ , params + list ( closure . keys ()), returns , staged_ast , closure , custom_callback = prepare_inlined_invoke ) if verbose >= 1 : print ( \"The transformed AST is:\" , file = sys . stderr ) print ( staged , file = sys . stderr ) print ( file = sys . stderr ) return staged var ( * args , ** kwargs ) \u00b6 Create an with variable a given initializer Parameters: initializer ( Sequence[Sequence[...Sequence[Expr]...]] ) \u2013 (Multi-level of) sequence of expressions. Will be data of the variable shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used Source code in freetensor/core/frontend.py def impl ( * args , ** kwargs ): if _overload . in_staging (): return staging ( * args , ** kwargs ) else : return original ( * args , ** kwargs ) optimize \u00b6 optimize ( func = None , schedule_callback = None , target = None , device = None , default_dynamic_range = True , verbose = None ) \u00b6 An one-click optimization from Python function to binary executable Usage: @optimize def f(...): ... It is equivalent to: @build_binary @codegen @lower @transform def f(...): ... Parameters: func ( Python function or AST ) \u2013 The user function to optimize. If not specified, a partial function will be returend, which can be used as a decorator schedule_callback ( Optional[Callable[[freetensor.core.schedule.Schedule], NoneType]] ) \u2013 Schedule(s) to apply target ( Optional[freetensor_ffi.Target] ) \u2013 The target architecture. You don't have to set target if you set device device ( Optional[freetensor.core.driver.Device] ) \u2013 Where to run the program default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( Optional[int] ) \u2013 Verbosity level. Can be 0, 1 or 2 Source code in freetensor/core/optimize.py def optimize ( func = None , schedule_callback : Optional [ Callable [[ Schedule ], None ]] = None , target : Optional [ Target ] = None , device : Optional [ Device ] = None , default_dynamic_range : bool = True , verbose : Optional [ int ] = None ): ''' An one-click optimization from Python function to binary executable Usage: ``` @optimize def f(...): ... ``` It is equivalent to: ``` @build_binary @codegen @lower @transform def f(...): ... ``` Parameters ---------- func : Python function or AST The user function to optimize. If not specified, a partial function will be returend, which can be used as a decorator schedule_callback : Callable (Optional) Schedule(s) to apply target : Target (Optional) The target architecture. You don't have to set target if you set device device : Device (Optional) Where to run the program default_dynamic_range : bool If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose : int (Optional) Verbosity level. Can be 0, 1 or 2 ''' if func is not None : if target is None and device is not None : target = device . target () if not issubclass ( type ( func ), ffi . AST ): ast = transform ( func , default_dynamic_range = default_dynamic_range , verbose = verbose ) else : ast = func ast = schedule ( ast , schedule_callback , verbose = verbose ) ast = lower ( ast , target , verbose = verbose ) code = codegen ( ast , target , verbose = verbose ) exe = build_binary ( code , device , verbose = verbose ) return exe else : return functools . partial ( optimize , schedule_callback = schedule_callback , target = target , device = device , default_dynamic_range = default_dynamic_range , verbose = verbose ) optimize_to_pytorch ( func = None , tapes =< GradTapeMode . NoReuseOnly : 2 > , forward_schedule_callback = None , backward_schedule_callback = None , target = None , device = None , default_dynamic_range = True , verbose = None ) \u00b6 Compile a FreeTensor function to a PyTorch call, whose gradient can be recognized by PyTorch The compiled function will be a typical PyTorch's \"function\" (rather than a PyTorch's \"module\"). Technically, this means it is a wrapper function around a PyTorch's Function 's apply method Schedules (if any) must be applied to the forward function and the backward function separated. For this reason, currently only first-order gradient is supported Parameters: func ( Python function or AST ) \u2013 The user function to optimize. If not specified, a partial function will be returend, which can be used as a decorator tapes ( Union[Sequence, freetensor_ffi.GradTapeMode] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history forward_schedule_callback ( Optional[Callable[[freetensor.core.schedule.Schedule], NoneType]] ) \u2013 Schedule(s) to apply to the forward function backward_schedule_callback ( Optional[Callable[[freetensor.core.schedule.Schedule], NoneType]] ) \u2013 Schedule(s) to apply to the backward function target ( Optional[freetensor_ffi.Target] ) \u2013 The target architecture. You don't have to set target if you set device device ( Optional[freetensor.core.driver.Device] ) \u2013 Where to run the program default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( Optional[int] ) \u2013 Verbosity level. Can be 0, 1 or 2 Source code in freetensor/core/optimize.py def optimize_to_pytorch ( func = None , tapes : Union [ Sequence , GradTapeMode ] = GradTapeMode . NoReuseOnly , forward_schedule_callback : Optional [ Callable [[ Schedule ], None ]] = None , backward_schedule_callback : Optional [ Callable [[ Schedule ], None ]] = None , target : Optional [ Target ] = None , device : Optional [ Device ] = None , default_dynamic_range : bool = True , verbose : Optional [ int ] = None ): ''' Compile a FreeTensor function to a PyTorch call, whose gradient can be recognized by PyTorch The compiled function will be a typical PyTorch's \"function\" (rather than a PyTorch's \"module\"). Technically, this means it is a wrapper function around a PyTorch's `Function`'s `apply` method Schedules (if any) must be applied to the forward function and the backward function separated. For this reason, currently only first-order gradient is supported Parameters ---------- func : Python function or AST The user function to optimize. If not specified, a partial function will be returend, which can be used as a decorator tapes : Union[Sequence, GradTapeMode] Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a `GradTapeMode`, then it will determine which intermediate variables to be stored by heuristics. Avail `GradTapeMode`s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history forward_schedule_callback : Callable (Optional) Schedule(s) to apply to the forward function backward_schedule_callback : Callable (Optional) Schedule(s) to apply to the backward function target : Target (Optional) The target architecture. You don't have to set target if you set device device : Device (Optional) Where to run the program default_dynamic_range : bool If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose : int (Optional) Verbosity level. Can be 0, 1 or 2 ''' if func is not None : import torch # Transform from Python source to AST if not issubclass ( type ( func ), ffi . AST ): ast = transform ( func , default_dynamic_range = default_dynamic_range , verbose = verbose ) else : ast = func # Compile lazily because we know `requires` and `provides` only when # executing. Re-compile when gradient requirements changes saved_requires = set () saved_provides = set () cur_requires = None cur_provides = None fwd_exe = None bwd_exe = None input_grad_map = None output_grad_map = None tape_rets = None def lazy_compile (): nonlocal saved_requires , saved_provides , cur_requires , cur_provides nonlocal fwd_exe , bwd_exe , input_grad_map , output_grad_map , tape_rets if saved_requires == cur_requires and saved_provides == cur_provides : return saved_requires = cur_requires saved_provides = cur_provides if len ( cur_requires ) != 0 : fwd_ast , bwd_ast , input_grad_map , output_grad_map = grad ( ast , requires = saved_requires , provides = saved_provides , tapes = tapes , # PyTorch requires explicitly marking saved states via # `save_for_backward()` tape_in_closure = False , verbose = verbose ) tape_rets = fwd_ast . returns [ len ( ast . returns ):] fwd_exe = optimize ( fwd_ast , forward_schedule_callback , target , device , default_dynamic_range , verbose ) bwd_exe = optimize ( bwd_ast , backward_schedule_callback , target , device , default_dynamic_range , verbose ) else : # No one needs grad. No need to do autograd fwd_ast = ast fwd_exe = optimize ( fwd_ast , forward_schedule_callback , target , device , default_dynamic_range , verbose ) bwd_exe = None input_grad_map = {} output_grad_map = {} tape_rets = [] # Generate a PyTorch Function class GeneratedPyTorchFunction ( torch . autograd . Function ): @staticmethod def forward ( ctx , * args , ** kvs ): nonlocal cur_requires , cur_provides # We only get to know provided gradients of output tensors when we # run `backward`, but we need to run autograd and compile the program # here in `forward`. We can only assume gradients are provided for # every output tensors, even if they are unrelated to the inputs. # Setting this option to True makes PyTorch generate zero gradient # for such outputs. (TODO: better solution?) ctx . set_materialize_grads ( True ) # Gather required gradients of the inputs cur_requires = set () for param , arg in zip ( ast . params , args ): if arg . requires_grad : cur_requires . add ( param . name ) for key , value in kvs . items (): if value . requires_grad : cur_requires . add ( key ) # For the reason above, we assume gradients are provided for every # output tensors cur_provides = set () for ret in ast . returns : cur_provides . add ( ret . name ) lazy_compile () fwd_exe . set_args ( * args , ** kvs ) fwd_exe . run () returns = fwd_exe . collect_returns ( always_return_pack = True ) returns = tuple ( item . torch () for item in returns ) # Save states for 1) all inputs and 2) all taped tensors (taped # outputs are also taped tensors). For taped tensors, we need to # make them output tensors, so PyTorch can recognize them. This is # an officially recommanded trick at # https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html#saving-intermediate-results # So, please be aware that only the first part in `returns` are real # return tensors saved_tensors = [] for arg in args : # 1) saved_tensors . append ( arg ) for ret in returns : # 2) and maybe other junks saved_tensors . append ( ret ) ctx . save_for_backward ( * saved_tensors ) return returns [ 0 ] if len ( returns ) == 1 else returns @staticmethod @torch . autograd . function . once_differentiable def backward ( ctx , * args , ** kvs ): saved_tensors = ctx . saved_tensors internal_kvs = {} for ret , arg in zip ( ast . returns , args ): internal_kvs [ output_grad_map [ ret . name ]] = arg for key , value in kvs : internal_kvs [ output_grad_map [ key ]] = value for param , saved in zip ( ast . params , saved_tensors ): # NOTE: Now we only support \"input\" parameters for PyTorch # interface (no \"inout\" or \"output\"), so we can forward all # parameters. If we support \"inout\" or \"output\" in the future, # we need to filter only \"input\" parameters here internal_kvs [ param . name ] = saved for tape_ret , saved in zip ( tape_rets , saved_tensors [ len ( ast . params ) + len ( ast . returns ):]): internal_kvs [ tape_ret . name ] = saved bwd_exe . set_args ( ** internal_kvs ) bwd_exe . run () input_grads = bwd_exe . collect_returns ( always_return_pack = True ) # PyTorch requires returning gradient of inputs in their original # order. If no gradient is required for an input, set it to None returns = tuple ( input_grads [ input_grad_map [ param . name ]] . torch ( ) if param . name in input_grad_map else None for param in ast . params ) return returns [ 0 ] if len ( returns ) == 1 else returns # Wrap around the PyTorch `Function`, to be a real Python \"function\", and # remove our extra tape outputs def generatedPyTorchFunction ( * args , ** kvs ): returns = GeneratedPyTorchFunction . apply ( * args , ** kvs ) returns_tuple = returns if isinstance ( returns , Sequence ) else ( returns ,) returns_tuple = returns_tuple [: len ( ast . returns )] return returns_tuple [ 0 ] if len ( returns_tuple ) == 1 else returns_tuple # If called inside a FreeTensor funcion, don't care about PyTorch, just # inline the transformed AST return staged_callable ( ast , generatedPyTorchFunction ) else : return functools . partial ( optimize_to_pytorch , tapes = tapes , forward_schedule_callback = forward_schedule_callback , backward_schedule_callback = backward_schedule_callback , target = target , device = device , default_dynamic_range = default_dynamic_range , verbose = verbose ) passes \u00b6 lower ( ast = None , target = None , skip_passes = None , verbose = None ) \u00b6 Lower an AST using a series of passes Parameters: ast ( AST ) \u2013 The AST to be lowered. Can be a Func or a Stmt . If not specified, a partial function of lower will be returned, which can be used as a decorator target ( Optional[freetensor_ffi.Target] ) \u2013 Lower the AST to a target with target-specific passes, then the AST can be used for codegen. If not set, use the default Target in Config skip_passes ( Optional[Sequence[str]] ) \u2013 Skip some pass for testing or debugging. Names in skip_passes are in underscore_style, as in Python. Please note that some passes will not be skipped even specified in these parameter, because they are indirectly called in some other passes verbose ( Optional[int] ) \u2013 0 = print nothing. 1 = print the lowered AST. 2 = print AST after every single passes Source code in freetensor/core/passes.py def lower ( ast = None , target : Optional [ ffi . Target ] = None , skip_passes : Optional [ Sequence [ str ]] = None , verbose : Optional [ int ] = None ): ''' Lower an AST using a series of passes Parameters ---------- ast : AST The AST to be lowered. Can be a `Func` or a `Stmt`. If not specified, a partial function of `lower` will be returned, which can be used as a decorator target : Target (Optional) Lower the AST to a target with target-specific passes, then the AST can be used for codegen. If not set, use the default Target in Config skip_passes : Sequence[str] (Optional) Skip some pass for testing or debugging. Names in `skip_passes` are in underscore_style, as in Python. Please note that some passes will not be skipped even specified in these parameter, because they are indirectly called in some other passes verbose : int (Optional) 0 = print nothing. 1 = print the lowered AST. 2 = print AST after every single passes ''' if ast is not None : return ffi . lower ( ast , target , set () if skip_passes is None else set ( skip_passes ), 0 if verbose is None else verbose ) else : _lower = lower if target is not None : _lower = functools . partial ( _lower , target = target ) if skip_passes is not None : _lower = functools . partial ( _lower , skip_passes = skip_passes ) if verbose is not None : _lower = functools . partial ( _lower , verbose = verbose ) return _lower schedule \u00b6 IDMap \u00b6 A dict-like container recording an ID-to-ID mapping, representing what IDs become what IDs after a schedule An IDMap can be looked up by numerical ID, or by Stmt instances or Selector strings of the original (before applying schedule) AST Source code in freetensor/core/schedule.py class IDMap : ''' A dict-like container recording an ID-to-ID mapping, representing what IDs become what IDs after a schedule An IDMap can be looked up by numerical ID, or by Stmt instances or Selector strings of the original (before applying schedule) AST ''' def __init__ ( self , old_ast , id_map : Dict [ ID , ID ]): self . old_ast = old_ast self . id_map = id_map def _lookup ( self , pattern : Union [ ID , ffi . Stmt , Selector , str ]) -> ID : if isinstance ( pattern , ID ): return pattern elif isinstance ( pattern , ffi . Stmt ): return pattern . id else : return find_stmt ( self . old_ast , Selector ( pattern )) . id def __contains__ ( self , key ): return self . _lookup ( key ) in self . id_map def __getitem__ ( self , key ): return self . id_map [ self . _lookup ( key )] def __iter__ ( self ): return iter ( self . map ) Schedule ( Schedule ) \u00b6 Source code in freetensor/core/schedule.py class Schedule ( ffi . Schedule ): def _lookup ( self , pattern : Union [ ID , ffi . Stmt , Selector , str ]) -> ID : if isinstance ( pattern , ID ): return pattern elif isinstance ( pattern , ffi . Stmt ): return pattern . id else : return self . find ( Selector ( pattern )) . id def _lookup_list ( self , pattern : Union [ ID , List [ ID ], ffi . Stmt , List [ ffi . Stmt ], Selector , List [ Selector ], str , List [ str ]] ) -> List [ ID ]: if isinstance ( pattern , Sequence ) and not isinstance ( pattern , str ): return functools . reduce ( lambda x , y : x + y , map ( self . _lookup_list , pattern )) elif isinstance ( pattern , ID ): return [ pattern ] elif isinstance ( pattern , ffi . Stmt ): return [ pattern . id ] else : return [ item . id for item in self . find_at_least_one ( Selector ( pattern )) ] def __init__ ( self , arg , verbose : int = 0 ): if isinstance ( arg , ffi . Schedule ): # from native Schedule object super () . __init__ ( arg ) else : # create a new schedule from a program super () . __init__ ( arg , verbose ) def ast ( self ): \"\"\" Get the scheduled AST without function signature This is mainly for debugging and testting purpose \"\"\" ret = super () . ast () if self . verbose >= 1 : print ( f \"The scheduled AST is: \\n { ret } \" ) return ret def func ( self ): \"\"\" Get the scheduled function \"\"\" ret = super () . func () if self . verbose >= 1 : print ( f \"The scheduled Func is: \\n { ret } \" ) return ret def fork ( self ): return Schedule ( super () . fork ()) def split ( self , node , factor =- 1 , nparts =- 1 , shift = 0 ): \"\"\" Split a loop into two nested loops To fission a loop into two consecutive loops, use `fission` instead Two modes are provided: 1. Specify `factor` and leave `nparts` to -1. It will result in an outer loop with length `ceil(n / factor)`, and an inner loop with length `factor`, where `n` is the original loop length added by `shift`. The original iterator `i` will be transformed to `i0 * factor + i1`, where `i0` and `i1` are the iterators of the new outer and inner loops, respectively 2. Specify `nparts` and leave `factor` to -1. It will result in an outer loop with length `nparts`, and an inner loop with length `ceil(n / nparts)`, where `n` is the original loop length added by `shift`. The original iterator `i` will be transformed to `i0 * ceil(n / nparts) + i1`, where `i0` and `i1` are the iterators of the new outer and inner loops, respectively Please note that the second mode will introduce an `i0 * ceil(n / nparts)` factor into the program, which cannot be recognized by polyhedral analysis, which may hinder some following schedules. If possible, plese use the first mode, and then reorder the inner and outer loops Suppose the original loop is labeled \"L\", the split two loops can be selected by \"$split.0{L}\" (the outer loop) and \"$split.1{L}\" (the inner loop). If one of the resulting loop is proved to have only a single iteration, it will be removed Parameters ---------- node : str, ID or Stmt The loop to be split factor : int Length of the inner loop. Set to -1 if using `nparts` nparts : int Length of the outer loop. Set to -1 if using `factor` Raises ------ InvalidSchedule if the loop is not found Returns ------- (Optional[ID], Optional[ID]) (outer loop ID, inner loop ID), either ID can be None if the loop is proved to have only a single iteration \"\"\" return ( i if i else None for i in super () . split ( self . _lookup ( node ), factor , nparts , shift )) def reorder ( self , order ): \"\"\" Reorder directly nested loops To swap consecutive loops, use `swap` instead Parameters ---------- order : array like of str, ID or Stmt Vector of loops. The requested order of the loops Raises ------ InvalidSchedule if the input is invalid or there are breaking dependences \"\"\" super () . reorder ( list ( map ( self . _lookup , order ))) def merge ( self , loop1 , loop2 ): \"\"\" Merge two directly nested loops into one To fuse consecutive loops, use `fuse` instead `parallelize`, `unroll` and `vectorize` properties will be reset on the merged loop Suppose the original loops are labeled \"L1\" and \"L2\", the merged loop can be selected by \"$merge{L1, L2}\" Parameters ---------- loop1, loop2 : str, ID or Stmt loops to be merged, can be in any order Raises ------ InvalidSchedule if the loops are not directly nested Returns ------- ID ID of the merged loop \"\"\" return super () . merge ( self . _lookup ( loop1 ), self . _lookup ( loop2 )) def permute ( self , loops , transform_func ): \"\"\" Permute perfectly nested loops (directly nested loops without statements in between) with the given loop space transformation function The transformed loops follow ascending lexical order of the transformed terms returned by `transformFunc` when called with original iteration Parameters ---------- loops : array like of str, ID or Stmt the list of perfectly nested loops to be permuted transform_func : Callable[[Expr], Expr] the loop space transformation function, should be bijective Returns ------- list of ID the list of IDs of permuted loops \"\"\" return super () . permute ([ self . _lookup ( l ) for l in loops ], transform_func ) def fission ( self , loop , side , splitter ): \"\"\" Fission a loop into two loops each containing part of the statements, one followed by another To split loop into two nested loops, use `split` instead Statements inside the original loop will be distributed to one or both (happening if they are scope statements) loops. If a statement is originally labeled \"S\", it can be selected by \"$fission.0{S}\" (from the first loop) or \"$fission.1{S}\" (from the second loop) after fission. If one of the resulting loop has an empty body, it will be removed Parameters ---------- loop : str, ID or Stmt The loop to be fissioned side : FissionSide If `After`, `splitter` is the last statement of the first loop. If `Before`, `splitter` is the first statement of the second loop splitter : str (Selector string), ID, Stmt, or list of them Where to fission the loop. If multiple statement are selected, fission the look before or after all of them Raises ------ InvalidSchedule if any dependence cannot be resolved Returns ------- (IDMap, IDMap) ({old ID -> new ID in 1st loop}, {old ID -> new ID in 2nd loop}). If a loop is removed because it has an empty body, it will not be in the returned map \"\"\" old_ast = self . ast () splitter_list = self . _lookup_list ( splitter ) # In DFS order if side == FissionSide . Before : splitter = splitter_list [ 0 ] else : splitter = splitter_list [ - 1 ] map1 , map2 = super () . fission ( self . _lookup ( loop ), side , splitter ) return IDMap ( old_ast , map1 ), IDMap ( old_ast , map2 ) def fuse ( self , loop0 , loop1 = None , strict = False ): \"\"\" Fuse two directly following loops with the same length into one To merge nested loops into one, use `merge` instead `parallelize`, `unroll` and `vectorize` properties will be reset on the fused loop Suppose the original loops are labeled \"L1\" and \"L2\", the fused loop can be selected by \"$fuse{L1, L2}\" Parameters ---------- loop0 : str, ID or Stmt The leading loop loop1 : str, ID or Stmt, Optional The following loop. If omitted, it will try to find a following loop of `loop0` strict : bool False by default. If set to True, throw an error if unable to determine whether the two loops are of the same length Raises ------ InvalidSchedule if the two loops are not directly following, the two loops are not of the same length, or there is any dependence cannot be resolved Returns ------- ID ID of the result loop \"\"\" if loop1 is None : return super () . fuse ( self . _lookup ( loop0 ), strict ) else : return super () . fuse ( self . _lookup ( loop0 ), self . _lookup ( loop1 ), strict ) def swap ( self , order ): \"\"\" Swap statements in the same block To reorder nested loops, use `reorder` instead Parameters ---------- order : List[str (Selector string), ID, List[ID], Stmt, or List[Stmt]] The statements. If one item of the `order` list contains multiple statements, the `order` list will be flattened Raises ------ InvalidSchedule if the statements are not found or the dependences cannot be solved \"\"\" super () . swap ( self . _lookup_list ( order )) def blend ( self , loop ): \"\"\" Unroll a loop and interleave statements from each iteration E.g. ``` for i = 0 to 2 { f(i); g(i); } ``` will be transformed to be ``` f(0); f(1); g(0); g(1); ``` Virtual threads in TVM can be implemented via blend Parameters ---------- loop : str, ID or Stmt The loop being transformed Raises ------ InvalidSchedule if the loop is not found, the loop length is not a constant, or the dependences cannot be solved \"\"\" super () . blend ( self . _lookup ( loop )) def cache ( self , stmt , var , mtype ): \"\"\" Cache a variable into a new local variable All needed data will be filled into the cache first, then all reads and writes will be directed to the cache, and finally all needed data will be flushed from the cache Note for reduction: This transformation preserves the computation order. It will transform ``` a += x a += y ``` to ``` a.cache = a + x + y a = a.cache ``` If you need a \"real\" cache for reduction, which reorders the computation, use `cache_reduction` instead Parameters ---------- stmt : str, ID or Stmt The statement or block (e.g. an If or a For) to be modified var : str Name of the variable to be cached mtype : MemType Where to cache Raises ------ InvalidSchedule if the ID or name is not found Returns ------- (ID, ID, ID, ID) (ID of the statement that fills the cache, ID of the statement that flushes from the cache, name of the cache variable, ID of the VarDef node of the cache variable) \"\"\" return super () . cache ( self . _lookup ( stmt ), var , MemType ( mtype )) def cache_reduction ( self , stmt , var , mtype ): \"\"\" Perform local reductions (e.g. sum) in a local variable first, and then reduce the local result to the global variable E.g. ``` a += x a += y ``` will be transformed to be ``` a.cache = x + y a += a.cache ``` Parameters ---------- stmt : str, ID or Stmt The statement or block (e.g. an If or a For) to be modified var : str Name of the variable to be cached. Only reductions are allowed on `var` in `stmt`. Plain reads or writes are not allowed mtype : MemType Where to cache Raises ------ InvalidSchedule if the ID or name is not found, or there are unsupported reads or writes Returns ------- (ID, ID, ID, ID) (ID of the statement that initialize the cache, ID of the statement that reduces the local result to the global result, name of the cache variable, ID of the VarDef node of the cache variable) \"\"\" return super () . cache_reduction ( self . _lookup ( stmt ), var , MemType ( mtype )) def set_mem_type ( self , vardef , mtype ): \"\"\" Change where a variable is stored Parameters ---------- vardef : str, ID or Stmt ID of the VarDef statement of the specific variable mtype : MemType Where the variable should be stored Raises ------ InvalidSchedule if the variable is not found \"\"\" super () . set_mem_type ( self . _lookup ( vardef ), MemType ( mtype )) def var_split ( self , vardef , dim , mode , factor =- 1 , nparts =- 1 ): \"\"\" Split a dimension of a variable into two Parameters ---------- vardef : str, ID or Stmt ID of the VarDef statement of the specific variable dim : int which dimension to be split mode : VarSplitMode When the dimension to split is not divisible by `factor` or `nparts`, the resulting shape may become larger. In `FixedSize` mode, the actual buffer size will not be changed, and gurads will be added to prevent out-of-bound accesses. In `RelaxedSize` mode, the buffer size may increase. The `RelaxedSize` mode cannot be applied to I/O variables factor : int Length of the inner (higher no.) dimension. Set to -1 if using `nparts` nparts : int Length of the outer (lower no.) loop. Set to -1 if using `factor` Raises ------ InvalidSchedule if the variable or the dimension is not found \"\"\" return super () . var_split ( self . _lookup ( vardef ), dim , mode , factor , nparts ) def var_merge ( self , vardef , dim ): \"\"\" Merge two dimensions of a variable Parameters ---------- vardef : str, ID or Stmt ID of the VarDef statement of the specific variable dim : int Merge the `dim`-th and the `(dim + 1)`-th dimension \"\"\" return super () . var_merge ( self . _lookup ( vardef ), dim ) def var_reorder ( self , vardef , order ): \"\"\" Reorder the dimensions of a variable Parameters ---------- vardef : str, ID or Stmt ID of the VarDef statement of the specific variable order : array like of str, ID or Stmt Vector of integers. The new order of the dimensions Raises ------ InvalidSchedule if the variable or the order is illegal \"\"\" return super () . var_reorder ( self . _lookup ( vardef ), order ) def move_to ( self , stmt , side , dst ): \"\"\" Move a statement to a new position This is a composite schedule command, which is implemented with other commands If moving a statement out of some loops, identical loops will be added around the moved statement, which is equivalent to fission these loops Parameters ---------- stmt : str, ID or Stmt The statement to be moved side : MoveToSide Whether `stmt` will be BEFORE or AFTER `dst dst : str (Selector string), ID, Stmt, or list of them Insert `stmt` to be directly after this statement. If multiple statements are selected, move to before or after all of them Raises ------ InvalidSchedule if there is no feasible path to move Returns ------- (ID, ID) (The new ID of the moved statement, The out-most newly introduced statments including the added loops) \"\"\" dst_list = self . _lookup_list ( dst ) # In DFS order if side == MoveToSide . Before : dst = dst_list [ 0 ] else : dst = dst_list [ - 1 ] return super () . move_to ( self . _lookup ( stmt ), side , dst ) def inline ( self , vardef ): \"\"\" Remove a variable. When the variable is used, recompute its value Parameters ---------- vardef : str, ID or Stmt The VarDef statement of the specific variable. It can not be an I/O varible Raises ------ InvalidSchedule if the variable cannot be completely removed \"\"\" return super () . inline ( self . _lookup ( vardef )) def parallelize ( self , loop , parallel ): \"\"\" Mark a loop with a parallel implementation This schedule follows a fork-join model: multiple workers (abstract threads) are created (but physically the threads may be cached in a thread pool) when the loop begins, do their jobs in parallel, and join when the loop ends OpenMP threads follow a typical fork-join model. CUDA threads run in a bulk-synchronous parallel (BSP) model, which can also be mimiked by the fork-join model: All threads start when the kernel get launched, but they only begin to do their jobs when the parallel loop begins. Nevertheless, the fork-join model needs the following extension to fully mimic a BSP model: Taking CUDA as an example, we allow binding a loop to `threadIdx.x` inside another loop bound to `threadIdx.x`, which is illegal in a classic fork-join model. For example, we may implement a matmul with collaborative fetch as below: ``` for i : threadIdx.x # Li for j : threadIdx.y # Lj local_sum = 0 # In gpu/local memory, unique to (i, j) for k0 # Lk0 for k : threadIdx.y # Lk1_a A_cache[k] = A[i, k] # In gpu/shared, shared by different j for k : threadIdx.x # Lk1_b B_cache[k] = B[k, j] # In gpu/shared, shared by different i for k # Lk1_c sum += A_cache[k] * B_cache[k] C[i, j] = local_sum ``` A seemingly plausible solution to avoid this extension is to reorder `Lk0` to outer-most, and then move `Lk1_a` and `Lk1_b` out of `Li` or `Lj`. This resolves the nested `threadIdx.x` and `threadIdx.y` binding problem by running `Li+Lk1_a`, `Lj+Lk1_b` and `Li+Lj` interleavingly, instead of running `Lk1_a` and `Lk1_b` inside `Li+Lj`. However, this approach is illegal, because the local variable `local_sum` can no longer be kept inside the body of `Li` and `Lj`: It has to be reused across multiple runs of `Li` and `Lj` Please also note that we can bind one `threadIdx.x` to two loops only when the body statement is loop-invariant to one of them. For example, the following binding is still illegal, even in our extended fork-join model, because it violates its serial semantics: ``` for i : threadIdx.x for j : threadIdx.x A[i, j] ++ ``` Parameters ---------- loop : str, ID or Stmt The loop parallel : ParallelScope Parallel scope \"\"\" super () . parallelize ( self . _lookup ( loop ), ParallelScope ( parallel )) def unroll ( self , loop , immediate = False ): \"\"\" Unroll a loop Parameters ---------- loop : str, ID or Stmt ID of the loop immediate : bool If false (by default), postpone the unroll procedure to the backend compiler, which saves scheduling time. If true, unroll the loop immediately, which may help further simplifications based on the unrolled result. If your purpose is just to fill the instruction cache, set it to false. If you are unrolling a loop that computes array indices, set it to true Raises ------ InvalidSchedule if the loop is not found or length of the loop is not a constant \"\"\" super () . unroll ( self . _lookup ( loop ), immediate ) def vectorize ( self , loop ): \"\"\" Vectorize a loop Please note that, as vectorization is different from architecture to achitecture, the scheduler may or may not postpone it to the backend compiler. The vectorization is a best-effort schedule Parameters ---------- loop : str, ID or Stmt ID of the loop Raises ------ InvalidSchedule if the ID or name is not found, or the dependence requirement is not met \"\"\" super () . vectorize ( self . _lookup ( loop )) def separate_tail ( self , noDuplicateVarDefs = False ): \"\"\" Seperate main iterations and tail iterations of a loop E.g. ``` for i = 0 -> 3 { for j = 0 -> 4 { if (i * 4 + j < 10) { ... } } } ``` Each loop will be separated into 2 parts: the body and the tail. After simplification, the program will finally be transformed to ``` for i = 0 -> 2 { for j = 0 -> 4 { ... } } for j = 0 -> 2 { ... } ``` Ideally, all programs can benefit from this schedule. However, this schedule may greatly increase the program size and make the compiling time way too long. Therefore, this transformation is implemented as a schedule, which can be applied optionally. (TODO: Optionally apply this schedule to part of the program) Parameters ---------- noDuplicateVarDefs : bool If there is two VarDef nodes in two branches, it may result in doubled memory use, since different thread may go to different branch. Set this parameter to true to stop duplicating VarDef nodes. \"\"\" super () . separate_tail ( noDuplicateVarDefs ) def as_matmul ( self , loop ): \"\"\" Transform nested loops to be a external call to a matrix multiplication Parameters ---------- loop : str, ID or Stmt ID of the loop Raises ------ InvalidSchedule if the loop cannot be transformed to be a matrix multiplication \"\"\" super () . as_matmul ( self . _lookup ( loop )) def pluto_fuse ( self , loop0 , loop1 , nest_level_0 = 0 , nest_level_1 = 0 , fusable_overlap_threshold = 1 , do_simplify = True ): \"\"\" Use Pluto+ algorithm to permute and fuse two loops, with as most parallelizable loops as possible at outermost levels. The two loops are required to be consequent; all directly nested levels are detected and subject to permutation. Remaining levels that cannot be fused are left inside the fused loops as two statements Parameters ---------- loop0 : str, ID or Stmt The first loop to fuse loop1 : str, ID or Stmt The second loop to fuse nest_level_0 : int The number of nesting levels of loop 0 to be considered, defaults to maximum possible nest_level_1 : int The number of nesting levels of loop 1 to be considered, defaults to maximum possible fusableOverlapThreshold : int The minimum overlapping size of two loops to be regarded fusable. Defaults to 1 do_simplify : bool Whether the result is simplified by the way, defaults to true Returns ------- (ID, int) The ID of fused loop and level of parallelizable loops Raises ------ InvalidSchedule if the loops are not consequent \"\"\" return super () . pluto_fuse ( self . _lookup ( loop0 ), self . _lookup ( loop1 ), nest_level_0 , nest_level_1 , fusable_overlap_threshold , do_simplify ) def pluto_permute ( self , loop , nest_level = 0 , do_simplify = True ): \"\"\" Use Pluto+ algorithm to permute a single loop, with as most parallelizable loops as possible at outermost levels. Parameters ---------- loop : str, ID or Stmt The loop to permute nest_level : int The number of nesting levels to be considered, defaults to maximum possible do_simplify : bool Whether the result is simplified by the way, defaults to true Returns ------- (ID, int) The ID of permuted loop and level of parallelizable loops \"\"\" return super () . pluto_permute ( self . _lookup ( loop ), nest_level , do_simplify ) def auto_schedule ( self , target ): \"\"\" (Experimental) Automatic scheduling using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_schedule ( target ) def auto_use_lib ( self , target ): \"\"\" (Experimental) Automatically use external libs using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_use_lib ( target ) def auto_fuse ( self , target ): \"\"\" (Experimental) Automatically fuse consecutive loops using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_fuse ( target ) def auto_parallelize ( self , target ): \"\"\" (Experimental) Automatically parallelize some loops using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_parallelize ( target ) def auto_set_mem_type ( self , target ): \"\"\" (Experimental) Automatically set memory types using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_set_mem_type ( target ) def auto_unroll ( self , target ): \"\"\" (Experimental) Automatically unroll loops using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_unroll ( target ) as_matmul ( self , loop ) \u00b6 Transform nested loops to be a external call to a matrix multiplication Parameters: loop ( str, ID or Stmt ) \u2013 ID of the loop Exceptions: InvalidSchedule \u2013 if the loop cannot be transformed to be a matrix multiplication Source code in freetensor/core/schedule.py def as_matmul ( self , loop ): \"\"\" Transform nested loops to be a external call to a matrix multiplication Parameters ---------- loop : str, ID or Stmt ID of the loop Raises ------ InvalidSchedule if the loop cannot be transformed to be a matrix multiplication \"\"\" super () . as_matmul ( self . _lookup ( loop )) ast ( self ) \u00b6 Get the scheduled AST without function signature This is mainly for debugging and testting purpose Source code in freetensor/core/schedule.py def ast ( self ): \"\"\" Get the scheduled AST without function signature This is mainly for debugging and testting purpose \"\"\" ret = super () . ast () if self . verbose >= 1 : print ( f \"The scheduled AST is: \\n { ret } \" ) return ret auto_fuse ( self , target ) \u00b6 (Experimental) Automatically fuse consecutive loops using some heuristics Parameters: target ( Target ) \u2013 Target architecture Source code in freetensor/core/schedule.py def auto_fuse ( self , target ): \"\"\" (Experimental) Automatically fuse consecutive loops using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_fuse ( target ) auto_parallelize ( self , target ) \u00b6 (Experimental) Automatically parallelize some loops using some heuristics Parameters: target ( Target ) \u2013 Target architecture Source code in freetensor/core/schedule.py def auto_parallelize ( self , target ): \"\"\" (Experimental) Automatically parallelize some loops using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_parallelize ( target ) auto_schedule ( self , target ) \u00b6 (Experimental) Automatic scheduling using some heuristics Parameters: target ( Target ) \u2013 Target architecture Source code in freetensor/core/schedule.py def auto_schedule ( self , target ): \"\"\" (Experimental) Automatic scheduling using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_schedule ( target ) auto_set_mem_type ( self , target ) \u00b6 (Experimental) Automatically set memory types using some heuristics Parameters: target ( Target ) \u2013 Target architecture Source code in freetensor/core/schedule.py def auto_set_mem_type ( self , target ): \"\"\" (Experimental) Automatically set memory types using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_set_mem_type ( target ) auto_unroll ( self , target ) \u00b6 (Experimental) Automatically unroll loops using some heuristics Parameters: target ( Target ) \u2013 Target architecture Source code in freetensor/core/schedule.py def auto_unroll ( self , target ): \"\"\" (Experimental) Automatically unroll loops using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_unroll ( target ) auto_use_lib ( self , target ) \u00b6 (Experimental) Automatically use external libs using some heuristics Parameters: target ( Target ) \u2013 Target architecture Source code in freetensor/core/schedule.py def auto_use_lib ( self , target ): \"\"\" (Experimental) Automatically use external libs using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_use_lib ( target ) blend ( self , loop ) \u00b6 Unroll a loop and interleave statements from each iteration E.g. for i = 0 to 2 { f(i); g(i); } will be transformed to be f(0); f(1); g(0); g(1); Virtual threads in TVM can be implemented via blend Parameters: loop ( str, ID or Stmt ) \u2013 The loop being transformed Exceptions: InvalidSchedule \u2013 if the loop is not found, the loop length is not a constant, or the dependences cannot be solved Source code in freetensor/core/schedule.py def blend ( self , loop ): \"\"\" Unroll a loop and interleave statements from each iteration E.g. ``` for i = 0 to 2 { f(i); g(i); } ``` will be transformed to be ``` f(0); f(1); g(0); g(1); ``` Virtual threads in TVM can be implemented via blend Parameters ---------- loop : str, ID or Stmt The loop being transformed Raises ------ InvalidSchedule if the loop is not found, the loop length is not a constant, or the dependences cannot be solved \"\"\" super () . blend ( self . _lookup ( loop )) cache ( self , stmt , var , mtype ) \u00b6 Cache a variable into a new local variable All needed data will be filled into the cache first, then all reads and writes will be directed to the cache, and finally all needed data will be flushed from the cache Note for reduction: This transformation preserves the computation order. It will transform a += x a += y to a.cache = a + x + y a = a.cache If you need a \"real\" cache for reduction, which reorders the computation, use cache_reduction instead Parameters: stmt ( str, ID or Stmt ) \u2013 The statement or block (e.g. an If or a For) to be modified var ( str ) \u2013 Name of the variable to be cached mtype ( MemType ) \u2013 Where to cache Exceptions: InvalidSchedule \u2013 if the ID or name is not found Returns: (ID, ID, ID, ID) \u2013 (ID of the statement that fills the cache, ID of the statement that flushes from the cache, name of the cache variable, ID of the VarDef node of the cache variable) Source code in freetensor/core/schedule.py def cache ( self , stmt , var , mtype ): \"\"\" Cache a variable into a new local variable All needed data will be filled into the cache first, then all reads and writes will be directed to the cache, and finally all needed data will be flushed from the cache Note for reduction: This transformation preserves the computation order. It will transform ``` a += x a += y ``` to ``` a.cache = a + x + y a = a.cache ``` If you need a \"real\" cache for reduction, which reorders the computation, use `cache_reduction` instead Parameters ---------- stmt : str, ID or Stmt The statement or block (e.g. an If or a For) to be modified var : str Name of the variable to be cached mtype : MemType Where to cache Raises ------ InvalidSchedule if the ID or name is not found Returns ------- (ID, ID, ID, ID) (ID of the statement that fills the cache, ID of the statement that flushes from the cache, name of the cache variable, ID of the VarDef node of the cache variable) \"\"\" return super () . cache ( self . _lookup ( stmt ), var , MemType ( mtype )) cache_reduction ( self , stmt , var , mtype ) \u00b6 Perform local reductions (e.g. sum) in a local variable first, and then reduce the local result to the global variable E.g. a += x a += y will be transformed to be a.cache = x + y a += a.cache Parameters: stmt ( str, ID or Stmt ) \u2013 The statement or block (e.g. an If or a For) to be modified var ( str ) \u2013 Name of the variable to be cached. Only reductions are allowed on var in stmt . Plain reads or writes are not allowed mtype ( MemType ) \u2013 Where to cache Exceptions: InvalidSchedule \u2013 if the ID or name is not found, or there are unsupported reads or writes Returns: (ID, ID, ID, ID) \u2013 (ID of the statement that initialize the cache, ID of the statement that reduces the local result to the global result, name of the cache variable, ID of the VarDef node of the cache variable) Source code in freetensor/core/schedule.py def cache_reduction ( self , stmt , var , mtype ): \"\"\" Perform local reductions (e.g. sum) in a local variable first, and then reduce the local result to the global variable E.g. ``` a += x a += y ``` will be transformed to be ``` a.cache = x + y a += a.cache ``` Parameters ---------- stmt : str, ID or Stmt The statement or block (e.g. an If or a For) to be modified var : str Name of the variable to be cached. Only reductions are allowed on `var` in `stmt`. Plain reads or writes are not allowed mtype : MemType Where to cache Raises ------ InvalidSchedule if the ID or name is not found, or there are unsupported reads or writes Returns ------- (ID, ID, ID, ID) (ID of the statement that initialize the cache, ID of the statement that reduces the local result to the global result, name of the cache variable, ID of the VarDef node of the cache variable) \"\"\" return super () . cache_reduction ( self . _lookup ( stmt ), var , MemType ( mtype )) fission ( self , loop , side , splitter ) \u00b6 Fission a loop into two loops each containing part of the statements, one followed by another To split loop into two nested loops, use split instead Statements inside the original loop will be distributed to one or both (happening if they are scope statements) loops. If a statement is originally labeled \"S\", it can be selected by \"$fission.0{S}\" (from the first loop) or \"$fission.1{S}\" (from the second loop) after fission. If one of the resulting loop has an empty body, it will be removed Parameters: loop ( str, ID or Stmt ) \u2013 The loop to be fissioned side ( FissionSide ) \u2013 If After , splitter is the last statement of the first loop. If Before , splitter is the first statement of the second loop splitter ( str (Selector string), ID, Stmt, or list of them ) \u2013 Where to fission the loop. If multiple statement are selected, fission the look before or after all of them Exceptions: InvalidSchedule \u2013 if any dependence cannot be resolved Returns: (IDMap, IDMap) \u2013 ({old ID -> new ID in 1st loop}, {old ID -> new ID in 2nd loop}). If a loop is removed because it has an empty body, it will not be in the returned map Source code in freetensor/core/schedule.py def fission ( self , loop , side , splitter ): \"\"\" Fission a loop into two loops each containing part of the statements, one followed by another To split loop into two nested loops, use `split` instead Statements inside the original loop will be distributed to one or both (happening if they are scope statements) loops. If a statement is originally labeled \"S\", it can be selected by \"$fission.0{S}\" (from the first loop) or \"$fission.1{S}\" (from the second loop) after fission. If one of the resulting loop has an empty body, it will be removed Parameters ---------- loop : str, ID or Stmt The loop to be fissioned side : FissionSide If `After`, `splitter` is the last statement of the first loop. If `Before`, `splitter` is the first statement of the second loop splitter : str (Selector string), ID, Stmt, or list of them Where to fission the loop. If multiple statement are selected, fission the look before or after all of them Raises ------ InvalidSchedule if any dependence cannot be resolved Returns ------- (IDMap, IDMap) ({old ID -> new ID in 1st loop}, {old ID -> new ID in 2nd loop}). If a loop is removed because it has an empty body, it will not be in the returned map \"\"\" old_ast = self . ast () splitter_list = self . _lookup_list ( splitter ) # In DFS order if side == FissionSide . Before : splitter = splitter_list [ 0 ] else : splitter = splitter_list [ - 1 ] map1 , map2 = super () . fission ( self . _lookup ( loop ), side , splitter ) return IDMap ( old_ast , map1 ), IDMap ( old_ast , map2 ) fork ( self ) \u00b6 fork(self: freetensor_ffi.Schedule) -> freetensor_ffi.Schedule Source code in freetensor/core/schedule.py def fork ( self ): return Schedule ( super () . fork ()) func ( self ) \u00b6 Get the scheduled function Source code in freetensor/core/schedule.py def func ( self ): \"\"\" Get the scheduled function \"\"\" ret = super () . func () if self . verbose >= 1 : print ( f \"The scheduled Func is: \\n { ret } \" ) return ret fuse ( self , loop0 , loop1 = None , strict = False ) \u00b6 Fuse two directly following loops with the same length into one To merge nested loops into one, use merge instead parallelize , unroll and vectorize properties will be reset on the fused loop Suppose the original loops are labeled \"L1\" and \"L2\", the fused loop can be selected by \"$fuse{L1, L2}\" Parameters: loop0 ( str, ID or Stmt ) \u2013 The leading loop loop1 ( str, ID or Stmt, Optional ) \u2013 The following loop. If omitted, it will try to find a following loop of loop0 strict ( bool ) \u2013 False by default. If set to True, throw an error if unable to determine whether the two loops are of the same length Exceptions: InvalidSchedule \u2013 if the two loops are not directly following, the two loops are not of the same length, or there is any dependence cannot be resolved Returns: ID \u2013 ID of the result loop Source code in freetensor/core/schedule.py def fuse ( self , loop0 , loop1 = None , strict = False ): \"\"\" Fuse two directly following loops with the same length into one To merge nested loops into one, use `merge` instead `parallelize`, `unroll` and `vectorize` properties will be reset on the fused loop Suppose the original loops are labeled \"L1\" and \"L2\", the fused loop can be selected by \"$fuse{L1, L2}\" Parameters ---------- loop0 : str, ID or Stmt The leading loop loop1 : str, ID or Stmt, Optional The following loop. If omitted, it will try to find a following loop of `loop0` strict : bool False by default. If set to True, throw an error if unable to determine whether the two loops are of the same length Raises ------ InvalidSchedule if the two loops are not directly following, the two loops are not of the same length, or there is any dependence cannot be resolved Returns ------- ID ID of the result loop \"\"\" if loop1 is None : return super () . fuse ( self . _lookup ( loop0 ), strict ) else : return super () . fuse ( self . _lookup ( loop0 ), self . _lookup ( loop1 ), strict ) inline ( self , vardef ) \u00b6 Remove a variable. When the variable is used, recompute its value Parameters: vardef ( str, ID or Stmt ) \u2013 The VarDef statement of the specific variable. It can not be an I/O varible Exceptions: InvalidSchedule \u2013 if the variable cannot be completely removed Source code in freetensor/core/schedule.py def inline ( self , vardef ): \"\"\" Remove a variable. When the variable is used, recompute its value Parameters ---------- vardef : str, ID or Stmt The VarDef statement of the specific variable. It can not be an I/O varible Raises ------ InvalidSchedule if the variable cannot be completely removed \"\"\" return super () . inline ( self . _lookup ( vardef )) merge ( self , loop1 , loop2 ) \u00b6 Merge two directly nested loops into one To fuse consecutive loops, use fuse instead parallelize , unroll and vectorize properties will be reset on the merged loop Suppose the original loops are labeled \"L1\" and \"L2\", the merged loop can be selected by \"$merge{L1, L2}\" Parameters: loop1, loop2 ( str, ID or Stmt ) \u2013 loops to be merged, can be in any order Exceptions: InvalidSchedule \u2013 if the loops are not directly nested Returns: ID \u2013 ID of the merged loop Source code in freetensor/core/schedule.py def merge ( self , loop1 , loop2 ): \"\"\" Merge two directly nested loops into one To fuse consecutive loops, use `fuse` instead `parallelize`, `unroll` and `vectorize` properties will be reset on the merged loop Suppose the original loops are labeled \"L1\" and \"L2\", the merged loop can be selected by \"$merge{L1, L2}\" Parameters ---------- loop1, loop2 : str, ID or Stmt loops to be merged, can be in any order Raises ------ InvalidSchedule if the loops are not directly nested Returns ------- ID ID of the merged loop \"\"\" return super () . merge ( self . _lookup ( loop1 ), self . _lookup ( loop2 )) move_to ( self , stmt , side , dst ) \u00b6 Move a statement to a new position This is a composite schedule command, which is implemented with other commands If moving a statement out of some loops, identical loops will be added around the moved statement, which is equivalent to fission these loops Parameters: stmt ( str, ID or Stmt ) \u2013 The statement to be moved side ( MoveToSide ) \u2013 Whether stmt will be BEFORE or AFTER `dst dst ( str (Selector string), ID, Stmt, or list of them ) \u2013 Insert stmt to be directly after this statement. If multiple statements are selected, move to before or after all of them Exceptions: InvalidSchedule \u2013 if there is no feasible path to move Returns: (ID, ID) \u2013 (The new ID of the moved statement, The out-most newly introduced statments including the added loops) Source code in freetensor/core/schedule.py def move_to ( self , stmt , side , dst ): \"\"\" Move a statement to a new position This is a composite schedule command, which is implemented with other commands If moving a statement out of some loops, identical loops will be added around the moved statement, which is equivalent to fission these loops Parameters ---------- stmt : str, ID or Stmt The statement to be moved side : MoveToSide Whether `stmt` will be BEFORE or AFTER `dst dst : str (Selector string), ID, Stmt, or list of them Insert `stmt` to be directly after this statement. If multiple statements are selected, move to before or after all of them Raises ------ InvalidSchedule if there is no feasible path to move Returns ------- (ID, ID) (The new ID of the moved statement, The out-most newly introduced statments including the added loops) \"\"\" dst_list = self . _lookup_list ( dst ) # In DFS order if side == MoveToSide . Before : dst = dst_list [ 0 ] else : dst = dst_list [ - 1 ] return super () . move_to ( self . _lookup ( stmt ), side , dst ) parallelize ( self , loop , parallel ) \u00b6 Mark a loop with a parallel implementation This schedule follows a fork-join model: multiple workers (abstract threads) are created (but physically the threads may be cached in a thread pool) when the loop begins, do their jobs in parallel, and join when the loop ends OpenMP threads follow a typical fork-join model. CUDA threads run in a bulk-synchronous parallel (BSP) model, which can also be mimiked by the fork-join model: All threads start when the kernel get launched, but they only begin to do their jobs when the parallel loop begins. Nevertheless, the fork-join model needs the following extension to fully mimic a BSP model: Taking CUDA as an example, we allow binding a loop to threadIdx.x inside another loop bound to threadIdx.x , which is illegal in a classic fork-join model. For example, we may implement a matmul with collaborative fetch as below: for i : threadIdx.x # Li for j : threadIdx.y # Lj local_sum = 0 # In gpu/local memory, unique to (i, j) for k0 # Lk0 for k : threadIdx.y # Lk1_a A_cache[k] = A[i, k] # In gpu/shared, shared by different j for k : threadIdx.x # Lk1_b B_cache[k] = B[k, j] # In gpu/shared, shared by different i for k # Lk1_c sum += A_cache[k] * B_cache[k] C[i, j] = local_sum A seemingly plausible solution to avoid this extension is to reorder Lk0 to outer-most, and then move Lk1_a and Lk1_b out of Li or Lj . This resolves the nested threadIdx.x and threadIdx.y binding problem by running Li+Lk1_a , Lj+Lk1_b and Li+Lj interleavingly, instead of running Lk1_a and Lk1_b inside Li+Lj . However, this approach is illegal, because the local variable local_sum can no longer be kept inside the body of Li and Lj : It has to be reused across multiple runs of Li and Lj Please also note that we can bind one threadIdx.x to two loops only when the body statement is loop-invariant to one of them. For example, the following binding is still illegal, even in our extended fork-join model, because it violates its serial semantics: for i : threadIdx.x for j : threadIdx.x A[i, j] ++ Parameters: loop ( str, ID or Stmt ) \u2013 The loop parallel ( ParallelScope ) \u2013 Parallel scope Source code in freetensor/core/schedule.py def parallelize ( self , loop , parallel ): \"\"\" Mark a loop with a parallel implementation This schedule follows a fork-join model: multiple workers (abstract threads) are created (but physically the threads may be cached in a thread pool) when the loop begins, do their jobs in parallel, and join when the loop ends OpenMP threads follow a typical fork-join model. CUDA threads run in a bulk-synchronous parallel (BSP) model, which can also be mimiked by the fork-join model: All threads start when the kernel get launched, but they only begin to do their jobs when the parallel loop begins. Nevertheless, the fork-join model needs the following extension to fully mimic a BSP model: Taking CUDA as an example, we allow binding a loop to `threadIdx.x` inside another loop bound to `threadIdx.x`, which is illegal in a classic fork-join model. For example, we may implement a matmul with collaborative fetch as below: ``` for i : threadIdx.x # Li for j : threadIdx.y # Lj local_sum = 0 # In gpu/local memory, unique to (i, j) for k0 # Lk0 for k : threadIdx.y # Lk1_a A_cache[k] = A[i, k] # In gpu/shared, shared by different j for k : threadIdx.x # Lk1_b B_cache[k] = B[k, j] # In gpu/shared, shared by different i for k # Lk1_c sum += A_cache[k] * B_cache[k] C[i, j] = local_sum ``` A seemingly plausible solution to avoid this extension is to reorder `Lk0` to outer-most, and then move `Lk1_a` and `Lk1_b` out of `Li` or `Lj`. This resolves the nested `threadIdx.x` and `threadIdx.y` binding problem by running `Li+Lk1_a`, `Lj+Lk1_b` and `Li+Lj` interleavingly, instead of running `Lk1_a` and `Lk1_b` inside `Li+Lj`. However, this approach is illegal, because the local variable `local_sum` can no longer be kept inside the body of `Li` and `Lj`: It has to be reused across multiple runs of `Li` and `Lj` Please also note that we can bind one `threadIdx.x` to two loops only when the body statement is loop-invariant to one of them. For example, the following binding is still illegal, even in our extended fork-join model, because it violates its serial semantics: ``` for i : threadIdx.x for j : threadIdx.x A[i, j] ++ ``` Parameters ---------- loop : str, ID or Stmt The loop parallel : ParallelScope Parallel scope \"\"\" super () . parallelize ( self . _lookup ( loop ), ParallelScope ( parallel )) permute ( self , loops , transform_func ) \u00b6 Permute perfectly nested loops (directly nested loops without statements in between) with the given loop space transformation function The transformed loops follow ascending lexical order of the transformed terms returned by transformFunc when called with original iteration Parameters: loops ( array like of str, ID or Stmt ) \u2013 the list of perfectly nested loops to be permuted transform_func ( Callable[[Expr], Expr] ) \u2013 the loop space transformation function, should be bijective Returns: list of ID \u2013 the list of IDs of permuted loops Source code in freetensor/core/schedule.py def permute ( self , loops , transform_func ): \"\"\" Permute perfectly nested loops (directly nested loops without statements in between) with the given loop space transformation function The transformed loops follow ascending lexical order of the transformed terms returned by `transformFunc` when called with original iteration Parameters ---------- loops : array like of str, ID or Stmt the list of perfectly nested loops to be permuted transform_func : Callable[[Expr], Expr] the loop space transformation function, should be bijective Returns ------- list of ID the list of IDs of permuted loops \"\"\" return super () . permute ([ self . _lookup ( l ) for l in loops ], transform_func ) pluto_fuse ( self , loop0 , loop1 , nest_level_0 = 0 , nest_level_1 = 0 , fusable_overlap_threshold = 1 , do_simplify = True ) \u00b6 Use Pluto+ algorithm to permute and fuse two loops, with as most parallelizable loops as possible at outermost levels. The two loops are required to be consequent; all directly nested levels are detected and subject to permutation. Remaining levels that cannot be fused are left inside the fused loops as two statements Parameters: loop0 ( str, ID or Stmt ) \u2013 The first loop to fuse loop1 ( str, ID or Stmt ) \u2013 The second loop to fuse nest_level_0 ( int ) \u2013 The number of nesting levels of loop 0 to be considered, defaults to maximum possible nest_level_1 ( int ) \u2013 The number of nesting levels of loop 1 to be considered, defaults to maximum possible fusableOverlapThreshold ( int ) \u2013 The minimum overlapping size of two loops to be regarded fusable. Defaults to 1 do_simplify ( bool ) \u2013 Whether the result is simplified by the way, defaults to true Exceptions: InvalidSchedule \u2013 if the loops are not consequent Returns: (ID, int) \u2013 The ID of fused loop and level of parallelizable loops Source code in freetensor/core/schedule.py def pluto_fuse ( self , loop0 , loop1 , nest_level_0 = 0 , nest_level_1 = 0 , fusable_overlap_threshold = 1 , do_simplify = True ): \"\"\" Use Pluto+ algorithm to permute and fuse two loops, with as most parallelizable loops as possible at outermost levels. The two loops are required to be consequent; all directly nested levels are detected and subject to permutation. Remaining levels that cannot be fused are left inside the fused loops as two statements Parameters ---------- loop0 : str, ID or Stmt The first loop to fuse loop1 : str, ID or Stmt The second loop to fuse nest_level_0 : int The number of nesting levels of loop 0 to be considered, defaults to maximum possible nest_level_1 : int The number of nesting levels of loop 1 to be considered, defaults to maximum possible fusableOverlapThreshold : int The minimum overlapping size of two loops to be regarded fusable. Defaults to 1 do_simplify : bool Whether the result is simplified by the way, defaults to true Returns ------- (ID, int) The ID of fused loop and level of parallelizable loops Raises ------ InvalidSchedule if the loops are not consequent \"\"\" return super () . pluto_fuse ( self . _lookup ( loop0 ), self . _lookup ( loop1 ), nest_level_0 , nest_level_1 , fusable_overlap_threshold , do_simplify ) pluto_permute ( self , loop , nest_level = 0 , do_simplify = True ) \u00b6 Use Pluto+ algorithm to permute a single loop, with as most parallelizable loops as possible at outermost levels. Parameters: loop ( str, ID or Stmt ) \u2013 The loop to permute nest_level ( int ) \u2013 The number of nesting levels to be considered, defaults to maximum possible do_simplify ( bool ) \u2013 Whether the result is simplified by the way, defaults to true Returns: (ID, int) \u2013 The ID of permuted loop and level of parallelizable loops Source code in freetensor/core/schedule.py def pluto_permute ( self , loop , nest_level = 0 , do_simplify = True ): \"\"\" Use Pluto+ algorithm to permute a single loop, with as most parallelizable loops as possible at outermost levels. Parameters ---------- loop : str, ID or Stmt The loop to permute nest_level : int The number of nesting levels to be considered, defaults to maximum possible do_simplify : bool Whether the result is simplified by the way, defaults to true Returns ------- (ID, int) The ID of permuted loop and level of parallelizable loops \"\"\" return super () . pluto_permute ( self . _lookup ( loop ), nest_level , do_simplify ) reorder ( self , order ) \u00b6 Reorder directly nested loops To swap consecutive loops, use swap instead Parameters: order ( array like of str, ID or Stmt ) \u2013 Vector of loops. The requested order of the loops Exceptions: InvalidSchedule \u2013 if the input is invalid or there are breaking dependences Source code in freetensor/core/schedule.py def reorder ( self , order ): \"\"\" Reorder directly nested loops To swap consecutive loops, use `swap` instead Parameters ---------- order : array like of str, ID or Stmt Vector of loops. The requested order of the loops Raises ------ InvalidSchedule if the input is invalid or there are breaking dependences \"\"\" super () . reorder ( list ( map ( self . _lookup , order ))) separate_tail ( self , noDuplicateVarDefs = False ) \u00b6 Seperate main iterations and tail iterations of a loop E.g. for i = 0 -> 3 { for j = 0 -> 4 { if (i * 4 + j < 10) { ... } } } Each loop will be separated into 2 parts: the body and the tail. After simplification, the program will finally be transformed to for i = 0 -> 2 { for j = 0 -> 4 { ... } } for j = 0 -> 2 { ... } Ideally, all programs can benefit from this schedule. However, this schedule may greatly increase the program size and make the compiling time way too long. Therefore, this transformation is implemented as a schedule, which can be applied optionally. (TODO: Optionally apply this schedule to part of the program) Parameters: noDuplicateVarDefs ( bool ) \u2013 If there is two VarDef nodes in two branches, it may result in doubled memory use, since different thread may go to different branch. Set this parameter to true to stop duplicating VarDef nodes. Source code in freetensor/core/schedule.py def separate_tail ( self , noDuplicateVarDefs = False ): \"\"\" Seperate main iterations and tail iterations of a loop E.g. ``` for i = 0 -> 3 { for j = 0 -> 4 { if (i * 4 + j < 10) { ... } } } ``` Each loop will be separated into 2 parts: the body and the tail. After simplification, the program will finally be transformed to ``` for i = 0 -> 2 { for j = 0 -> 4 { ... } } for j = 0 -> 2 { ... } ``` Ideally, all programs can benefit from this schedule. However, this schedule may greatly increase the program size and make the compiling time way too long. Therefore, this transformation is implemented as a schedule, which can be applied optionally. (TODO: Optionally apply this schedule to part of the program) Parameters ---------- noDuplicateVarDefs : bool If there is two VarDef nodes in two branches, it may result in doubled memory use, since different thread may go to different branch. Set this parameter to true to stop duplicating VarDef nodes. \"\"\" super () . separate_tail ( noDuplicateVarDefs ) set_mem_type ( self , vardef , mtype ) \u00b6 Change where a variable is stored Parameters: vardef ( str, ID or Stmt ) \u2013 ID of the VarDef statement of the specific variable mtype ( MemType ) \u2013 Where the variable should be stored Exceptions: InvalidSchedule \u2013 if the variable is not found Source code in freetensor/core/schedule.py def set_mem_type ( self , vardef , mtype ): \"\"\" Change where a variable is stored Parameters ---------- vardef : str, ID or Stmt ID of the VarDef statement of the specific variable mtype : MemType Where the variable should be stored Raises ------ InvalidSchedule if the variable is not found \"\"\" super () . set_mem_type ( self . _lookup ( vardef ), MemType ( mtype )) split ( self , node , factor =- 1 , nparts =- 1 , shift = 0 ) \u00b6 Split a loop into two nested loops To fission a loop into two consecutive loops, use fission instead Two modes are provided: Specify factor and leave nparts to -1. It will result in an outer loop with length ceil(n / factor) , and an inner loop with length factor , where n is the original loop length added by shift . The original iterator i will be transformed to i0 * factor + i1 , where i0 and i1 are the iterators of the new outer and inner loops, respectively Specify nparts and leave factor to -1. It will result in an outer loop with length nparts , and an inner loop with length ceil(n / nparts) , where n is the original loop length added by shift . The original iterator i will be transformed to i0 * ceil(n / nparts) + i1 , where i0 and i1 are the iterators of the new outer and inner loops, respectively Please note that the second mode will introduce an i0 * ceil(n / nparts) factor into the program, which cannot be recognized by polyhedral analysis, which may hinder some following schedules. If possible, plese use the first mode, and then reorder the inner and outer loops Suppose the original loop is labeled \"L\", the split two loops can be selected by \"$split.0{L}\" (the outer loop) and \"$split.1{L}\" (the inner loop). If one of the resulting loop is proved to have only a single iteration, it will be removed Parameters: node ( str, ID or Stmt ) \u2013 The loop to be split factor ( int ) \u2013 Length of the inner loop. Set to -1 if using nparts nparts ( int ) \u2013 Length of the outer loop. Set to -1 if using factor Exceptions: InvalidSchedule \u2013 if the loop is not found Returns: (Optional[ID], Optional[ID]) \u2013 (outer loop ID, inner loop ID), either ID can be None if the loop is proved to have only a single iteration Source code in freetensor/core/schedule.py def split ( self , node , factor =- 1 , nparts =- 1 , shift = 0 ): \"\"\" Split a loop into two nested loops To fission a loop into two consecutive loops, use `fission` instead Two modes are provided: 1. Specify `factor` and leave `nparts` to -1. It will result in an outer loop with length `ceil(n / factor)`, and an inner loop with length `factor`, where `n` is the original loop length added by `shift`. The original iterator `i` will be transformed to `i0 * factor + i1`, where `i0` and `i1` are the iterators of the new outer and inner loops, respectively 2. Specify `nparts` and leave `factor` to -1. It will result in an outer loop with length `nparts`, and an inner loop with length `ceil(n / nparts)`, where `n` is the original loop length added by `shift`. The original iterator `i` will be transformed to `i0 * ceil(n / nparts) + i1`, where `i0` and `i1` are the iterators of the new outer and inner loops, respectively Please note that the second mode will introduce an `i0 * ceil(n / nparts)` factor into the program, which cannot be recognized by polyhedral analysis, which may hinder some following schedules. If possible, plese use the first mode, and then reorder the inner and outer loops Suppose the original loop is labeled \"L\", the split two loops can be selected by \"$split.0{L}\" (the outer loop) and \"$split.1{L}\" (the inner loop). If one of the resulting loop is proved to have only a single iteration, it will be removed Parameters ---------- node : str, ID or Stmt The loop to be split factor : int Length of the inner loop. Set to -1 if using `nparts` nparts : int Length of the outer loop. Set to -1 if using `factor` Raises ------ InvalidSchedule if the loop is not found Returns ------- (Optional[ID], Optional[ID]) (outer loop ID, inner loop ID), either ID can be None if the loop is proved to have only a single iteration \"\"\" return ( i if i else None for i in super () . split ( self . _lookup ( node ), factor , nparts , shift )) swap ( self , order ) \u00b6 Swap statements in the same block To reorder nested loops, use reorder instead Parameters: order ( List[str (Selector string), ID, List[ID], Stmt, or List[Stmt]] ) \u2013 The statements. If one item of the order list contains multiple statements, the order list will be flattened Exceptions: InvalidSchedule \u2013 if the statements are not found or the dependences cannot be solved Source code in freetensor/core/schedule.py def swap ( self , order ): \"\"\" Swap statements in the same block To reorder nested loops, use `reorder` instead Parameters ---------- order : List[str (Selector string), ID, List[ID], Stmt, or List[Stmt]] The statements. If one item of the `order` list contains multiple statements, the `order` list will be flattened Raises ------ InvalidSchedule if the statements are not found or the dependences cannot be solved \"\"\" super () . swap ( self . _lookup_list ( order )) unroll ( self , loop , immediate = False ) \u00b6 Unroll a loop Parameters: loop ( str, ID or Stmt ) \u2013 ID of the loop immediate ( bool ) \u2013 If false (by default), postpone the unroll procedure to the backend compiler, which saves scheduling time. If true, unroll the loop immediately, which may help further simplifications based on the unrolled result. If your purpose is just to fill the instruction cache, set it to false. If you are unrolling a loop that computes array indices, set it to true Exceptions: InvalidSchedule \u2013 if the loop is not found or length of the loop is not a constant Source code in freetensor/core/schedule.py def unroll ( self , loop , immediate = False ): \"\"\" Unroll a loop Parameters ---------- loop : str, ID or Stmt ID of the loop immediate : bool If false (by default), postpone the unroll procedure to the backend compiler, which saves scheduling time. If true, unroll the loop immediately, which may help further simplifications based on the unrolled result. If your purpose is just to fill the instruction cache, set it to false. If you are unrolling a loop that computes array indices, set it to true Raises ------ InvalidSchedule if the loop is not found or length of the loop is not a constant \"\"\" super () . unroll ( self . _lookup ( loop ), immediate ) var_merge ( self , vardef , dim ) \u00b6 Merge two dimensions of a variable Parameters: vardef ( str, ID or Stmt ) \u2013 ID of the VarDef statement of the specific variable dim ( int ) \u2013 Merge the dim -th and the (dim + 1) -th dimension Source code in freetensor/core/schedule.py def var_merge ( self , vardef , dim ): \"\"\" Merge two dimensions of a variable Parameters ---------- vardef : str, ID or Stmt ID of the VarDef statement of the specific variable dim : int Merge the `dim`-th and the `(dim + 1)`-th dimension \"\"\" return super () . var_merge ( self . _lookup ( vardef ), dim ) var_reorder ( self , vardef , order ) \u00b6 Reorder the dimensions of a variable Parameters: vardef ( str, ID or Stmt ) \u2013 ID of the VarDef statement of the specific variable order ( array like of str, ID or Stmt ) \u2013 Vector of integers. The new order of the dimensions Exceptions: InvalidSchedule \u2013 if the variable or the order is illegal Source code in freetensor/core/schedule.py def var_reorder ( self , vardef , order ): \"\"\" Reorder the dimensions of a variable Parameters ---------- vardef : str, ID or Stmt ID of the VarDef statement of the specific variable order : array like of str, ID or Stmt Vector of integers. The new order of the dimensions Raises ------ InvalidSchedule if the variable or the order is illegal \"\"\" return super () . var_reorder ( self . _lookup ( vardef ), order ) var_split ( self , vardef , dim , mode , factor =- 1 , nparts =- 1 ) \u00b6 Split a dimension of a variable into two Parameters: vardef ( str, ID or Stmt ) \u2013 ID of the VarDef statement of the specific variable dim ( int ) \u2013 which dimension to be split mode ( VarSplitMode ) \u2013 When the dimension to split is not divisible by factor or nparts , the resulting shape may become larger. In FixedSize mode, the actual buffer size will not be changed, and gurads will be added to prevent out-of-bound accesses. In RelaxedSize mode, the buffer size may increase. The RelaxedSize mode cannot be applied to I/O variables factor ( int ) \u2013 Length of the inner (higher no.) dimension. Set to -1 if using nparts nparts ( int ) \u2013 Length of the outer (lower no.) loop. Set to -1 if using factor Exceptions: InvalidSchedule \u2013 if the variable or the dimension is not found Source code in freetensor/core/schedule.py def var_split ( self , vardef , dim , mode , factor =- 1 , nparts =- 1 ): \"\"\" Split a dimension of a variable into two Parameters ---------- vardef : str, ID or Stmt ID of the VarDef statement of the specific variable dim : int which dimension to be split mode : VarSplitMode When the dimension to split is not divisible by `factor` or `nparts`, the resulting shape may become larger. In `FixedSize` mode, the actual buffer size will not be changed, and gurads will be added to prevent out-of-bound accesses. In `RelaxedSize` mode, the buffer size may increase. The `RelaxedSize` mode cannot be applied to I/O variables factor : int Length of the inner (higher no.) dimension. Set to -1 if using `nparts` nparts : int Length of the outer (lower no.) loop. Set to -1 if using `factor` Raises ------ InvalidSchedule if the variable or the dimension is not found \"\"\" return super () . var_split ( self . _lookup ( vardef ), dim , mode , factor , nparts ) vectorize ( self , loop ) \u00b6 Vectorize a loop Please note that, as vectorization is different from architecture to achitecture, the scheduler may or may not postpone it to the backend compiler. The vectorization is a best-effort schedule Parameters: loop ( str, ID or Stmt ) \u2013 ID of the loop Exceptions: InvalidSchedule \u2013 if the ID or name is not found, or the dependence requirement is not met Source code in freetensor/core/schedule.py def vectorize ( self , loop ): \"\"\" Vectorize a loop Please note that, as vectorization is different from architecture to achitecture, the scheduler may or may not postpone it to the backend compiler. The vectorization is a best-effort schedule Parameters ---------- loop : str, ID or Stmt ID of the loop Raises ------ InvalidSchedule if the ID or name is not found, or the dependence requirement is not met \"\"\" super () . vectorize ( self . _lookup ( loop )) schedule ( ast = None , callback = None , verbose = None ) \u00b6 Apply any schedule on an AST through a user callback Parameters: ast ( Func or Stmt ) \u2013 The AST to schedule. If not specified, a partial function will be returned that cna be used as a decorator callback ( Callable[[freetensor.core.schedule.Schedule], NoneType] ) \u2013 Specify what schedule(s) to do in this callback verbose ( Optional[int] ) \u2013 0 = print nothing. 1 = print the final AST. 2 = print an AST after each schedule Source code in freetensor/core/schedule.py def schedule ( ast = None , callback : Callable [[ Schedule ], None ] = None , verbose : Optional [ int ] = None ): ''' Apply any schedule on an AST through a user callback Parameters ---------- ast : Func or Stmt The AST to schedule. If not specified, a partial function will be returned that cna be used as a decorator callback : Callable Specify what schedule(s) to do in this callback verbose : int (Optional) 0 = print nothing. 1 = print the final AST. 2 = print an AST after each schedule ''' if ast is not None : if callback is None : return ast if verbose is None : verbose = 0 s = Schedule ( ast , verbose = verbose ) callback ( s ) if ast . type () == ffi . ASTNodeType . Func : return s . func () else : return s . ast () else : f = schedule if callback is not None : f = functools . partial ( f , callback = callback ) if verbose is not None : f = functools . partial ( f , verbose = verbose ) return f staging \u00b6 A staging framework to support the FreeTensor frontend. AllowShortcutScope dataclass \u00b6 Allow return scope. This is a context manager that allows return in statically deterministic control flow. Source code in freetensor/core/staging.py @dataclass class AllowShortcutScope : '''Allow return scope. This is a context manager that allows return in statically deterministic control flow. ''' overload : StagingOverload should_allow : bool def __enter__ ( self ): self . prev = self . overload . is_shortcut_allowed self . overload . is_shortcut_allowed = self . should_allow def __exit__ ( self , exc_class , exc_value , traceback ): self . overload . is_shortcut_allowed = self . prev BreakException ( Exception ) \u00b6 Exception to be raised by StagingOverload.break_stmt. Breaks from a for loop. Source code in freetensor/core/staging.py class BreakException ( Exception ): '''Exception to be raised by StagingOverload.break_stmt. Breaks from a for loop.''' pass ContinueException ( Exception ) \u00b6 Exception to be raised by StagingOverload.continue_stmt. Continues a for loop. Source code in freetensor/core/staging.py class ContinueException ( Exception ): '''Exception to be raised by StagingOverload.continue_stmt. Continues a for loop.''' pass ReturnException ( Exception ) \u00b6 Exception to be raised by StagingOverload.return_stmt. Holds a return value that will be passed through to the function wrapper. Source code in freetensor/core/staging.py class ReturnException ( Exception ): '''Exception to be raised by StagingOverload.return_stmt. Holds a return value that will be passed through to the function wrapper.''' def __init__ ( self , value : Any ) -> None : self . value = value StagedAssignable ( ABC ) \u00b6 Source code in freetensor/core/staging.py class StagedAssignable ( abc . ABC ): @abc . abstractmethod def assign ( self , name : str ): raise NotImplementedError () __class__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in freetensor/core/staging.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls ) __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). Source code in freetensor/core/staging.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. Source code in freetensor/core/staging.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). Source code in freetensor/core/staging.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/staging.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) StagedPredicate ( ABC ) \u00b6 Source code in freetensor/core/staging.py class StagedPredicate ( abc . ABC ): @abc . abstractmethod def logical_and ( self , lazy_other : Callable [[], StagedPredicate ]) -> StagedPredicate : raise NotImplementedError () @abc . abstractmethod def logical_or ( self , lazy_other : Callable [[], StagedPredicate ]) -> StagedPredicate : raise NotImplementedError () @abc . abstractmethod def logical_not ( self ): raise NotImplementedError () @abc . abstractmethod def if_then_else_stmt ( self , then_body : Callable [[], None ], else_body : Optional [ Callable [[], None ]]): raise NotImplementedError () @abc . abstractmethod def if_then_else_expr ( self , then_expr : Callable [[], Any ], else_expr : Callable [[], Any ]): raise NotImplementedError () @abc . abstractmethod def while_stmt ( self , body : Callable [[], None ]): raise NotImplementedError () @abc . abstractmethod def assert_stmt ( self ): raise NotImplementedError () __class__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in freetensor/core/staging.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls ) __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). Source code in freetensor/core/staging.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. Source code in freetensor/core/staging.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). Source code in freetensor/core/staging.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/staging.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) StagedTypeAnnotation \u00b6 Source code in freetensor/core/staging.py class StagedTypeAnnotation ( metaclass = StagedTypeAnnotationMeta ): @abc . abstractmethod def annotate ( self , name : str ): raise NotImplementedError () __class__ ( ABCMeta ) inherited \u00b6 Source code in freetensor/core/staging.py class StagedTypeAnnotationMeta ( abc . ABCMeta ): def __getitem__ ( self , args ): return self ( * args ) __base__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in freetensor/core/staging.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls ) __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). Source code in freetensor/core/staging.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. Source code in freetensor/core/staging.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). Source code in freetensor/core/staging.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/staging.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/staging.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) StagedTypeAnnotationMeta ( ABCMeta ) \u00b6 Source code in freetensor/core/staging.py class StagedTypeAnnotationMeta ( abc . ABCMeta ): def __getitem__ ( self , args ): return self ( * args ) __base__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in freetensor/core/staging.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls ) __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). Source code in freetensor/core/staging.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. Source code in freetensor/core/staging.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). Source code in freetensor/core/staging.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/staging.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/staging.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) StagedUnpackAssignable ( ABC ) \u00b6 Source code in freetensor/core/staging.py class StagedUnpackAssignable ( abc . ABC ): @abc . abstractmethod def assign ( self , names ): raise NotImplementedError () __class__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in freetensor/core/staging.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls ) __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). Source code in freetensor/core/staging.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. Source code in freetensor/core/staging.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). Source code in freetensor/core/staging.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/staging.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) StagingError ( Exception ) \u00b6 Error occurred during staging function execution (i.e. IR tree generation). Source code in freetensor/core/staging.py class StagingError ( Exception ): '''Error occurred during staging function execution (i.e. IR tree generation).''' def __init__ ( self , overload : StagingOverload , message : str ) -> None : # TODO: add output of StagingContext.call_stack super () . __init__ ( f ' { message } : \\n { \"\" . join ( traceback . format_list ( overload . debug_call_stack )) } ' . lstrip ()) StagingOverload \u00b6 Source code in freetensor/core/staging.py class StagingOverload : def __init__ ( self ) -> None : self . is_shortcut_allowed : bool = True self . debug_call_stack : List [ traceback . FrameSummary ] = [] def custom_attr ( self , obj : Any , attr : str ) -> Any : ''' Customized attribute accessor. The framework first looks for a Python native attribute. If not found, it looks for this overloaded custom attribute resolver. The default implementation provides no custom attribute. Can be overridden by subclasses. Parameters ---------- obj : Any Object to access attribute. attr : str Attribute name. Returns ------- Any : The attribute value. Throws ------ AttributeError : If the attribute is not found. ''' return None def metadata ( self , content ) -> None : ''' Metadata handler. A metadata line is a comment starting with `#! ` and followed by a metadata, represented as a string parameter. Defaults to a no-op. Can be overridden by subclasses. Parameters ---------- content : str The metadata content. ''' pass def at_position ( self , filename : str , lineno : int ) -> None : ''' Code position handler. Defaults to a no-op. Can be overridden by subclasses. Parameters ---------- filename : str Name of the file containing code for the next statement. lineno : int Line number of the next statement. ''' pass def error ( self , content : str ): return StagingError ( self , content ) def allow_shortcut_scope ( self , allow : bool ): '''Opens a scope that allows shortcut control flows in a statically deterministic context. Need to be closed by `with` statement.''' return AllowShortcutScope ( self , allow ) def foreach ( self , names , iter , body : Callable [[ Any ], None ]) -> None : '''Customized foreach wrapper. If `value` is instance of `StagedIterable`, its regarded as a customized foreach behavior and used to generate code for the python for loop. Otherwise, we try to execute the loop as usual. ''' if isinstance ( iter , StagedIterable ): iter . foreach ( names , body ) else : for iter_var in iter : try : body ( iter_var ) except BreakException : break except ContinueException : continue def unpack_assign_stmt ( self , names , values ): '''Customized assign wrapper for one or more targets. If `values` is instance of `StagedUnpackAssignable`, it's regarded as a customized assign behavior and gets executed with all the assigned targets' names. Otherwise, it calls `assign_stmt` with each sub-assignments. Please note that `names` can be nested tuples like `(\"a\", (\"b\", \"c\"))`. Please also note that `names` can also be a single string like \"a\" even if `values` is a tuple. There is no unpacking in this case ''' if isinstance ( values , StagedUnpackAssignable ): return values . assign ( names ) elif isinstance ( names , str ): return self . assign_stmt ( names , values ) else : assert isinstance ( names , Sequence ) values = tuple ( values ) if len ( names ) != len ( values ): raise self . error ( \"Number of return values does not match when unpacking\" ) returns = [] for name , value in zip ( names , values ): returns . append ( self . unpack_assign_stmt ( name , value )) return tuple ( returns ) def assign_stmt ( self , name : str , value ): '''Customized assign wrapper. If `value` is instance of `StagedAssignable`, it's regarded as a customized assign behavior and gets executed with the assigned target variable name. This wrapper is used for initializing a variable. ''' if isinstance ( value , StagedAssignable ): return value . assign ( name ) else : return value def if_then_else_stmt ( self , predicate , then_body , else_body = None ): '''If-then-else statement staging tool. When predicate is deterministic in staging, only one branch is generated. Otherwise, a If node in IR is generated. ''' if isinstance ( predicate , StagedPredicate ): predicate . if_then_else_stmt ( then_body , else_body ) else : if predicate : then_body () elif else_body : else_body () def if_then_else_expr ( self , predicate , then_expr , else_expr ): '''If-then-else expression staging tool.''' if isinstance ( predicate , StagedPredicate ): return predicate . if_then_else_expr ( then_expr , else_expr ) else : if predicate : return then_expr () else : return else_expr () def while_stmt ( self , fpred , body ): '''While statement staging tool.''' first_pred = fpred () if isinstance ( first_pred , StagedPredicate ): first_pred . while_stmt ( body ) else : if first_pred : try : body () except BreakException : return except ContinueException : pass while fpred (): try : body () except BreakException : break except ContinueException : continue def assert_stmt ( self , test ): '''Assert staging tool.''' if isinstance ( test , StagedPredicate ): test . assert_stmt () else : assert test def return_stmt ( self , value , funcname ): '''Return staging tool. Only allow return in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Return is only allowed in statically deterministic control flow.' ) if isinstance ( value , StagedUnpackAssignable ): # We don't know how many items are there, so no unpacking value = value . assign ( funcname ) if isinstance ( value , StagedAssignable ): value = value . assign ( funcname ) raise ReturnException ( value ) def break_stmt ( self ): '''Break staging tool. Only allow break in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Break is only allowed in statically deterministic control flow.' ) raise BreakException () def continue_stmt ( self ): '''Continue staging tool. Only allow continue in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Continue is only allowed in statically deterministic control flow.' ) raise ContinueException () def load_attr ( self , obj , attr : str ): '''Load attribute staging tool. Allows customization of reading attributes.''' try : return getattr ( obj , attr ) except AttributeError : try : # Have to use AttributeError again, since a custom attribute might have # a None value result = self . custom_attr ( obj , attr ) successful = True except AttributeError : successful = False if successful : return result else : raise def and_expr ( self , * lazy_args ): def reducer ( a , fb ): if isinstance ( a , StagedPredicate ): return a . logical_and ( fb ) else : # This is not a simple logical and; it's equivalent to a if-then-else. # Thus, if a is True, fb() is returned, preserving the original value, # which might be a StagedPredicate. return a and fb () return functools . reduce ( reducer , lazy_args , True ) def or_expr ( self , * lazy_args ): def reducer ( a , fb ): if isinstance ( a , StagedPredicate ): return a . logical_or ( fb ) else : return a or fb () return functools . reduce ( reducer , lazy_args , False ) def not_expr ( self , arg ): if isinstance ( arg , StagedPredicate ): return arg . logical_not () else : return not arg def functiondef_decorator ( self , filename ): return functools . partial ( self . functiondef_wrapper , filename ) def functiondef_wrapper ( self , filename , func ): '''Function definition wrapper. This wrapper performs extra initialization and cleanup for function definition. ''' def wrapped ( * args , ** kwargs ): # Push debug call stack with some random line number. # It will be updated by `mark_position` calls in the function. self . debug_call_stack . append ( traceback . FrameSummary ( filename , 1 , func . __name__ )) # The called function can now return from itself, despite what the outer # control flow is. with self . allow_shortcut_scope ( True ): try : func ( * args , ** kwargs ) except ReturnException as e : result = e . value else : # No return_stmt was called, naturally returns None result = None # Pop debug call stack. self . debug_call_stack . pop () return result return wrapped def annotate_stmt ( self , name : str , ty ): if isinstance ( ty , StagedTypeAnnotation ): return ty . annotate ( name ) return None def mark_position ( self , lineno : int ): # FrameSummary is immutable, so we have to initialize a new one with updated # line number. self . debug_call_stack [ - 1 ] = traceback . FrameSummary ( self . debug_call_stack [ - 1 ] . filename , lineno , self . debug_call_stack [ - 1 ] . name ) self . at_position ( self . debug_call_stack [ - 1 ] . filename , self . debug_call_stack [ - 1 ] . lineno ) def into_staging ( self , func , extra_locals : Dict [ str , Any ] = None , src : str = None , verbose = False ): assert inspect . isfunction ( func ) if extra_locals is None : extra_locals = {} if src is None : lines , lineno = ins . getsourcelines ( func ) src = '' . join ( lines ) file = ins . getfile ( func ) else : lineno = 1 file = f '<staging: { func . __name__ } >' # Inject overload to extra_locals. extra_locals [ '__staging_overload__' ] = self # To transform a function, except essential AST transformation, we have to pass # the globals and locals (actually captured outer local variables) to the # transformed function properly. # Note that: # 1. We have to pass both globals and locals to `exec`. # 2. We cannot insert locals to the globals `dict`, otherwise it will pollute # the globals `dict`. # 3. We cannot copy the globals `dict` before passing it to exec, otherwise the # staged function cannot write to globals and get later updates in the global. # Thus, we have to pass the globals and locals to the transformed function # separately. if func . __closure__ : assert len ( func . __code__ . co_freevars ) == len ( func . __closure__ ) func_locals = { name : cell for name , cell in zip ( func . __code__ . co_freevars , func . __closure__ ) } else : func_locals = {} # Translate `#! ` comments to metadata calls. src = process_annotating_comments ( src ) # Wrap the code if it has a indentation. if src [ 0 ] == ' ' or src [ 0 ] == ' \\t ' : src = 'if True: \\n ' + src tree = ast . parse ( src ) assert len ( tree . body ) == 1 and isinstance ( tree . body [ 0 ], ast . If ) # Replace with the real body to eliminate the faked if. tree . body = tree . body [ 0 ] . body # Modify lineno to match with the location. lineno -= 1 else : tree = ast . parse ( src ) tree = Transformer ( file , lineno ) . visit ( tree ) # Instead of passing the `func_local` directly to `exec`, we instead wrap the # staging function. This is to workaround an issue of CPython. (See # https://github.com/python/cpython/issues/86084). # The sketch is: # ``` # def __freetensor_staging_wrapper__(__freetensor_extra_locals__, # __freetensor_local_cells__): # some_extra_local = __freetensor_extra_locals__['some_extra_local'] # some_captured = None # # def original_func(): # nonlocal some_captured # some_captured = __freetensor_local_cells__.some_captured # try: # ... # original function body # finally: # __freetensor_local_cells__.some_captured = some_captured # # return original_func # ``` # Note that `__freetensor_local_cells__` is a `LocalsDictWrapper` object. # It in-turn accesses cell.cell_contents to get/set the value of the local # variable. # The `LocalsDictWrapper` is a helper class to reduce code generation complexity. WRAPPER_NAME = '__freetensor_staging_wrapper__' assert isinstance ( tree , ast . Module ) assert len ( tree . body ) == 1 and isinstance ( tree . body [ 0 ], ast . FunctionDef ) # Modify function body. if len ( func_locals ) > 0 : tree . body [ 0 ] . body = ([ # Declare them as nonlocals to assign to outer scope. ast . Nonlocal ( list ( func_locals . keys ())), ] + [ # Fetch latest values of the closure variables. ast . Assign ([ ast . Name ( name , ast . Store ())], ast . Attribute ( ast . Name ( '__freetensor_local_cells__' , ast . Load ()), name , ast . Load ())) for name in func_locals . keys () ] + [ # Use a try-finally to ensure closure write back. ast . Try ( body = tree . body [ 0 ] . body , handlers = [], orelse = [], finalbody = [ ast . Assign ([ ast . Attribute ( ast . Name ( '__freetensor_local_cells__' , ast . Load ()), name , ast . Store ()) ], ast . Name ( name , ast . Load ())) for name in func_locals . keys () ]) ]) tree . body = [ ast . FunctionDef ( name = WRAPPER_NAME , args = ast . arguments ( posonlyargs = [], args = [ ast . arg ( '__freetensor_extra_locals__' , None ), ast . arg ( '__freetensor_local_cells__' , None ) ], vararg = None , kwonlyargs = [], kw_defaults = [], kwarg = None , defaults = []), body = [ # Captured closure variables are not fetched here, only declared. ast . Assign ([ ast . Name ( name , ast . Store ())], ast . Constant ( None )) for name in func_locals . keys () ] + [ # Extra locals are fetched here. ast . Assign ([ ast . Name ( name , ast . Store ())], ast . Subscript ( ast . Name ( '__freetensor_extra_locals__' , ast . Load ()), ast_index ( ast . Constant ( name )), ast . Load ())) for name in extra_locals . keys () ] + tree . body + [ ast . Return ( value = ast . Name ( id = func . __name__ , ctx = ast . Load ()))], decorator_list = [], returns = None ), ] tree = ast . fix_missing_locations ( tree ) if verbose : import astor source = astor . to_source ( tree ) from pygments import highlight from pygments.lexers import PythonLexer from pygments.formatters import TerminalFormatter print ( highlight ( source , PythonLexer (), TerminalFormatter ( bg = 'dark' , linenos = True )), file = sys . stderr ) tree = source # make debug info match dumped source # Create an empty locals dict to avoid polluting the original globals. empty_locals = {} exec ( compile ( tree , f '<staging: { func . __name__ } >' , 'exec' ), func . __globals__ , empty_locals ) f_wrapper = empty_locals [ WRAPPER_NAME ] # Pass the closure to the wrapper and retrieve the staging function with # correct captured variables. f_staging = f_wrapper ( extra_locals , LocalsDictWrapper ( func_locals )) return f_staging allow_shortcut_scope ( self , allow ) \u00b6 Opens a scope that allows shortcut control flows in a statically deterministic context. Need to be closed by with statement. Source code in freetensor/core/staging.py def allow_shortcut_scope ( self , allow : bool ): '''Opens a scope that allows shortcut control flows in a statically deterministic context. Need to be closed by `with` statement.''' return AllowShortcutScope ( self , allow ) assert_stmt ( self , test ) \u00b6 Assert staging tool. Source code in freetensor/core/staging.py def assert_stmt ( self , test ): '''Assert staging tool.''' if isinstance ( test , StagedPredicate ): test . assert_stmt () else : assert test assign_stmt ( self , name , value ) \u00b6 Customized assign wrapper. If value is instance of StagedAssignable , it's regarded as a customized assign behavior and gets executed with the assigned target variable name. This wrapper is used for initializing a variable. Source code in freetensor/core/staging.py def assign_stmt ( self , name : str , value ): '''Customized assign wrapper. If `value` is instance of `StagedAssignable`, it's regarded as a customized assign behavior and gets executed with the assigned target variable name. This wrapper is used for initializing a variable. ''' if isinstance ( value , StagedAssignable ): return value . assign ( name ) else : return value at_position ( self , filename , lineno ) \u00b6 Code position handler. Defaults to a no-op. Can be overridden by subclasses. Parameters: filename ( str ) \u2013 Name of the file containing code for the next statement. lineno ( int ) \u2013 Line number of the next statement. Source code in freetensor/core/staging.py def at_position ( self , filename : str , lineno : int ) -> None : ''' Code position handler. Defaults to a no-op. Can be overridden by subclasses. Parameters ---------- filename : str Name of the file containing code for the next statement. lineno : int Line number of the next statement. ''' pass break_stmt ( self ) \u00b6 Break staging tool. Only allow break in static control flow. Source code in freetensor/core/staging.py def break_stmt ( self ): '''Break staging tool. Only allow break in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Break is only allowed in statically deterministic control flow.' ) raise BreakException () continue_stmt ( self ) \u00b6 Continue staging tool. Only allow continue in static control flow. Source code in freetensor/core/staging.py def continue_stmt ( self ): '''Continue staging tool. Only allow continue in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Continue is only allowed in statically deterministic control flow.' ) raise ContinueException () custom_attr ( self , obj , attr ) \u00b6 Customized attribute accessor. The framework first looks for a Python native attribute. If not found, it looks for this overloaded custom attribute resolver. The default implementation provides no custom attribute. Can be overridden by subclasses. Parameters: obj ( Any ) \u2013 Object to access attribute. attr ( str ) \u2013 Attribute name. Returns: Any \u2013 The attribute value. Source code in freetensor/core/staging.py def custom_attr ( self , obj : Any , attr : str ) -> Any : ''' Customized attribute accessor. The framework first looks for a Python native attribute. If not found, it looks for this overloaded custom attribute resolver. The default implementation provides no custom attribute. Can be overridden by subclasses. Parameters ---------- obj : Any Object to access attribute. attr : str Attribute name. Returns ------- Any : The attribute value. Throws ------ AttributeError : If the attribute is not found. ''' return None foreach ( self , names , iter , body ) \u00b6 Customized foreach wrapper. If value is instance of StagedIterable , its regarded as a customized foreach behavior and used to generate code for the python for loop. Otherwise, we try to execute the loop as usual. Source code in freetensor/core/staging.py def foreach ( self , names , iter , body : Callable [[ Any ], None ]) -> None : '''Customized foreach wrapper. If `value` is instance of `StagedIterable`, its regarded as a customized foreach behavior and used to generate code for the python for loop. Otherwise, we try to execute the loop as usual. ''' if isinstance ( iter , StagedIterable ): iter . foreach ( names , body ) else : for iter_var in iter : try : body ( iter_var ) except BreakException : break except ContinueException : continue functiondef_wrapper ( self , filename , func ) \u00b6 Function definition wrapper. This wrapper performs extra initialization and cleanup for function definition. Source code in freetensor/core/staging.py def functiondef_wrapper ( self , filename , func ): '''Function definition wrapper. This wrapper performs extra initialization and cleanup for function definition. ''' def wrapped ( * args , ** kwargs ): # Push debug call stack with some random line number. # It will be updated by `mark_position` calls in the function. self . debug_call_stack . append ( traceback . FrameSummary ( filename , 1 , func . __name__ )) # The called function can now return from itself, despite what the outer # control flow is. with self . allow_shortcut_scope ( True ): try : func ( * args , ** kwargs ) except ReturnException as e : result = e . value else : # No return_stmt was called, naturally returns None result = None # Pop debug call stack. self . debug_call_stack . pop () return result return wrapped if_then_else_expr ( self , predicate , then_expr , else_expr ) \u00b6 If-then-else expression staging tool. Source code in freetensor/core/staging.py def if_then_else_expr ( self , predicate , then_expr , else_expr ): '''If-then-else expression staging tool.''' if isinstance ( predicate , StagedPredicate ): return predicate . if_then_else_expr ( then_expr , else_expr ) else : if predicate : return then_expr () else : return else_expr () if_then_else_stmt ( self , predicate , then_body , else_body = None ) \u00b6 If-then-else statement staging tool. When predicate is deterministic in staging, only one branch is generated. Otherwise, a If node in IR is generated. Source code in freetensor/core/staging.py def if_then_else_stmt ( self , predicate , then_body , else_body = None ): '''If-then-else statement staging tool. When predicate is deterministic in staging, only one branch is generated. Otherwise, a If node in IR is generated. ''' if isinstance ( predicate , StagedPredicate ): predicate . if_then_else_stmt ( then_body , else_body ) else : if predicate : then_body () elif else_body : else_body () load_attr ( self , obj , attr ) \u00b6 Load attribute staging tool. Allows customization of reading attributes. Source code in freetensor/core/staging.py def load_attr ( self , obj , attr : str ): '''Load attribute staging tool. Allows customization of reading attributes.''' try : return getattr ( obj , attr ) except AttributeError : try : # Have to use AttributeError again, since a custom attribute might have # a None value result = self . custom_attr ( obj , attr ) successful = True except AttributeError : successful = False if successful : return result else : raise metadata ( self , content ) \u00b6 Metadata handler. A metadata line is a comment starting with #! and followed by a metadata, represented as a string parameter. Defaults to a no-op. Can be overridden by subclasses. Parameters: content ( str ) \u2013 The metadata content. Source code in freetensor/core/staging.py def metadata ( self , content ) -> None : ''' Metadata handler. A metadata line is a comment starting with `#! ` and followed by a metadata, represented as a string parameter. Defaults to a no-op. Can be overridden by subclasses. Parameters ---------- content : str The metadata content. ''' pass return_stmt ( self , value , funcname ) \u00b6 Return staging tool. Only allow return in static control flow. Source code in freetensor/core/staging.py def return_stmt ( self , value , funcname ): '''Return staging tool. Only allow return in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Return is only allowed in statically deterministic control flow.' ) if isinstance ( value , StagedUnpackAssignable ): # We don't know how many items are there, so no unpacking value = value . assign ( funcname ) if isinstance ( value , StagedAssignable ): value = value . assign ( funcname ) raise ReturnException ( value ) unpack_assign_stmt ( self , names , values ) \u00b6 Customized assign wrapper for one or more targets. If values is instance of StagedUnpackAssignable , it's regarded as a customized assign behavior and gets executed with all the assigned targets' names. Otherwise, it calls assign_stmt with each sub-assignments. Please note that names can be nested tuples like (\"a\", (\"b\", \"c\")) . Please also note that names can also be a single string like \"a\" even if values is a tuple. There is no unpacking in this case Source code in freetensor/core/staging.py def unpack_assign_stmt ( self , names , values ): '''Customized assign wrapper for one or more targets. If `values` is instance of `StagedUnpackAssignable`, it's regarded as a customized assign behavior and gets executed with all the assigned targets' names. Otherwise, it calls `assign_stmt` with each sub-assignments. Please note that `names` can be nested tuples like `(\"a\", (\"b\", \"c\"))`. Please also note that `names` can also be a single string like \"a\" even if `values` is a tuple. There is no unpacking in this case ''' if isinstance ( values , StagedUnpackAssignable ): return values . assign ( names ) elif isinstance ( names , str ): return self . assign_stmt ( names , values ) else : assert isinstance ( names , Sequence ) values = tuple ( values ) if len ( names ) != len ( values ): raise self . error ( \"Number of return values does not match when unpacking\" ) returns = [] for name , value in zip ( names , values ): returns . append ( self . unpack_assign_stmt ( name , value )) return tuple ( returns ) while_stmt ( self , fpred , body ) \u00b6 While statement staging tool. Source code in freetensor/core/staging.py def while_stmt ( self , fpred , body ): '''While statement staging tool.''' first_pred = fpred () if isinstance ( first_pred , StagedPredicate ): first_pred . while_stmt ( body ) else : if first_pred : try : body () except BreakException : return except ContinueException : pass while fpred (): try : body () except BreakException : break except ContinueException : continue TransformError ( Exception ) \u00b6 Error occurred during AST transforming from python function to staging function that generates IR tree. Source code in freetensor/core/staging.py class TransformError ( Exception ): '''Error occurred during AST transforming from python function to staging function that generates IR tree.''' def __init__ ( self , message : str , filename : str , base_lineno : int , error_node : ast . AST ) -> None : super () . __init__ ( f 'At { filename } : { base_lineno + error_node . lineno } : \\n { message } .' ) Transformer ( NodeTransformer ) dataclass \u00b6 Transformer(filename: 'str', base_lineno: 'int', curr_func: 'str' = None, nonlocals: 'List[List[str]]' = None) Source code in freetensor/core/staging.py @dataclass class Transformer ( ast . NodeTransformer ): filename : str base_lineno : int curr_func : str = None nonlocals : List [ List [ str ]] = None def visit ( self , node : ast . AST ): new_node = super () . visit ( node ) if isinstance ( node , ast . stmt ) and not isinstance ( node , ast . FunctionDef ): if not isinstance ( new_node , list ): new_node = [ new_node ] return [ ast . Expr ( call_helper ( StagingOverload . mark_position , ast . Constant ( self . base_lineno + node . lineno - 1 ))) ] + new_node return new_node def visit_Assign ( self , old_node : ast . Assign ) -> ast . Assign : '''Rule: `lhs = rhs` -> `lhs = unpack_assign_stmt('lhs', rhs)` `x.lhs = rhs` -> `x.lhs = unpack_assign_stmt('lhs', rhs)` `a, (b, c) = (x, (y, z))` -> `a, (b, c) = unpack_assign_stmt(('a', ('b', 'c')), (x, (y, z)))` `a = b = c` -> `a = unpack_assign_stmt('a', c); b = unpack_assign_stmt('b', c)` If `unpack_assign_stmt` is not overloaded, `assign_stmt` will be called for each item ''' node : ast . Assign = self . generic_visit ( old_node ) class UnoverloadableExcept ( BaseException ): pass def recursive_get_names ( target ): if isinstance ( target , ast . Name ): return ast . Constant ( target . id ) elif isinstance ( target , ast . Attribute ): return ast . Constant ( target . attr ) elif isinstance ( target , ast . Tuple ): # Unpacking: (a, b) = c l = [] for t in target . elts : l . append ( recursive_get_names ( t )) return ast . Tuple ( l , ast . Load ()) else : raise UnoverloadableExcept () def do_visit_assign ( targets ): try : names = recursive_get_names ( targets ) return ast . Assign ([ targets ], call_helper ( StagingOverload . unpack_assign_stmt , names , node . value )) except UnoverloadableExcept : return ast . Assign ([ targets ], node . value ) # If there are more than one item in `node.targets`, it means multiple # assignments like `a = b = c`. For unpacking like `(a, b) = c`, it # is represented as one tuple as a target item new_nodes = [] for target in node . targets : new_nodes . append ( do_visit_assign ( target )) return new_nodes def handleType_AnnAssign ( self , node : ast . AnnAssign ) -> Any : x = node . target assert isinstance ( x , ast . Name ) assert node . value is None x_str = ast . Constant ( x . id ) Ty = node . annotation intermediate = f 'freetensor__annotate__ { x . id } ' intermediate_store = ast . Name ( intermediate , ast . Store ()) intermediate_load = ast . Name ( intermediate , ast . Load ()) node = [ ast . Assign ([ intermediate_store ], call_helper ( StagingOverload . annotate_stmt , x_str , Ty )), ast . If ( intermediate_load , [ ast . Assign ([ x ], intermediate_load )], []) ] return node def visit_AnnAssign ( self , old_node : ast . AnnAssign ) -> Any : '''Rule: `x: Ty` -> ``` freetensor__annotate__x = annotate_stmt('x', Ty) if freetensor__annotate__x: x = freetensor__annotate__x ```: pure annotation ''' node : ast . AnnAssign = self . generic_visit ( old_node ) if isinstance ( node . target , ast . Name ) and node . value is None : node = self . handleType_AnnAssign ( node ) return node def visit_For ( self , old_node : ast . For ): '''Rule: ``` for x in iter: body ``` -> ``` def for_body(x): body foreach('x', iter, for_body) ```''' if len ( old_node . orelse ) == 0 : with NonlocalTransformingScope ( self ) as nonlocals : # While opening a fake function, For loops initiates an iter name as # well. Need to remove it from the outer nonlocals list to implement # shadowing. Only For loops behaves as such, so handle it specially here. nonlocals = set ( nonlocals ) def recursive_remove_id ( target ): if isinstance ( target , ast . Name ): if target . id in nonlocals : nonlocals . remove ( target . id ) else : assert isinstance ( target , ast . Tuple ) for t in target . elts : recursive_remove_id ( t ) recursive_remove_id ( old_node . target ) nonlocals = list ( nonlocals ) def recursive_get_names ( target ): if isinstance ( target , ast . Name ): return ast . Constant ( target . id ) else : l = [] assert isinstance ( target , ast . Tuple ) for t in target . elts : l . append ( recursive_get_names ( t )) return ast . Tuple ( l , ast . Load ()) target_names = recursive_get_names ( old_node . target ) node : ast . For = self . generic_visit ( old_node ) node = [ function_helper ( 'for_body' , [ '__item__' ], [ ast . Assign ([ node . target ], ast . Name ( '__item__' , ast . Load ())) ] + node . body , nonlocals ), ast . Expr ( call_helper ( StagingOverload . foreach , target_names , node . iter , ast . Name ( 'for_body' , ast . Load ()))) ] else : node = self . generic_visit ( old_node ) return node def visit_While ( self , old_node : ast . While ) -> Any : '''Rule: ``` while pred: body ``` -> ``` def while_body(): body while_stmt(lambda: pred, while_body) ```''' with NonlocalTransformingScope ( self ) as nonlocals : node : ast . While = self . generic_visit ( old_node ) node = [ function_helper ( 'while_body' , [], node . body , nonlocals ), ast . Expr ( call_helper ( StagingOverload . while_stmt , ast . Lambda ( _EMPTY_ARGS , node . test ), ast . Name ( 'while_body' , ast . Load ()))) ] return node def visit_If ( self , old_node : ast . If ): '''Rule: ``` if pred: body else: orelse ``` -> ``` def then_body(): body def else_body(): orelse if_then_else_stmt(pred, then_body, else_body) ``` ''' test = self . visit ( old_node . test ) with NonlocalTransformingScope ( self ) as nonlocals : new_node = [ function_helper ( 'then_body' , [], [ z for x in old_node . body for y in [ self . visit ( x )] for z in ( y if isinstance ( y , list ) else [ y ]) ], nonlocals ) ] then_body = ast . Name ( 'then_body' , ast . Load ()) if old_node . orelse : with NonlocalTransformingScope ( self ) as nonlocals : new_node . append ( function_helper ( 'else_body' , [], [ z for x in old_node . orelse for y in [ self . visit ( x )] for z in ( y if isinstance ( y , list ) else [ y ]) ], nonlocals )) else_body = ast . Name ( 'else_body' , ast . Load ()) else : else_body = ast . Constant ( None ) new_node . append ( ast . Expr ( call_helper ( StagingOverload . if_then_else_stmt , test , then_body , else_body ))) return new_node def visit_IfExp ( self , old_node : ast . IfExp ): '''Rule: `body if test else orelse` -> `if_then_else_expr(test, body, orelse)`''' node = self . generic_visit ( old_node ) node = call_helper ( StagingOverload . if_then_else_expr , node . test , ast . Lambda ( _EMPTY_ARGS , node . body ), ast . Lambda ( _EMPTY_ARGS , node . orelse )) return node def visit_FunctionDef ( self , old_node : ast . FunctionDef ) -> Any : prev_func = self . curr_func self . curr_func = old_node . name # nested functions follow original Python (shitty) scoping, # thus backup the nonlocals stack and prepare a clean one. prev_nonlocals = self . nonlocals self . nonlocals = None with NonlocalTransformingScope ( self ): # mark arguments as nonlocal for name in old_node . args . args + old_node . args . kwonlyargs : self . nonlocals [ - 1 ] . append ( name . arg ) if old_node . args . vararg : self . nonlocals [ - 1 ] . append ( old_node . args . vararg . arg ) if old_node . args . kwarg : self . nonlocals [ - 1 ] . append ( old_node . args . kwarg . arg ) # Transform the function body node : ast . FunctionDef = self . generic_visit ( old_node ) # Cleanup the decorators node . decorator_list = [ call_helper ( StagingOverload . functiondef_decorator , ast . Constant ( self . filename )) ] # Handle the type annotations node . body = [ stmt for arg in node . args . posonlyargs + node . args . args if arg . annotation for stmt in self . handleType_AnnAssign ( ast . AnnAssign ( ast . Name ( arg . arg , ast . Store ()), arg . annotation , None , 1 )) ] + node . body # Cleanup annotations; we don't need them anymore for arg in [ node . args . vararg , node . args . kwarg ] + node . args . posonlyargs + node . args . args + node . args . kwonlyargs : if arg is not None : arg . annotation = None self . curr_func = prev_func self . nonlocals = prev_nonlocals return node def visit_Assert ( self , old_node : ast . Assert ) -> Any : node : ast . Assert = self . generic_visit ( old_node ) node = ast . Expr ( call_helper ( StagingOverload . assert_stmt , node . test )) return node def visit_BoolOp ( self , old_node : ast . BoolOp ) -> Any : node : ast . BoolOp = self . generic_visit ( old_node ) if isinstance ( node . op , ast . And ): libfunc = StagingOverload . and_expr elif isinstance ( node . op , ast . Or ): libfunc = StagingOverload . or_expr else : return node node = call_helper ( libfunc , * [ ast . Lambda ( _EMPTY_ARGS , v ) for v in node . values ]) return node def visit_UnaryOp ( self , old_node : ast . UnaryOp ) -> Any : node : ast . UnaryOp = self . generic_visit ( old_node ) if isinstance ( node . op , ast . Not ): node = call_helper ( StagingOverload . not_expr , node . operand ) return node def visit_Compare ( self , old_node : ast . Compare ) -> Any : '''Expand multiple comparison into `and` expression.''' if len ( old_node . comparators ) == 1 : return self . generic_visit ( old_node ) lhs = old_node . left node = ast . BoolOp ( ast . And (), []) for op , rhs in zip ( old_node . ops , old_node . comparators ): node . values . append ( ast . Compare ( lhs , [ op ], [ rhs ])) lhs = rhs return self . visit ( node ) def visit_Attribute ( self , old_node : ast . Attribute ) -> Any : node : ast . Attribute = self . generic_visit ( old_node ) if isinstance ( node . ctx , ast . Load ): if not ( isinstance ( node . value , ast . Name ) and node . value . id == '__staging_overload__' ): node = call_helper ( StagingOverload . load_attr , node . value , ast . Constant ( node . attr )) return node def visit_Return ( self , old_node : ast . Return ) -> Any : node : ast . Return = self . generic_visit ( old_node ) assert self . curr_func is not None node = ast . Expr ( call_helper ( StagingOverload . return_stmt , node . value , ast . Constant ( self . curr_func ))) return node def visit_Lambda ( self , old_node : ast . Lambda ) -> Any : with NonlocalTransformingScope ( self ): node : ast . Lambda = self . generic_visit ( old_node ) return node def visit_comprehension ( self , old_node : ast . comprehension ) -> Any : with NonlocalTransformingScope ( self ): node : ast . comprehension = self . generic_visit ( old_node ) return node def visit_Name ( self , node : ast . Name ) -> Any : if isinstance ( node . ctx , ast . Store ): self . nonlocals [ - 1 ] . append ( node . id ) return self . generic_visit ( node ) def visit_AsyncFunctionDef ( self , node : ast . AsyncFunctionDef ) -> Any : raise TransformError ( 'Async functions not supported.' , self . filename , self . base_lineno , node ) def visit_ClassDef ( self , node : ast . ClassDef ) -> Any : raise TransformError ( 'Class definitions not supported.' , self . filename , self . base_lineno , node ) def visit_Yield ( self , node : ast . Yield ) -> Any : raise NotImplementedError () def visit_YieldFrom ( self , node : ast . YieldFrom ) -> Any : raise NotImplementedError () def visit_Break ( self , node : ast . Break ) -> Any : return ast . Expr ( call_helper ( StagingOverload . break_stmt )) def visit_Continue ( self , node : ast . Continue ) -> Any : return ast . Expr ( call_helper ( StagingOverload . continue_stmt )) generic_visit ( self , node ) inherited \u00b6 Called if no explicit visitor function exists for a node. Source code in freetensor/core/staging.py def generic_visit ( self , node ): for field , old_value in iter_fields ( node ): if isinstance ( old_value , list ): new_values = [] for value in old_value : if isinstance ( value , AST ): value = self . visit ( value ) if value is None : continue elif not isinstance ( value , AST ): new_values . extend ( value ) continue new_values . append ( value ) old_value [:] = new_values elif isinstance ( old_value , AST ): new_node = self . visit ( old_value ) if new_node is None : delattr ( node , field ) else : setattr ( node , field , new_node ) return node visit ( self , node ) \u00b6 Visit a node. Source code in freetensor/core/staging.py def visit ( self , node : ast . AST ): new_node = super () . visit ( node ) if isinstance ( node , ast . stmt ) and not isinstance ( node , ast . FunctionDef ): if not isinstance ( new_node , list ): new_node = [ new_node ] return [ ast . Expr ( call_helper ( StagingOverload . mark_position , ast . Constant ( self . base_lineno + node . lineno - 1 ))) ] + new_node return new_node visit_AnnAssign ( self , old_node ) \u00b6 Rule: x: Ty -> freetensor__annotate__x = annotate_stmt('x', Ty) if freetensor__annotate__x: x = freetensor__annotate__x : pure annotation Source code in freetensor/core/staging.py def visit_AnnAssign ( self , old_node : ast . AnnAssign ) -> Any : '''Rule: `x: Ty` -> ``` freetensor__annotate__x = annotate_stmt('x', Ty) if freetensor__annotate__x: x = freetensor__annotate__x ```: pure annotation ''' node : ast . AnnAssign = self . generic_visit ( old_node ) if isinstance ( node . target , ast . Name ) and node . value is None : node = self . handleType_AnnAssign ( node ) return node visit_Assign ( self , old_node ) \u00b6 Rule: lhs = rhs -> lhs = unpack_assign_stmt('lhs', rhs) x.lhs = rhs -> x.lhs = unpack_assign_stmt('lhs', rhs) a, (b, c) = (x, (y, z)) -> a, (b, c) = unpack_assign_stmt(('a', ('b', 'c')), (x, (y, z))) a = b = c -> a = unpack_assign_stmt('a', c); b = unpack_assign_stmt('b', c) If unpack_assign_stmt is not overloaded, assign_stmt will be called for each item Source code in freetensor/core/staging.py def visit_Assign ( self , old_node : ast . Assign ) -> ast . Assign : '''Rule: `lhs = rhs` -> `lhs = unpack_assign_stmt('lhs', rhs)` `x.lhs = rhs` -> `x.lhs = unpack_assign_stmt('lhs', rhs)` `a, (b, c) = (x, (y, z))` -> `a, (b, c) = unpack_assign_stmt(('a', ('b', 'c')), (x, (y, z)))` `a = b = c` -> `a = unpack_assign_stmt('a', c); b = unpack_assign_stmt('b', c)` If `unpack_assign_stmt` is not overloaded, `assign_stmt` will be called for each item ''' node : ast . Assign = self . generic_visit ( old_node ) class UnoverloadableExcept ( BaseException ): pass def recursive_get_names ( target ): if isinstance ( target , ast . Name ): return ast . Constant ( target . id ) elif isinstance ( target , ast . Attribute ): return ast . Constant ( target . attr ) elif isinstance ( target , ast . Tuple ): # Unpacking: (a, b) = c l = [] for t in target . elts : l . append ( recursive_get_names ( t )) return ast . Tuple ( l , ast . Load ()) else : raise UnoverloadableExcept () def do_visit_assign ( targets ): try : names = recursive_get_names ( targets ) return ast . Assign ([ targets ], call_helper ( StagingOverload . unpack_assign_stmt , names , node . value )) except UnoverloadableExcept : return ast . Assign ([ targets ], node . value ) # If there are more than one item in `node.targets`, it means multiple # assignments like `a = b = c`. For unpacking like `(a, b) = c`, it # is represented as one tuple as a target item new_nodes = [] for target in node . targets : new_nodes . append ( do_visit_assign ( target )) return new_nodes visit_Compare ( self , old_node ) \u00b6 Expand multiple comparison into and expression. Source code in freetensor/core/staging.py def visit_Compare ( self , old_node : ast . Compare ) -> Any : '''Expand multiple comparison into `and` expression.''' if len ( old_node . comparators ) == 1 : return self . generic_visit ( old_node ) lhs = old_node . left node = ast . BoolOp ( ast . And (), []) for op , rhs in zip ( old_node . ops , old_node . comparators ): node . values . append ( ast . Compare ( lhs , [ op ], [ rhs ])) lhs = rhs return self . visit ( node ) visit_For ( self , old_node ) \u00b6 Rule: for x in iter: body -> def for_body(x): body foreach('x', iter, for_body) Source code in freetensor/core/staging.py def visit_For ( self , old_node : ast . For ): '''Rule: ``` for x in iter: body ``` -> ``` def for_body(x): body foreach('x', iter, for_body) ```''' if len ( old_node . orelse ) == 0 : with NonlocalTransformingScope ( self ) as nonlocals : # While opening a fake function, For loops initiates an iter name as # well. Need to remove it from the outer nonlocals list to implement # shadowing. Only For loops behaves as such, so handle it specially here. nonlocals = set ( nonlocals ) def recursive_remove_id ( target ): if isinstance ( target , ast . Name ): if target . id in nonlocals : nonlocals . remove ( target . id ) else : assert isinstance ( target , ast . Tuple ) for t in target . elts : recursive_remove_id ( t ) recursive_remove_id ( old_node . target ) nonlocals = list ( nonlocals ) def recursive_get_names ( target ): if isinstance ( target , ast . Name ): return ast . Constant ( target . id ) else : l = [] assert isinstance ( target , ast . Tuple ) for t in target . elts : l . append ( recursive_get_names ( t )) return ast . Tuple ( l , ast . Load ()) target_names = recursive_get_names ( old_node . target ) node : ast . For = self . generic_visit ( old_node ) node = [ function_helper ( 'for_body' , [ '__item__' ], [ ast . Assign ([ node . target ], ast . Name ( '__item__' , ast . Load ())) ] + node . body , nonlocals ), ast . Expr ( call_helper ( StagingOverload . foreach , target_names , node . iter , ast . Name ( 'for_body' , ast . Load ()))) ] else : node = self . generic_visit ( old_node ) return node visit_If ( self , old_node ) \u00b6 Rule: if pred: body else: orelse -> def then_body(): body def else_body(): orelse if_then_else_stmt(pred, then_body, else_body) Source code in freetensor/core/staging.py def visit_If ( self , old_node : ast . If ): '''Rule: ``` if pred: body else: orelse ``` -> ``` def then_body(): body def else_body(): orelse if_then_else_stmt(pred, then_body, else_body) ``` ''' test = self . visit ( old_node . test ) with NonlocalTransformingScope ( self ) as nonlocals : new_node = [ function_helper ( 'then_body' , [], [ z for x in old_node . body for y in [ self . visit ( x )] for z in ( y if isinstance ( y , list ) else [ y ]) ], nonlocals ) ] then_body = ast . Name ( 'then_body' , ast . Load ()) if old_node . orelse : with NonlocalTransformingScope ( self ) as nonlocals : new_node . append ( function_helper ( 'else_body' , [], [ z for x in old_node . orelse for y in [ self . visit ( x )] for z in ( y if isinstance ( y , list ) else [ y ]) ], nonlocals )) else_body = ast . Name ( 'else_body' , ast . Load ()) else : else_body = ast . Constant ( None ) new_node . append ( ast . Expr ( call_helper ( StagingOverload . if_then_else_stmt , test , then_body , else_body ))) return new_node visit_IfExp ( self , old_node ) \u00b6 Rule: body if test else orelse -> if_then_else_expr(test, body, orelse) Source code in freetensor/core/staging.py def visit_IfExp ( self , old_node : ast . IfExp ): '''Rule: `body if test else orelse` -> `if_then_else_expr(test, body, orelse)`''' node = self . generic_visit ( old_node ) node = call_helper ( StagingOverload . if_then_else_expr , node . test , ast . Lambda ( _EMPTY_ARGS , node . body ), ast . Lambda ( _EMPTY_ARGS , node . orelse )) return node visit_While ( self , old_node ) \u00b6 Rule: while pred: body -> def while_body(): body while_stmt(lambda: pred, while_body) Source code in freetensor/core/staging.py def visit_While ( self , old_node : ast . While ) -> Any : '''Rule: ``` while pred: body ``` -> ``` def while_body(): body while_stmt(lambda: pred, while_body) ```''' with NonlocalTransformingScope ( self ) as nonlocals : node : ast . While = self . generic_visit ( old_node ) node = [ function_helper ( 'while_body' , [], node . body , nonlocals ), ast . Expr ( call_helper ( StagingOverload . while_stmt , ast . Lambda ( _EMPTY_ARGS , node . test ), ast . Name ( 'while_body' , ast . Load ()))) ] return node call_helper ( callee , * args , ** kwargs ) \u00b6 Call helper that generates a python AST Call node with given callee (overload member) and arguments AST node. Source code in freetensor/core/staging.py def call_helper ( callee , * args : ast . expr , ** kwargs : ast . expr ): '''Call helper that generates a python AST Call node with given callee (overload member) and arguments AST node.''' return ast . Call ( ast . Attribute ( ast . Name ( '__staging_overload__' , ast . Load ()), callee . __name__ , ast . Load ()), list ( args ), [ ast . keyword ( k , w ) for k , w in kwargs . items ()]) function_helper ( name , args , body , nonlocals ) \u00b6 Function helper that generates a python AST FunctionDef node with given name, arguments name, and body. Source code in freetensor/core/staging.py def function_helper ( name : str , args : Sequence [ str ], body : List [ ast . stmt ], nonlocals : List [ str ]): '''Function helper that generates a python AST FunctionDef node with given name, arguments name, and body.''' nonlocal_body = ([ ast . Nonlocal ( nonlocals )] if len ( nonlocals ) > 0 else []) + body return ast . FunctionDef ( name = name , args = ast . arguments ( args = [], vararg = None , kwarg = None , posonlyargs = [ ast . arg ( a , None ) for a in args ], defaults = [], kwonlyargs = [], kw_defaults = []), body = nonlocal_body , returns = None , decorator_list = []) stmt \u00b6 Facility to build AST statements Classes and functions in this module are internally used by transformer to construct ASTs. They are also used by some internal tests. API of these classes and functions are subject to changes. End users are encouraged to use transformer , instead of this module. Classes and functions in this module are all in BigCamel naming style, to distinguish from expressions in expr.py Assert \u00b6 Scope used to create an Assert node This scope is internally used by transformer and tests E.g.: with Assert(i > 0): ... # Assertion body Source code in freetensor/core/stmt.py class Assert : ''' Scope used to create an Assert node This scope is internally used by `transformer` and tests E.g.: ``` with Assert(i > 0): ... # Assertion body ``` ''' def __init__ ( self , cond ): self . cond = cond def __enter__ ( self ): ctx_stack . push () def __exit__ ( self , exc_type , exc_value , traceback ): if exc_value is not None : # Do not generate an AST node return False # Do not suppress the exception body = ctx_stack . pop () . make_stmt () top = ctx_stack . top () top . append_stmt ( ffi . makeAssert ( self . cond , body , top . get_metadata ())) Else \u00b6 Scope used to create an else branch of an If node This scope is internally used by transformer and tests E.g.: with If(i > 0): ... # True branch with Else(): ... # Else branch Source code in freetensor/core/stmt.py class Else : ''' Scope used to create an else branch of an If node This scope is internally used by `transformer` and tests E.g.: ``` with If(i > 0): ... # True branch with Else(): ... # Else branch ``` ''' def __init__ ( self ): pass def __enter__ ( self ): ctx_stack . push () def __exit__ ( self , exc_type , exc_value , traceback ): if exc_value is not None : # Do not generate an AST node return False # Do not suppress the exception body = ctx_stack . pop () . make_stmt () ctx_stack . top () . append_if_else_stmt ( body ) For \u00b6 Scope used to create a For node This scope is internally used by transformer and tests E.g.: with For('i', 0, n) as i: ... # Loop body Source code in freetensor/core/stmt.py class For : ''' Scope used to create a For node This scope is internally used by `transformer` and tests E.g.: ``` with For('i', 0, n) as i: ... # Loop body ``` ''' def __init__ ( self , iter_var : str , begin , end , step = 1 , label : Optional [ str ] = None , no_deps : Optional [ Sequence [ str ]] = None , prefer_libs : Optional [ bool ] = None ): self . iter_var = iter_var self . begin = begin self . end = end self . step = step self . label = label self . no_deps = no_deps self . prefer_libs = prefer_libs self . borrowed_vardefs = set () for x in [ begin , end , step ]: for name in ffi . all_reads ( ffi . Expr ( x )): self . borrowed_vardefs . add ( open_vardefs [ name ]) def __enter__ ( self ): for item in self . borrowed_vardefs : item . lend_out () ctx_stack . push () return ffi . makeVar ( self . iter_var ) def __exit__ ( self , exc_type , exc_value , traceback ): for item in self . borrowed_vardefs : item . reclaim () if exc_value is not None : # Do not generate an AST node return False # Do not suppress the exception body = ctx_stack . pop () . make_stmt () top = ctx_stack . top () top . append_for_stmt ( self . iter_var , self . begin , self . end , self . step , body , metadata = ffi . SourceMetadata ([ self . label ]) if self . label is not None else None , no_deps = self . no_deps , prefer_libs = self . prefer_libs ) If \u00b6 Scope used to create an If node This scope is internally used by transformer and tests E.g.: with If(i > 0): ... # Branch body Source code in freetensor/core/stmt.py class If : ''' Scope used to create an If node This scope is internally used by `transformer` and tests E.g.: ``` with If(i > 0): ... # Branch body ``` ''' def __init__ ( self , cond ): self . cond = cond def __enter__ ( self ): ctx_stack . push () def __exit__ ( self , exc_type , exc_value , traceback ): if exc_value is not None : # Do not generate an AST node return False # Do not suppress the exception body = ctx_stack . pop () . make_stmt () ctx_stack . top () . append_if_then_stmt ( self . cond , body ) Invoke \u00b6 Inlined invocation of another AST Invoke is used as a scope ( with Invoke(...) as returned_vars ), so that variables returned by the callee can be used in the socpe Invoke can be used for invoking a gradient function, which has already been lowered as an AST. Please note that once a user function has been lowered as an AST, the dimensionalities of its tensors get fixed. Therefore, to invoke ordinary user functions, please use inline in transformer instead, which supports generic types Source code in freetensor/core/stmt.py class Invoke : ''' Inlined invocation of another AST `Invoke` is used as a scope (`with Invoke(...) as returned_vars`), so that variables returned by the callee can be used in the socpe `Invoke` can be used for invoking a gradient function, which has already been lowered as an AST. Please note that once a user function has been lowered as an AST, the dimensionalities of its tensors get fixed. Therefore, to invoke ordinary user functions, please use `inline` in `transformer` instead, which supports generic types ''' def __init__ ( self , ret_names : Sequence [ str ], func : ffi . Func , args : Sequence = [], kvs : Mapping = {}): self . args = args self . kvs = kvs self . func , returns = ffi . strip_returns ( func ) self . vardefs = [] # Outer to inner assert len ( ret_names ) == len ( returns ) for name , ret in zip ( ret_names , returns ): self . vardefs . append ( _VarDef ( name , ret . tensor . shape , ret . tensor . dtype , \"cache\" , ret . mtype )) def __enter__ ( self ): varrefs = [] ret_names = [] for vardef in self . vardefs : varref = vardef . __enter__ () varrefs . append ( varref ) ret_names . append ( varref . name ) ctx_stack . top () . append_stmt ( ffi . inlined_invoke ( ctx_stack . top () . get_metadata (), self . func , self . args , self . kvs , ret_names )) return varrefs [ 0 ] if len ( varrefs ) == 1 else tuple ( varrefs ) def __exit__ ( self , exc_type , exc_value , traceback ): for vardef in reversed ( self . vardefs ): vardef . __exit__ ( exc_type , exc_value , traceback ) NamedScope \u00b6 Scope used to create an StmtSeq node with an explicit ID E.g.: with NamedScope(): ... # body This scope is used for testing only. StmtSeq nodes can be deleted in many lowering passes Source code in freetensor/core/stmt.py class NamedScope : ''' Scope used to create an StmtSeq node with an explicit ID E.g.: ``` with NamedScope(): ... # body ``` This scope is used for testing only. StmtSeq nodes can be deleted in many lowering passes ''' def __init__ ( self , * labels : str ): self . labels = labels def __enter__ ( self ): ctx_stack . push () def __exit__ ( self , exc_type , exc_value , traceback ): if exc_value is not None : # Do not generate an AST node return False # Do not suppress the exception finished_scope = ctx_stack . pop () metadata = ctx_stack . top () . get_metadata ( self . labels ) body = finished_scope . make_stmt ( metadata ) ctx_stack . top () . append_stmt ( body ) Any () \u00b6 Create an Any node (only for testing) Any nodes matches any statement nodes in ast.match Source code in freetensor/core/stmt.py def Any (): ''' Create an Any node (only for testing) Any nodes matches any statement nodes in `ast.match` ''' ctx_stack . top () . append_stmt ( ffi . makeAny ()) Eval ( expr ) \u00b6 Create an Eval node This scope is internally used by transformer and tests Source code in freetensor/core/stmt.py def Eval ( expr ): ''' Create an Eval node This scope is internally used by `transformer` and tests ''' top = ctx_stack . top () top . append_stmt ( ffi . makeEval ( expr , top . get_metadata ())) MarkLabel ( label ) \u00b6 Mark the ID of the following statement This scope is internally used by transformer and tests Source code in freetensor/core/stmt.py def MarkLabel ( label : str ): \"\"\" Mark the ID of the following statement This scope is internally used by `transformer` and tests \"\"\" ctx_stack . top () . add_label ( label ) VarDef ( * args , ** kvs ) \u00b6 A factory function that creates a VarDef or a series of nested VarDef s This scope is internally used by transformer and tests Source code in freetensor/core/stmt.py def VarDef ( * args , ** kvs ): ''' A factory function that creates a VarDef or a series of nested `VarDef`s This scope is internally used by `transformer` and tests ''' if len ( args ) == 1 : return _VarsDef ( args [ 0 ]) else : return _VarDef ( * args , ** kvs ) libop special \u00b6 assign \u00b6 add_to ( * _args , ** _kvs ) \u00b6 (Broadcasted) add to a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor Source code in freetensor/libop/assign.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) assign ( * _args , ** _kvs ) \u00b6 (Broadcasted) assign to a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor Source code in freetensor/libop/assign.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) floordiv_to ( * _args , ** _kvs ) \u00b6 (Broadcasted) rounding-towards-negative-infinity integer division (following Python convention, but not C) from a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor Source code in freetensor/libop/assign.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) mod_to ( * _args , ** _kvs ) \u00b6 (Broadcasted) modulo (results are non-negative, following Python convention, but not C) from a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor Source code in freetensor/libop/assign.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) mul_to ( * _args , ** _kvs ) \u00b6 (Broadcasted) multiply to a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor Source code in freetensor/libop/assign.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) sub_to ( * _args , ** _kvs ) \u00b6 (Broadcasted) subtract from a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor Source code in freetensor/libop/assign.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) truediv_to ( * _args , ** _kvs ) \u00b6 (Broadcasted) floating-point division from a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor Source code in freetensor/libop/assign.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) constant \u00b6 zeros ( shape , dtype , mtype = None ) \u00b6 Create a zero tensor Parameters: shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used Returns: The zero tensor Source code in freetensor/libop/constant.py @core . inline def zeros ( shape , dtype , mtype = None ): ''' Create a zero tensor Parameters ---------- shape : Sequence[Expr] or Var Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype : str or DataType Data type of the variable mtype : str or MemType (Optional) Memory type of the variable. If omitted, the main memory type of the default Target in config will be used Returns ------- VarRef : The zero tensor ''' y = core . empty ( shape , dtype , mtype ) #! label: recur zeros_ ( y ) return y zeros_ ( y ) \u00b6 Fill zeros to a tensor Parameters: y ( VarRef ) \u2013 The tensor to fill Source code in freetensor/libop/constant.py @core . inline def zeros_ ( y ): ''' Fill zeros to a tensor Parameters ---------- y : VarRef The tensor to fill ''' if core . ndim ( y ) == 0 : y [()] = core . zero_value ( y . dtype ) else : #! label: L_elem for i in range ( core . shape ( y , 0 )): #! label: recur zeros_ ( y [ i ]) conv \u00b6 conv ( X , W , B = None , auto_pad = 'NOTSET' , dilations = None , group = 1 , kernel_shape = None , pads = None , strides = None ) \u00b6 Convolution. The result is returned Parameters follow ONNX convention. Currently only 2-D convolution is supported Source code in freetensor/libop/conv.py @core . inline def conv ( X , W , B = None , auto_pad : str = 'NOTSET' , dilations : Optional [ Sequence [ int ]] = None , group : int = 1 , kernel_shape : Optional [ Sequence [ int ]] = None , pads : Optional [ Sequence [ int ]] = None , strides : Optional [ Sequence [ int ]] = None ): ''' Convolution. The result is returned Parameters follow ONNX convention. Currently only 2-D convolution is supported ''' n_spatial_dim = 2 # Currently only 2-D convolution is supported (TODO) if dilations is None : dilations = [ 1 ] * n_spatial_dim if strides is None : strides = [ 1 ] * n_spatial_dim if pads is None : if auto_pad == 'VALID' : pads = list ( zip ( * ([[ 0 , 0 ]] * n_spatial_dim ))) pads = pads [ 0 ] + pads [ 1 ] elif auto_pad == 'SAME_UPPER' : assert kernel_shape is not None , \"SAME_UPPER pad with dynamic kernel_shape is currently not supported\" # TODO pads = list ( zip ( * [ calc_same_upper_pad ( dil , kern , stride ) for dil , kern , stride in zip ( dilations , kernel_shape , strides ) ])) pads = pads [ 0 ] + pads [ 1 ] elif auto_pad == 'SAME_LOWER' : assert kernel_shape is not None , \"SAME_UPPER pad with dynamic kernel_shape is currently not supported\" # TODO pads = list ( zip ( * [ calc_same_lower_pad ( dil , kern , stride ) for dil , kern , stride in zip ( dilations , kernel_shape , strides ) ])) pads = pads [ 0 ] + pads [ 1 ] else : assert False , \"auto_pad should be set if pads is not specified\" dtype = core . up_cast ( X . dtype , W . dtype ) mtype = core . same_mtype ( X . mtype , W . mtype ) if B is not None : dtype = core . up_cast ( dtype , B . dtype ) mtype = core . same_mtype ( mtype , B . mtype ) #! label: V_Y Y = core . empty ([ X . shape ( 0 ), W . shape ( 0 ), calc_out_size ( X . shape ( 2 ), dilations [ 0 ], W . shape ( 2 ), pads [ 0 ], pads [ 2 ], strides [ 0 ]), calc_out_size ( X . shape ( 3 ), dilations [ 1 ], W . shape ( 3 ), pads [ 1 ], pads [ 3 ], strides [ 1 ]) ], dtype , mtype ) #! label: recur conv_ ( X , W , B , Y , auto_pad , dilations , group , kernel_shape , pads , strides ) return Y conv_ ( X , W , B , Y , auto_pad = 'NOTSET' , dilations = None , group = 1 , kernel_shape = None , pads = None , strides = None ) \u00b6 Convolution. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D convolution is supported Source code in freetensor/libop/conv.py @core . inline def conv_ ( X , W , B , Y , auto_pad : str = 'NOTSET' , dilations : Optional [ Sequence [ int ]] = None , group : int = 1 , kernel_shape : Optional [ Sequence [ int ]] = None , pads : Optional [ Sequence [ int ]] = None , strides : Optional [ Sequence [ int ]] = None ): ''' Convolution. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D convolution is supported ''' n_spatial_dim = 2 # Currently only 2-D convolution is supported (TODO) if dilations is None : dilations = [ 1 ] * n_spatial_dim if strides is None : strides = [ 1 ] * n_spatial_dim if pads is None : if auto_pad == 'VALID' : pads = list ( zip ( * ([[ 0 , 0 ]] * n_spatial_dim ))) pads = pads [ 0 ] + pads [ 1 ] elif auto_pad == 'SAME_UPPER' : assert kernel_shape is not None , \"SAME_UPPER pad with dynamic kernel_shape is currently not supported\" # TODO pads = list ( zip ( * [ calc_same_upper_pad ( dil , kern , stride ) for dil , kern , stride in zip ( dilations , kernel_shape , strides ) ])) pads = pads [ 0 ] + pads [ 1 ] elif auto_pad == 'SAME_LOWER' : assert kernel_shape is not None , \"SAME_UPPER pad with dynamic kernel_shape is currently not supported\" # TODO pads = list ( zip ( * [ calc_same_lower_pad ( dil , kern , stride ) for dil , kern , stride in zip ( dilations , kernel_shape , strides ) ])) pads = pads [ 0 ] + pads [ 1 ] else : assert False , \"auto_pad should be set if pads is not specified\" if B is None : # yapf: disable #! label: L_n for n in range ( X . shape ( 0 )): #! label: L_g for g in range ( group ): #! label: L_c_out for c_out in range ( W . shape ( 0 ) // group ): #! label: L_h for h in range ( Y . shape ( 2 )): #! label: L_w for w in range ( Y . shape ( 3 )): #! label: init Y [ n , g * ( W . shape ( 0 ) // group ) + c_out , h , w ] = 0 #! label: L_c_in for c_in in range ( W . shape ( 1 )): #! label: L_kh for kh in range ( W . shape ( 2 )): #! label: L_kw for kw in range ( W . shape ( 3 )): # h_in = h * stride + kh * dilation - pad # w_in = w * stride + kw * dilation - pad if ( h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ] >= 0 and h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ] < X . shape ( 2 ) and w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ] >= 0 and w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ] < X . shape ( 3 )): #! label: compute Y [ n , g * ( W . shape ( 0 ) // group ) + c_out , h , w ] += X [ n , g * W . shape ( 1 ) + c_in , h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ], w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ] ] * W [ g * ( W . shape ( 0 ) // group ) + c_out , c_in , kh , kw ] # yapf: enable else : # yapf: disable #! label: L_n for n in range ( X . shape ( 0 )): #! label: L_g for g in range ( group ): #! label: L_c_out for c_out in range ( W . shape ( 0 ) // group ): #! label: L_h for h in range ( Y . shape ( 2 )): #! label: L_w for w in range ( Y . shape ( 3 )): #! label: init Y [ n , g * ( W . shape ( 0 ) // group ) + c_out , h , w ] = B [ g * ( W . shape ( 0 ) // group ) + c_out ] #! label: L_c_in for c_in in range ( W . shape ( 1 )): #! label: L_kh for kh in range ( W . shape ( 2 )): #! label: L_kw for kw in range ( W . shape ( 3 )): # h_in = h * stride + kh * dilation - pad # w_in = w * stride + kw * dilation - pad if ( h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ] >= 0 and h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ] < X . shape ( 2 ) and w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ] >= 0 and w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ] < X . shape ( 3 )): #! label: compute Y [ n , g * ( W . shape ( 0 ) // group ) + c_out , h , w ] += X [ n , g * W . shape ( 1 ) + c_in , h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ], w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ] ] * W [ g * ( W . shape ( 0 ) // group ) + c_out , c_in , kh , kw ] # yapf: enable element_wise \u00b6 abs ( * _args , ** _kvs ) \u00b6 Element-wise absolute value of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) abs_ ( * _args , ** _kvs ) \u00b6 Element-wise absolute value of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) add ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise addition of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) add_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise addition of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) ceil ( * _args , ** _kvs ) \u00b6 Element-wise ceil of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) ceil_ ( * _args , ** _kvs ) \u00b6 Element-wise ceil of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) ceildiv ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise rounding-towards-positive-infinity integer division of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) ceildiv_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise rounding-towards-positive-infinity integer division of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) eq ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise equal of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) eq_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise equal of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) exp ( * _args , ** _kvs ) \u00b6 Element-wise natrual exponent of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) exp_ ( * _args , ** _kvs ) \u00b6 Element-wise natrual exponent of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) floor ( * _args , ** _kvs ) \u00b6 Element-wise floor of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) floor_ ( * _args , ** _kvs ) \u00b6 Element-wise floor of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) floordiv ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise rounding-towards-negative-infinity integer division (following Python convention, but not C, recommended for performance) of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) floordiv_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise rounding-towards-negative-infinity integer division (following Python convention, but not C, recommended for performance) of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) ge ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise greater-than-or-equal-to of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) ge_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise greater-than-or-equal-to of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) gt ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise greater-than of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) gt_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise greater-than of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) l_and ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise logical and of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) l_and_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise logical and of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) l_not ( * _args , ** _kvs ) \u00b6 Element-wise logical not of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) l_not_ ( * _args , ** _kvs ) \u00b6 Element-wise logical not of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) l_or ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise logical or of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) l_or_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise logical or of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) le ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise less-than-or-equal-to of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) le_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise less-than-or-equal-to of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) lt ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise less-than of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) lt_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise less-than of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) max ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise maximum of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) max_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise maximum of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) min ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise minimum of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) min_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise minimum of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) mod ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise modulo (results are non-negative, following Python convention, but not C, recommended for performance) of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) mod_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise modulo (results are non-negative, following Python convention, but not C, recommended for performance) of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) mul ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise multiplication of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) mul_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise multiplication of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) ne ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise non-equal of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) ne_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise non-equal of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) neg ( * _args , ** _kvs ) \u00b6 Element-wise negation of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) neg_ ( * _args , ** _kvs ) \u00b6 Element-wise negation of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) relu ( * _args , ** _kvs ) \u00b6 Element-wise ReLU of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) relu_ ( * _args , ** _kvs ) \u00b6 Element-wise ReLU of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) remainder ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise remainder (results can be positive or negative, following C convention, but not Python, NOT recommended for performance) of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) remainder_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise remainder (results can be positive or negative, following C convention, but not Python, NOT recommended for performance) of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) round_towards_0_div ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise rounding-towards-0 integer division (following C convention, but not Python, NOT recommended for performance) of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) round_towards_0_div_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise rounding-towards-0 integer division (following C convention, but not Python, NOT recommended for performance) of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) sigmoid ( * _args , ** _kvs ) \u00b6 Element-wise sigmoid of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) sigmoid_ ( * _args , ** _kvs ) \u00b6 Element-wise sigmoid of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) sqrt ( * _args , ** _kvs ) \u00b6 Element-wise square root of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) sqrt_ ( * _args , ** _kvs ) \u00b6 Element-wise square root of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) square ( * _args , ** _kvs ) \u00b6 Element-wise square of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) square_ ( * _args , ** _kvs ) \u00b6 Element-wise square of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) sub ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise subtraction of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) sub_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise subtraction of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) tanh ( * _args , ** _kvs ) \u00b6 Element-wise tanh of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) tanh_ ( * _args , ** _kvs ) \u00b6 Element-wise tanh of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) truediv ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise floating-point division of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) truediv_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise floating-point division of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) matmul \u00b6 einsum ( fmt , * args ) \u00b6 Einstein summation. The result is returned Parameters: fmt ( str ) \u2013 The format string. E.g. \"ik,kj->ij\" represents a matrix multiplcation args ( Sequence[VarRef] ) \u2013 All inputs arguments. E.g. if fmt is \"ik,kj->ij\" , it iterates axis i and k of args[0] , axis k and j of args[1] , axis i and j of the returned value Returns: The result tensor Source code in freetensor/libop/matmul.py @core . inline def einsum ( fmt : str , * args ): ''' Einstein summation. The result is returned Parameters ---------- fmt : str The format string. E.g. `\"ik,kj->ij\"` represents a matrix multiplcation args : Sequence[VarRef] All inputs arguments. E.g. if `fmt` is `\"ik,kj->ij\"`, it iterates axis `i` and `k` of `args[0]`, axis `k` and `j` of `args[1]`, axis `i` and `j` of the returned value Returns ------- VarRef : The result tensor ''' lefts , right = fmt . split ( '->' ) lefts = lefts . split ( ',' ) shapes = [] for v in right : offsets = [ left . find ( v ) for left in lefts ] iter_args , iter_offsets = zip ( * filter ( lambda x : x [ 1 ] != - 1 , zip ( args , offsets ))) assert len ( iter_args ) > 0 shapes . append ( iter_args [ 0 ] . shape ( iter_offsets [ 0 ])) # FIXME: compute dtype and mtype from every inputs Y = core . empty ( shapes , args [ 0 ] . dtype , args [ 0 ] . mtype ) einsum_ ( fmt , * args , Y ) return Y einsum_ ( fmt , * args ) \u00b6 Einstein summation. The result is written to the last argument Parameters: fmt ( str ) \u2013 The format string. E.g. \"ik,kj->ij\" represents a matrix multiplcation args ( Sequence[VarRef] ) \u2013 All arguments including inputs and the output. E.g. if fmt is \"ik,kj->ij\" , it iterates axis i and k of args[0] , axis k and j of args[1] , axis i and j of args[2] Source code in freetensor/libop/matmul.py @core . inline def einsum_ ( fmt : str , * args ): ''' Einstein summation. The result is written to the last argument Parameters ---------- fmt : str The format string. E.g. `\"ik,kj->ij\"` represents a matrix multiplcation args : Sequence[VarRef] All arguments including inputs and the output. E.g. if `fmt` is `\"ik,kj->ij\"`, it iterates axis `i` and `k` of `args[0]`, axis `k` and `j` of `args[1]`, axis `i` and `j` of `args[2]` ''' lefts , right = fmt . split ( '->' ) lefts = lefts . split ( ',' ) order = right for left in lefts : for idx in left : if idx not in order : order += idx _einsum_ ( lefts , right , order , True , * args ) gemm ( A , B , C = None , has_bias = False , trans_A = False , trans_B = False , alpha = 1.0 , beta = 1.0 ) \u00b6 General matrix multiplcation following BLAS convention and return the result It performs Y = alpha tr?(A) @ tr?(B) + C , where @ represents matrix multiplication, tr? represents an optional transposition Parameters: A ( VarRef ) \u2013 The left-hand-side operand of matrix multiplication B ( VarRef ) \u2013 The right-hand-side operand of matrix multiplication C ( VarRef (Optional) ) \u2013 The bias tensor trans_A ( bool ) \u2013 If true, transpose A . Defaults to False trans_B ( bool ) \u2013 If true, transpose B . Defaults to False alpha ( float ) \u2013 Coefficient of tr?(A) @ tr?(B) . Defaults to 1.0 beta ( float ) \u2013 Coefficient of C . Defaults to 1.0 Returns: The resulting tensor Source code in freetensor/libop/matmul.py @core . inline def gemm ( A , B , C = None , has_bias : bool = False , trans_A : bool = False , trans_B : bool = False , alpha : float = 1.0 , beta : float = 1.0 ): ''' General matrix multiplcation following BLAS convention and return the result It performs `Y = alpha tr?(A) @ tr?(B) + C`, where `@` represents matrix multiplication, `tr?` represents an optional transposition Parameters ---------- A : VarRef The left-hand-side operand of matrix multiplication B : VarRef The right-hand-side operand of matrix multiplication C : VarRef (Optional) The bias tensor trans_A : bool (Optional) If true, transpose `A`. Defaults to False trans_B : bool (Optional) If true, transpose `B`. Defaults to False alpha : Number (Optional) Coefficient of `tr?(A) @ tr?(B)`. Defaults to 1.0 beta : Number (Optional) Coefficient of `C`. Defaults to 1.0 Returns ------- VarRef : The resulting tensor ''' dtype = core . up_cast ( A . dtype , B . dtype ) mtype = core . same_mtype ( A . mtype , B . mtype ) if C is not None : dtype = core . up_cast ( dtype , C . dtype ) mtype = core . same_mtype ( mtype , C . mtype ) Y = core . empty ( _comp_shape ( A , B , trans_A , trans_B ), dtype , mtype ) #! label: recur gemm_ ( A , B , C , Y , trans_A , trans_B , alpha , beta ) return Y gemm_ ( A , B , C , Y , trans_A = False , trans_B = False , alpha = 1.0 , beta = 1.0 ) \u00b6 General matrix multiplcation following BLAS convention. The result is written to an existing tensor It performs Y = alpha tr?(A) @ tr?(B) + C , where @ represents matrix multiplication, tr? represents an optional transposition Parameters: A ( VarRef ) \u2013 The left-hand-side operand of matrix multiplication B ( VarRef ) \u2013 The right-hand-side operand of matrix multiplication C ( VarRef (Optional) ) \u2013 The bias tensor Y ( VarRef ) \u2013 The resulting tensor trans_A ( bool ) \u2013 If true, transpose A . Defaults to False trans_B ( bool ) \u2013 If true, transpose B . Defaults to False alpha ( float ) \u2013 Coefficient of tr?(A) @ tr?(B) . Defaults to 1.0 beta ( float ) \u2013 Coefficient of C . Defaults to 1.0 Source code in freetensor/libop/matmul.py @core . inline def gemm_ ( A , B , C , Y , trans_A : bool = False , trans_B : bool = False , alpha : float = 1.0 , beta : float = 1.0 ): ''' General matrix multiplcation following BLAS convention. The result is written to an existing tensor It performs `Y = alpha tr?(A) @ tr?(B) + C`, where `@` represents matrix multiplication, `tr?` represents an optional transposition Parameters ---------- A : VarRef The left-hand-side operand of matrix multiplication B : VarRef The right-hand-side operand of matrix multiplication C : VarRef (Optional) The bias tensor Y : VarRef The resulting tensor trans_A : bool (Optional) If true, transpose `A`. Defaults to False trans_B : bool (Optional) If true, transpose `B`. Defaults to False alpha : Number (Optional) Coefficient of `tr?(A) @ tr?(B)`. Defaults to 1.0 beta : Number (Optional) Coefficient of `C`. Defaults to 1.0 ''' a_fmt = 'ki' if trans_A else 'ik' b_fmt = 'jk' if trans_B else 'kj' fmt = f \" { a_fmt } , { b_fmt } ->ij\" if C is None : #! label: einsum einsum_ ( fmt , A , B , Y ) #! label: mul_to mul_to ( Y , alpha ) else : #! label: einsum einsum_ ( fmt , A , B , Y ) #! label: mul_to mul_to ( Y , alpha ) #! label: add_to add_to ( Y , mul ( beta , C )) matmul ( A , B ) \u00b6 Matrix multiplcation. The result is returned Parameters: A ( VarRef ) \u2013 The left-hand-side operand B ( VarRef ) \u2013 The right-hand-side operand Returns: The resulting tensor Source code in freetensor/libop/matmul.py @core . inline def matmul ( A , B ): ''' Matrix multiplcation. The result is returned Parameters ---------- A : VarRef The left-hand-side operand B : VarRef The right-hand-side operand Returns ------- VarRef : The resulting tensor ''' #! label: einsum Y = einsum ( _make_matmul_fmt ( A . ndim , B . ndim ), A , B ) return Y matmul_ ( A , B , Y ) \u00b6 Matrix multiplcation. The result is written to an existing tensor Parameters: A ( VarRef ) \u2013 The left-hand-side operand B ( VarRef ) \u2013 The right-hand-side operand C ( VarRef ) \u2013 The resulting tensor Source code in freetensor/libop/matmul.py @core . inline def matmul_ ( A , B , Y ): ''' Matrix multiplcation. The result is written to an existing tensor Parameters ---------- A : VarRef The left-hand-side operand B : VarRef The right-hand-side operand C : VarRef The resulting tensor ''' #! label: einsum einsum_ ( _make_matmul_fmt ( A . ndim , B . ndim ), A , B , Y ) pooling \u00b6 global_avg_pool ( X ) \u00b6 Global averaging pooling. The result is returned Parameters follow ONNX convention. Currently only 2-D pooling is supported Source code in freetensor/libop/pooling.py @core . inline def global_avg_pool ( X ): ''' Global averaging pooling. The result is returned Parameters follow ONNX convention. Currently only 2-D pooling is supported ''' n_spatial_dim = 2 # Currently only 2-D pooling is supported (TODO) Y = core . empty ([ X . shape ( 0 ), X . shape ( 1 )], X . dtype , X . mtype ) #! label: recur global_avg_pool_ ( X , Y ) return Y global_avg_pool_ ( X , Y ) \u00b6 Global averaging pooling. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D pooling is supported Source code in freetensor/libop/pooling.py @core . inline def global_avg_pool_ ( X , Y ): ''' Global averaging pooling. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D pooling is supported ''' n_spatial_dim = 2 # Currently only 2-D pooling is supported (TODO) #! label: L_n for n in range ( X . shape ( 0 )): #! label: L_c for c in range ( X . shape ( 1 )): #! label: init Y [ n , c ] = 0 #! label: L_h for h in range ( X . shape ( 2 )): #! label: L_w for w in range ( X . shape ( 3 )): #! label: compute Y [ n , c ] += X [ n , c , h , w ] #! label: flush Y [ n , c ] /= X . shape ( 2 ) * X . shape ( 3 ) max_pool ( X , auto_pad = 'NOTSET' , dilations = None , kernel_shape = None , pads = None , strides = None ) \u00b6 Maximum pooling. The result is returned Parameters follow ONNX convention. Currently only 2-D pooling is supported Source code in freetensor/libop/pooling.py @core . inline def max_pool ( X , auto_pad : str = 'NOTSET' , dilations : Optional [ Sequence [ int ]] = None , kernel_shape : Sequence [ int ] = None , pads : Optional [ Sequence [ int ]] = None , strides : Optional [ Sequence [ int ]] = None ): ''' Maximum pooling. The result is returned Parameters follow ONNX convention. Currently only 2-D pooling is supported ''' n_spatial_dim = 2 # Currently only 2-D pooling is supported (TODO) # TODO: ceil_mode # TODO: return_indices if dilations is None : dilations = [ 1 ] * n_spatial_dim if strides is None : # NOTE: strides default to 1 in ONNX, while default to kernel_shape in PyTorch strides = [ 1 ] * n_spatial_dim if pads is None : if auto_pad == 'VALID' : pads = list ( zip ( * ([[ 0 , 0 ]] * n_spatial_dim ))) pads = pads [ 0 ] + pads [ 1 ] elif auto_pad == 'SAME_UPPER' : pads = list ( zip ( * [ calc_same_upper_pad ( dil , kern , stride ) for dil , kern , stride in zip ( dilations , kernel_shape , strides ) ])) pads = pads [ 0 ] + pads [ 1 ] elif auto_pad == 'SAME_LOWER' : pads = list ( zip ( * [ calc_same_lower_pad ( dil , kern , stride ) for dil , kern , stride in zip ( dilations , kernel_shape , strides ) ])) pads = pads [ 0 ] + pads [ 1 ] else : assert False , \"auto_pad should be set if pads is not specified\" Y = core . empty ([ X . shape ( 0 ), X . shape ( 1 ), calc_out_size ( X . shape ( 2 ), dilations [ 0 ], kernel_shape [ 0 ], pads [ 0 ], pads [ 2 ], strides [ 0 ]), calc_out_size ( X . shape ( 3 ), dilations [ 1 ], kernel_shape [ 1 ], pads [ 1 ], pads [ 3 ], strides [ 1 ]) ], X . dtype , X . mtype ) #! label: recur max_pool_ ( X , Y , auto_pad , dilations , kernel_shape , pads , strides ) return Y max_pool_ ( X , Y , auto_pad = 'NOTSET' , dilations = None , kernel_shape = None , pads = None , strides = None ) \u00b6 Maximum pooling. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D pooling is supported Source code in freetensor/libop/pooling.py @core . inline def max_pool_ ( X , Y , auto_pad : str = 'NOTSET' , dilations : Optional [ Sequence [ int ]] = None , kernel_shape : Sequence [ int ] = None , pads : Optional [ Sequence [ int ]] = None , strides : Optional [ Sequence [ int ]] = None ): ''' Maximum pooling. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D pooling is supported ''' n_spatial_dim = 2 # Currently only 2-D pooling is supported (TODO) # TODO: ceil_mode # TODO: return_indices if dilations is None : dilations = [ 1 ] * n_spatial_dim if strides is None : # NOTE: strides default to 1 in ONNX, while default to kernel_shape in PyTorch strides = [ 1 ] * n_spatial_dim if pads is None : if auto_pad == 'VALID' : pads = list ( zip ( * ([[ 0 , 0 ]] * n_spatial_dim ))) pads = pads [ 0 ] + pads [ 1 ] elif auto_pad == 'SAME_UPPER' : pads = list ( zip ( * [ calc_same_upper_pad ( dil , kern , stride ) for dil , kern , stride in zip ( dilations , kernel_shape , strides ) ])) pads = pads [ 0 ] + pads [ 1 ] elif auto_pad == 'SAME_LOWER' : pads = list ( zip ( * [ calc_same_lower_pad ( dil , kern , stride ) for dil , kern , stride in zip ( dilations , kernel_shape , strides ) ])) pads = pads [ 0 ] + pads [ 1 ] else : assert False , \"auto_pad should be set if pads is not specified\" # yapf: disable #! label: L_n for n in range ( X . shape ( 0 )): #! label: L_c for c in range ( X . shape ( 1 )): #! label: L_h for h in range ( Y . shape ( 2 )): #! label: L_w for w in range ( Y . shape ( 3 )): #! label: init Y [ n , c , h , w ] = core . min_value ( X . dtype ) #! label: L_kh for kh in range ( kernel_shape [ 0 ]): #! label: L_kw for kw in range ( kernel_shape [ 1 ]): # h_in = h * stride + kh * dilation - pad # w_in = w * stride + kw * dilation - pad if ( h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ] >= 0 and h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ] < X . shape ( 2 ) and w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ] >= 0 and w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ] < X . shape ( 3 )): #! label: compute Y [ n , c , h , w ] = core . max ( Y [ n , c , h , w ], X [ n , c , h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ], w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ]]) # yapf: enable reduction \u00b6 all ( * _args , ** _kvs ) \u00b6 Reduction of logical and of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/reduction.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) all_ ( * _args , ** _kvs ) \u00b6 Reduction of logical and of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Source code in freetensor/libop/reduction.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) any ( * _args , ** _kvs ) \u00b6 Reduction of logical or of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/reduction.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) any_ ( * _args , ** _kvs ) \u00b6 Reduction of logical or of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Source code in freetensor/libop/reduction.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) reduce_max ( x , axes , keepdims = True ) \u00b6 Maximum of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/reduction.py @core . inline def reduce_max ( x , axes : Sequence [ int ], keepdims : bool = True ): ''' Maximum of a tensor through one or more dimensions and return the result Parameters ---------- x : VarRef The input tensor axes : Sequence[int] (Optional) Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims : bool (Optional) Keep the reduced dimensions as singleton dimensions. Defaults to True Returns ------- VarRef The result tensor ''' #! label: impl y = _general_reduce ( core . max , core . min_value ( core . dtype ( x )), x , axes , keepdims ) return y reduce_max_ ( x , y , axes , keepdims = True ) \u00b6 Maximum of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Source code in freetensor/libop/reduction.py @core . inline def reduce_max_ ( x , y , axes : Sequence [ int ], keepdims : bool = True ): ''' Maximum of a tensor through one or more dimensions. The result is written to another tensor Parameters ---------- x : VarRef The input tensor y : VarRef The result tensor axes : Sequence[int] (Optional) Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims : bool (Optional) Keep the reduced dimensions as singleton dimensions. Defaults to True ''' #! label: impl _general_reduce_ ( core . max , core . min_value ( core . dtype ( x )), x , y , axes , keepdims ) reduce_min ( x , axes , keepdims = True ) \u00b6 Minimum of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/reduction.py @core . inline def reduce_min ( x , axes : Sequence [ int ], keepdims : bool = True ): ''' Minimum of a tensor through one or more dimensions and return the result Parameters ---------- x : VarRef The input tensor axes : Sequence[int] (Optional) Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims : bool (Optional) Keep the reduced dimensions as singleton dimensions. Defaults to True Returns ------- VarRef The result tensor ''' #! label: impl y = _general_reduce ( core . min , core . max_value ( core . dtype ( x )), x , axes , keepdims ) return y reduce_min_ ( x , y , axes , keepdims = True ) \u00b6 Minimum of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Source code in freetensor/libop/reduction.py @core . inline def reduce_min_ ( x , y , axes : Sequence [ int ], keepdims : bool = True ): ''' Minimum of a tensor through one or more dimensions. The result is written to another tensor Parameters ---------- x : VarRef The input tensor y : VarRef The result tensor axes : Sequence[int] (Optional) Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims : bool (Optional) Keep the reduced dimensions as singleton dimensions. Defaults to True ''' #! label: impl _general_reduce_ ( core . min , core . max_value ( core . dtype ( x )), x , y , axes , keepdims ) reduce_prod ( * _args , ** _kvs ) \u00b6 Product of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/reduction.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) reduce_prod_ ( * _args , ** _kvs ) \u00b6 Product of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Source code in freetensor/libop/reduction.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) reduce_sum ( * _args , ** _kvs ) \u00b6 Sum of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/reduction.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) reduce_sum_ ( * _args , ** _kvs ) \u00b6 Sum of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Source code in freetensor/libop/reduction.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs ) reshape \u00b6 expand ( a , expand_shape ) \u00b6 Broadcast a tensor to a given shape, following the broadcasting rules Parameters: a ( VarRef ) \u2013 The input tensor b ( Sequence of expressions ) \u2013 The broadcasted shape Returns: The broadcasted tensor Source code in freetensor/libop/reshape.py @core . inline def expand ( a , expand_shape ): ''' Broadcast a tensor to a given shape, following the broadcasting rules Parameters ---------- a : VarRef The input tensor b : Sequence of expressions The broadcasted shape Returns ------- VarRef : The broadcasted tensor ''' # FIXME: out_shape = broadcast(a.shape, expand_shape) out = core . empty ( expand_shape , core . dtype ( a ), core . mtype ( a )) #! label: recur expand_ ( a , out ) return out expand_ ( a , out ) \u00b6 Broadcast a tensor to an existing tensor, following the broadcasting rules Parameters: a ( VarRef ) \u2013 The input tensor b ( VarRef ) \u2013 The broadcasted tensor Source code in freetensor/libop/reshape.py @core . inline def expand_ ( a , out ): ''' Broadcast a tensor to an existing tensor, following the broadcasting rules Parameters ---------- a : VarRef The input tensor b : VarRef The broadcasted tensor ''' if out . ndim == 0 : out [()] = a else : #! label: L_elem for i in range ( out . shape ( 0 )): if core . ndim ( a ) < out . ndim : #! label: recur expand_ ( a , out [ i ]) else : #! label: recur expand_ ( a [ i % a . shape ( 0 )], out [ i ]) flatten ( x , axis = 1 ) \u00b6 Flatten a tensor to have fewer dimensions, and return the result Parameters: x ( VarRef ) \u2013 The input tensor axis ( int (Optional) ) \u2013 The result tensor will have up to axis dimensions. All dimensions after axis will be flatten to 1-D. Negative axis means counting form the last dimension Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/reshape.py @core . inline def flatten ( x , axis = 1 ): ''' Flatten a tensor to have fewer dimensions, and return the result Parameters ---------- x : VarRef The input tensor axis : int (Optional) The result tensor will have up to `axis` dimensions. All dimensions after `axis` will be flatten to 1-D. Negative axis means counting form the last dimension Returns ------- VarRef The result tensor ''' y = core . empty ( _flatten_comp_shape ( x , axis ), core . dtype ( x ), core . mtype ( x )) #! label: recur flatten_ ( x , y , axis ) return y flatten_ ( x , y , axis = 1 ) \u00b6 Flatten a tensor to have fewer dimensions, and write to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axis ( int ) \u2013 The result tensor will have up to axis dimensions. All dimensions after axis will be flatten to 1-D. Negative axis means counting form the last dimension Source code in freetensor/libop/reshape.py @core . inline def flatten_ ( x , y , axis : int = 1 ): ''' Flatten a tensor to have fewer dimensions, and write to another tensor Parameters ---------- x : VarRef The input tensor y : VarRef The result tensor axis : int (Optional) The result tensor will have up to `axis` dimensions. All dimensions after `axis` will be flatten to 1-D. Negative axis means counting form the last dimension ''' if axis == 0 : #! label: recur _flatten_inner_ ( x , y [ 0 ]) else : #! label: L_outer for i in range ( x . shape ( 0 )): #! label: recur flatten_ ( x [ i ], y [ i * ( y . shape ( 0 ) // x . shape ( 0 )):( i + 1 ) * ( y . shape ( 0 ) // x . shape ( 0 ))], axis - 1 ) reshape ( x , shape ) \u00b6 Reshape a tensor into a different shape with the same size This operator will try to generate nested loops instead of looping over all elements in a plain loop, so schedules can be better applied. It guarantees to generates loops in the following cases: Splitting a dimension. E.g. 4 to 2x2, and there will be a 2x2 loop nest. Merging dimensions. E.g. 2x2 to 4, and there will be a 2x2 loop nest. Each non-affecting dimension will be iterated by a unique loop. E.g. 3x5x7 to 5x3x7, and there will be a 15x7 loop nest, where the \"7\" dimension will be iterated by a unique loop. Parameters: x ( VarRef ) \u2013 The input tensor shape ( list of expression ) \u2013 The target shape Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/reshape.py @core . inline def reshape ( x , shape ): ''' Reshape a tensor into a different shape with the same size This operator will try to generate nested loops instead of looping over all elements in a plain loop, so schedules can be better applied. It guarantees to generates loops in the following cases: 1. Splitting a dimension. E.g. 4 to 2x2, and there will be a 2x2 loop nest. 2. Merging dimensions. E.g. 2x2 to 4, and there will be a 2x2 loop nest. 3. Each non-affecting dimension will be iterated by a unique loop. E.g. 3x5x7 to 5x3x7, and there will be a 15x7 loop nest, where the \"7\" dimension will be iterated by a unique loop. Parameters ---------- x : VarRef The input tensor shape : list of expression The target shape Returns ------- VarRef The result tensor ''' y = core . empty ( shape , core . dtype ( x ), core . mtype ( x )) reshape_ ( x , y ) return y reshape_ ( x , y ) \u00b6 Fill a tensor into another tensor with the same size but maybe different shape This operator will try to generate nested loops instead of looping over all elements in a plain loop, so schedules can be better applied. It guarantees to generates loops in the following cases: Splitting a dimension. E.g. 4 to 2x2, and there will be a 2x2 loop nest. Merging dimensions. E.g. 2x2 to 4, and there will be a 2x2 loop nest. Each non-affecting dimension will be iterated by a unique loop. E.g. 3x5x7 to 5x3x7, and there will be a 15x7 loop nest, where the \"7\" dimension will be iterated by a unique loop. Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/reshape.py @core . inline def reshape_ ( x , y ): ''' Fill a tensor into another tensor with the same size but maybe different shape This operator will try to generate nested loops instead of looping over all elements in a plain loop, so schedules can be better applied. It guarantees to generates loops in the following cases: 1. Splitting a dimension. E.g. 4 to 2x2, and there will be a 2x2 loop nest. 2. Merging dimensions. E.g. 2x2 to 4, and there will be a 2x2 loop nest. 3. Each non-affecting dimension will be iterated by a unique loop. E.g. 3x5x7 to 5x3x7, and there will be a 15x7 loop nest, where the \"7\" dimension will be iterated by a unique loop. Parameters ---------- x : VarRef The input tensor y : VarRef The result tensor ''' if core . ndim ( x ) == 0 and core . ndim ( y ) == 0 : y [ ... ] = x [ ... ] elif core . ndim ( x ) > 0 and core . ndim ( y ) == 0 : assert x . shape ( 0 ) == 1 reshape_ ( x [ 0 ], y ) elif core . ndim ( y ) > 0 and core . ndim ( x ) == 0 : assert y . shape ( 0 ) == 1 reshape_ ( x , y [ 0 ]) else : factor_x0 = _factorize ( x . shape ( 0 )) factor_y0 = _factorize ( y . shape ( 0 )) x0_divisible_y0 = _factor_pairs_divisible ( factor_x0 , factor_y0 ) y0_divisible_x0 = _factor_pairs_divisible ( factor_y0 , factor_x0 ) if x0_divisible_y0 and y0_divisible_x0 : # Identical dimension assert x . shape ( 0 ) == y . shape ( 0 ) for i in range ( x . shape ( 0 )): reshape_ ( x [ i ], y [ i ]) elif x0_divisible_y0 : # Splitting a dimension. Iterating y assert x . shape ( 0 ) % y . shape ( 0 ) == 0 x_chunk_len = x . shape ( 0 ) // y . shape ( 0 ) for i in range ( y . shape ( 0 )): # Construct the slice with `length` here to make the next dimension simple reshape_ ( x [ core . ffi . FrontendVarIdx ( i * x_chunk_len , None , x_chunk_len )], y [ i ]) elif y0_divisible_x0 : # Merging dimensions. Iterating x assert y . shape ( 0 ) % x . shape ( 0 ) == 0 y_chunk_len = y . shape ( 0 ) // x . shape ( 0 ) for i in range ( x . shape ( 0 )): # Construct the slice with `length` here to make the next dimension simple reshape_ ( x [ i ], y [ core . ffi . FrontendVarIdx ( i * y_chunk_len , None , y_chunk_len )]) else : # Find next non-affecting dimension, and use one loop to reshape all # affecting dimensions before it factor_x = ( 1 , {}) factor_y = ( 1 , {}) l = 0 r = 0 while l < core . ndim ( x ): factor_x = _factor_pairs_mul ( factor_x , _factorize ( x . shape ( l ))) l += 1 while r < core . ndim ( y ): factor_y_new = _factor_pairs_mul ( factor_y , _factorize ( y . shape ( r ))) if _factor_pairs_divisible ( factor_x , factor_y_new ): factor_y = factor_y_new r += 1 else : break if _factor_pairs_divisible ( factor_y , factor_x ): break if not _factor_pairs_divisible ( factor_y , factor_x ): r = core . ndim ( y ) x_lengths = [ 1 ] * ( l + 1 ) y_lengths = [ 1 ] * ( r + 1 ) for k in core . static_range ( l - 1 , - 1 , - 1 ): x_lengths [ k ] = x . shape ( k ) * x_lengths [ k + 1 ] for k in core . static_range ( r - 1 , - 1 , - 1 ): y_lengths [ k ] = y . shape ( k ) * y_lengths [ k + 1 ] assert x_lengths [ 0 ] == y_lengths [ 0 ] for i in range ( x_lengths [ 0 ]): x_next , y_next = x , y for k in core . static_range ( l ): x_next = x_next [ i // x_lengths [ k + 1 ] % x . shape ( k )] for k in core . static_range ( r ): y_next = y_next [ i // y_lengths [ k + 1 ] % y . shape ( k )] reshape_ ( x_next , y_next ) unsqueeze ( x , axes ) \u00b6 Insert singleton dimensions to a tensor, and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] ) \u2013 Dimension numbers of the new singleton dimensions. Negative axis means counting from the last dimension Returns: VarRef \u2013 The resulting tensor Source code in freetensor/libop/reshape.py @core . inline def unsqueeze ( x , axes : Sequence [ int ]): ''' Insert singleton dimensions to a tensor, and return the result Parameters ---------- x : VarRef The input tensor axes : Dimension numbers of the new singleton dimensions. Negative axis means counting from the last dimension Returns ------- VarRef The resulting tensor ''' y = core . empty ( _unsqueeze_comp_shape ( _circular_axes ( axes , core . ndim ( x )), x ), core . dtype ( x ), core . mtype ( x )) #! label: recur unsqueeze_ ( x , y , axes ) return y unsqueeze_ ( x , y , axes ) \u00b6 Insert singleton dimensions to a tensor, and write the result to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The resulting tensor axes ( Sequence[int] ) \u2013 Dimension numbers of the new singleton dimensions. Negative axis means counting from the last dimension Source code in freetensor/libop/reshape.py @core . inline def unsqueeze_ ( x , y , axes : Sequence [ int ]): ''' Insert singleton dimensions to a tensor, and write the result to another tensor Parameters ---------- x : VarRef The input tensor y : VarRef The resulting tensor axes : Dimension numbers of the new singleton dimensions. Negative axis means counting from the last dimension ''' axes = _circular_axes ( axes , core . ndim ( x )) if y . ndim == 0 : y [()] = x elif begin_with_0 ( axes ): #! label: recur unsqueeze_ ( x , y [ 0 ], all_minus_one ( axes [ 1 :])) else : #! label: L for i in range ( x . shape ( 0 )): #! label: recur unsqueeze_ ( x [ i ], y [ i ], all_minus_one ( axes )) softmax \u00b6 softmax ( x , axis =- 1 ) \u00b6 Softmax of tensor x along an axis and return the result Parameters: x ( VarRef ) \u2013 The input tensor axis ( int (Optional) ) \u2013 Axis that the softmax is performed along. Negative axis means count from the last dimension Returns: The result tensor Source code in freetensor/libop/softmax.py @core . inline def softmax ( x , axis =- 1 ): ''' Softmax of tensor `x` along an axis and return the result Parameters ---------- x : VarRef The input tensor axis : int (Optional) Axis that the softmax is performed along. Negative axis means count from the last dimension Returns ------- VarRef : The result tensor ''' #! label: max maxval = reduce_max ( x , axes = [ axis ], keepdims = True ) #! label: sub corrected = sub ( x , maxval ) #! label: exp exponent = exp ( corrected ) #! label: sum summation = reduce_sum ( exponent , axes = [ axis ], keepdims = True ) #! label: div out = truediv ( exponent , summation ) return out softmax_ ( x , y , axis =- 1 ) \u00b6 Softmax of tensor x along an axis, and write to tensor y Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axis ( int ) \u2013 Axis that the softmax is performed along. Negative axis means count from the last dimension Source code in freetensor/libop/softmax.py @core . inline def softmax_ ( x , y , axis : int = - 1 ): ''' Softmax of tensor `x` along an axis, and write to tensor `y` Parameters ---------- x : VarRef The input tensor y : VarRef The result tensor axis : int (Optional) Axis that the softmax is performed along. Negative axis means count from the last dimension ''' #! label: max maxval = reduce_max ( x , axes = [ axis ], keepdims = True ) #! label: sub corrected = sub ( x , maxval ) #! label: exp exponent = exp ( corrected ) #! label: sum summation = reduce_sum ( exponent , axes = [ axis ], keepdims = True ) #! label: div truediv_ ( exponent , summation , y )","title":"Python API"},{"location":"api/#python-api","text":"","title":"Python API"},{"location":"api/#freetensor.core","text":"","title":"core"},{"location":"api/#freetensor.core.auto_schedule","text":"","title":"auto_schedule"},{"location":"api/#freetensor.core.auto_schedule.AutoSchedule","text":"Source code in freetensor/core/auto_schedule.py class AutoSchedule ( ffi . AutoSchedule ): def __init__ ( self , schedule , target , device , * , population = 64 , explore_ratio = 0.1 , tag = \"\" , min_block_size = 0 , continue_training = False , random_seed = None , rule_set = None , verbose = 0 ): ''' Automatic scheduler Parameters ---------- schedule : Schedule A Schedule object to apply schedules onto target : Target The type of devices to compile to population : int How many programs to test in each iteration explore_ratio : float Portion of random programs in the population. Higher ratio focuses on exploration, while lower ratio focuses on exploitation continue_trianing : bool Continue to train an existing XGBoost model file if found random_seed : Optional[int] Random seed. Setting a deterministic random seed and using a fixed OpenMP thread count (since we are using thread-local random number generators) resulting deterministic pseudo random numbers, but please note that the whole auto-scheduling procedure is still non-deterministic, becuase it measures real performance. Default to a non-deterministic seed rule_set : Optional[set] Explicitly control over what rules to use. None for defualt rules verbose : int Verbosity level. 0 = print nothing, 1 = print tuning progress, 2 = print extra info mation of each rule ''' self . population = population self . n_explore = int ( population * explore_ratio ) self . n_exploit = population - self . n_explore self . model = None self . xgb_params = {} self . save_file_name = tag + \"_xgb.model\" if continue_training and os . path . isfile ( self . save_file_name ): self . model = xgb . Booster () self . model . load_model ( self . save_file_name ) self . verbose = verbose def predict_func ( features ): return self . predict ( features ) def update_func ( features , times ): return self . update ( features , times ) super ( AutoSchedule , self ) . __init__ ( schedule , target , device , predict_func , update_func , tag , min_block_size , random_seed , rule_set , verbose ) def set_params ( self , * args , ** kws ): super ( AutoSchedule , self ) . set_params ( args , kws ) def run ( self , iteration ): for i in range ( iteration ): if self . verbose >= 1 : print ( \"Iteration\" , i ) self . search_one_round ( self . population , self . n_exploit , self . n_explore ) return self . get_best_schedule () def predict ( self , features ): if not self . model : return [ 1 ] * len ( features ) return self . model . predict ( xgb . DMatrix ( np . array ( features ), missing =- 1 )) def update ( self , features , times ): dtrain = xgb . DMatrix ( np . array ( features ), np . array ( times ), missing =- 1 ) self . model = xgb . train ( self . xgb_params , dtrain , xgb_model = self . model ) self . model . save_model ( self . save_file_name )","title":"AutoSchedule"},{"location":"api/#freetensor.core.auto_schedule.AutoSchedule.__init__","text":"Automatic scheduler Parameters: schedule ( Schedule ) \u2013 A Schedule object to apply schedules onto target ( Target ) \u2013 The type of devices to compile to population ( int ) \u2013 How many programs to test in each iteration explore_ratio ( float ) \u2013 Portion of random programs in the population. Higher ratio focuses on exploration, while lower ratio focuses on exploitation continue_trianing ( bool ) \u2013 Continue to train an existing XGBoost model file if found random_seed ( Optional[int] ) \u2013 Random seed. Setting a deterministic random seed and using a fixed OpenMP thread count (since we are using thread-local random number generators) resulting deterministic pseudo random numbers, but please note that the whole auto-scheduling procedure is still non-deterministic, becuase it measures real performance. Default to a non-deterministic seed rule_set ( Optional[set] ) \u2013 Explicitly control over what rules to use. None for defualt rules verbose ( int ) \u2013 Verbosity level. 0 = print nothing, 1 = print tuning progress, 2 = print extra info mation of each rule Source code in freetensor/core/auto_schedule.py def __init__ ( self , schedule , target , device , * , population = 64 , explore_ratio = 0.1 , tag = \"\" , min_block_size = 0 , continue_training = False , random_seed = None , rule_set = None , verbose = 0 ): ''' Automatic scheduler Parameters ---------- schedule : Schedule A Schedule object to apply schedules onto target : Target The type of devices to compile to population : int How many programs to test in each iteration explore_ratio : float Portion of random programs in the population. Higher ratio focuses on exploration, while lower ratio focuses on exploitation continue_trianing : bool Continue to train an existing XGBoost model file if found random_seed : Optional[int] Random seed. Setting a deterministic random seed and using a fixed OpenMP thread count (since we are using thread-local random number generators) resulting deterministic pseudo random numbers, but please note that the whole auto-scheduling procedure is still non-deterministic, becuase it measures real performance. Default to a non-deterministic seed rule_set : Optional[set] Explicitly control over what rules to use. None for defualt rules verbose : int Verbosity level. 0 = print nothing, 1 = print tuning progress, 2 = print extra info mation of each rule ''' self . population = population self . n_explore = int ( population * explore_ratio ) self . n_exploit = population - self . n_explore self . model = None self . xgb_params = {} self . save_file_name = tag + \"_xgb.model\" if continue_training and os . path . isfile ( self . save_file_name ): self . model = xgb . Booster () self . model . load_model ( self . save_file_name ) self . verbose = verbose def predict_func ( features ): return self . predict ( features ) def update_func ( features , times ): return self . update ( features , times ) super ( AutoSchedule , self ) . __init__ ( schedule , target , device , predict_func , update_func , tag , min_block_size , random_seed , rule_set , verbose )","title":"__init__()"},{"location":"api/#freetensor.core.auto_schedule.AutoSchedule.set_params","text":"set_params(self: freetensor_ffi.AutoSchedule, args: List[freetensor_ffi.Array], kws: Dict[str, freetensor_ffi.Array] = {}) -> None Source code in freetensor/core/auto_schedule.py def set_params ( self , * args , ** kws ): super ( AutoSchedule , self ) . set_params ( args , kws )","title":"set_params()"},{"location":"api/#freetensor.core.autograd","text":"","title":"autograd"},{"location":"api/#freetensor.core.autograd.ArgRetDict","text":"Look an object using either a function argument or return value's name or its position Source code in freetensor/core/autograd.py class ArgRetDict : ''' Look an object using either a function argument or return value's name or its position ''' def __init__ ( self , func , d ): self . func = func self . d = d def __getitem__ ( self , key ): if type ( key ) is Return : key = key . get_name ( self . func ) return self . d [ key ] def __contains__ ( self , key ): # Python's auto fallback from __getitem__ to __contains__ only works for # integer index if type ( key ) is Return : key = key . get_name ( self . func ) return key in self . d def __str__ ( self ): return str ( self . d )","title":"ArgRetDict"},{"location":"api/#freetensor.core.autograd.Return","text":"Alias of a return value of a function Return(n) represents the n-th return value (counted from 0) Return() can be used if there is only one return value Source code in freetensor/core/autograd.py class Return : ''' Alias of a return value of a function `Return(n)` represents the n-th return value (counted from 0) `Return()` can be used if there is only one return value ''' def __init__ ( self , n : Optional [ int ] = None ): self . n = n def get_name ( self , func ): assert len ( func . returns ) > 0 , f \" { func . name } has no return value\" if self . n is not None : return func . returns [ self . n ] . name else : assert len ( func . returns ) == 1 , f \" { func . name } has more than one return value, and you need to specify the number of a return value\" return func . returns [ 0 ] . name def __str__ ( self ): return f \"Return( { self . n } )\"","title":"Return"},{"location":"api/#freetensor.core.autograd.grad","text":"Reverse mode automatic differentiation It returns a forward function and a backward function. The forward has the same interface of the original function, but it will store some intermediate tensors (the tape) to be reused by the backward function in some global states. The backward function computes the gradients. grad is an out-of-place version. The resulting gradient are returned from the backward function. Parameters: func ( Func ) \u2013 The original function requires ( Sequence[str] ) \u2013 Name of input variables that need gradients provides ( Sequence[Union[str, freetensor.core.autograd.Return]] ) \u2013 Name of output variables whose gradients are known. A return value of a function can be specified with a Return object tapes ( Union[Sequence, freetensor_ffi.GradTapeMode] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure ( bool ) \u2013 True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True Returns: tuple \u2013 ( 0. Forward AST. 1. Backward AST. 2. Mapping from names in requries to its gradient name. 3. Mapping from names in provides to its gradient name. ) Source code in freetensor/core/autograd.py def grad ( func : ffi . Func , requires : Sequence [ str ], provides : Sequence [ Union [ str , Return ]], tapes : Union [ Sequence , GradTapeMode ] = GradTapeMode . NoReuseOnly , tape_in_closure : bool = True , verbose : Optional [ int ] = None ): ''' Reverse mode automatic differentiation It returns a forward function and a backward function. The forward has the same interface of the original function, but it will store some intermediate tensors (the tape) to be reused by the backward function in some global states. The backward function computes the gradients. `grad` is an out-of-place version. The resulting gradient are returned from the backward function. Parameters ---------- func : AST The original function requires : Sequence[str] Name of input variables that need gradients provides : Sequence[Union[str, Return]] Name of output variables whose gradients are known. A return value of a function can be specified with a `Return` object tapes : Union[Sequence, GradTapeMode] Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a `GradTapeMode`, then it will determine which intermediate variables to be stored by heuristics. Avail `GradTapeMode`s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure : bool True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True Returns ------- tuple ( 0. Forward AST. 1. Backward AST. 2. Mapping from names in requries to its gradient name. 3. Mapping from names in provides to its gradient name. ) ''' return _grad_func ( ffi . grad , func , requires , provides , tapes , tape_in_closure , verbose = verbose )","title":"grad()"},{"location":"api/#freetensor.core.autograd.grad_","text":"Reverse mode automatic differentiation It returns a forward function and a backward function. The forward has the same interface of the original function, but it will store some intermediate tensors (the tape) to be reused by the backward function in some global states. The backward function computes the gradients. grad_ is an inplace version. The resulting gradient are mutable arguments of the backward function. Parameters: func ( Func ) \u2013 The original function requires ( Sequence[str] ) \u2013 Name of input variables that need gradients provides ( Sequence[Union[str, freetensor.core.autograd.Return]] ) \u2013 Name of output variables whose gradients are known. A return value of a function can be specified with a Return object tapes ( Union[Sequence, freetensor_ffi.GradTapeMode] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure ( bool ) \u2013 True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True Returns: tuple \u2013 ( 0. Forward AST. 1. Backward AST. 2. Mapping from names in requries to its gradient name. 3. Mapping from names in provides to its gradient name. ) Source code in freetensor/core/autograd.py def grad_ ( func : ffi . Func , requires : Sequence [ str ], provides : Sequence [ Union [ str , Return ]], tapes : Union [ Sequence , GradTapeMode ] = GradTapeMode . NoReuseOnly , tape_in_closure : bool = True , verbose : Optional [ int ] = None ): ''' Reverse mode automatic differentiation It returns a forward function and a backward function. The forward has the same interface of the original function, but it will store some intermediate tensors (the tape) to be reused by the backward function in some global states. The backward function computes the gradients. `grad_` is an inplace version. The resulting gradient are mutable arguments of the backward function. Parameters ---------- func : AST The original function requires : Sequence[str] Name of input variables that need gradients provides : Sequence[Union[str, Return]] Name of output variables whose gradients are known. A return value of a function can be specified with a `Return` object tapes : Union[Sequence, GradTapeMode] Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a `GradTapeMode`, then it will determine which intermediate variables to be stored by heuristics. Avail `GradTapeMode`s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure : bool True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True Returns ------- tuple ( 0. Forward AST. 1. Backward AST. 2. Mapping from names in requries to its gradient name. 3. Mapping from names in provides to its gradient name. ) ''' return _grad_func ( ffi . grad_ , func , requires , provides , tapes , tape_in_closure , verbose = verbose )","title":"grad_()"},{"location":"api/#freetensor.core.autograd.grad_body","text":"grad or grad_ on a function body (for internal tests only) Source code in freetensor/core/autograd.py def grad_body ( stmt : ffi . Stmt , requires : Sequence [ Union [ str , Return ]], provides : Sequence [ Union [ str , Return ]], tapes : Union [ Sequence , GradTapeMode ] = GradTapeMode . NoReuseOnly ): ''' `grad` or `grad_` on a function body (for internal tests only) ''' req = set ( requires ) prov = set ( provides ) if type ( tapes ) is not GradTapeMode : tapes = { find_stmt ( stmt , t ) . id for t in tapes } return ffi . grad_body ( stmt , req , prov , tapes )","title":"grad_body()"},{"location":"api/#freetensor.core.codegen","text":"","title":"codegen"},{"location":"api/#freetensor.core.codegen.codegen","text":"Generate native code Parameters: ast ( AST ) \u2013 The AST to be lowered. It must includes function signature to determine parameters and return values. If not specified, a partial function is returned, which can be used as a decorator target ( Optional[freetensor_ffi.Target] ) \u2013 The target architecture. If omitted, use the default one in config Source code in freetensor/core/codegen.py def codegen ( ast = None , target : Optional [ ffi . Target ] = None , verbose : Optional [ bool ] = None ) -> NativeCode : ''' Generate native code Parameters ---------- ast : AST The AST to be lowered. It must includes function signature to determine parameters and return values. If not specified, a partial function is returned, which can be used as a decorator target : Target (Optional) The target architecture. If omitted, use the default one in config ''' if ast is not None : if target is None : target = config . default_target () raw_code = ffi . code_gen ( ast , target ) if verbose : print ( debug . with_line_no ( raw_code ), file = sys . stderr ) return NativeCode ( ast , raw_code , target ) else : f = codegen if target is not None : f = functools . partial ( f , target = target ) if verbose is not None : f = functools . partial ( f , verbose = verbose ) return f","title":"codegen()"},{"location":"api/#freetensor.core.config","text":"Global configurations","title":"config"},{"location":"api/#freetensor.core.config.backend_compiler_cxx","text":"backend_compiler_cxx() -> List[str] Backend compiler used to compile generated C++ code Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"backend_compiler_cxx()"},{"location":"api/#freetensor.core.config.backend_compiler_nvcc","text":"backend_compiler_nvcc() -> List[str] Backend compiler used to compile generated CUDA code Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"backend_compiler_nvcc()"},{"location":"api/#freetensor.core.config.debug_binary","text":"debug_binary() -> bool Check if compiling binary in debug mode Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"debug_binary()"},{"location":"api/#freetensor.core.config.default_device","text":"default_device() -> freetensor_ffi.Device Check current default device Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"default_device()"},{"location":"api/#freetensor.core.config.default_target","text":"default_target() -> freetensor_ffi.Target Check current default target Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"default_target()"},{"location":"api/#freetensor.core.config.pretty_print","text":"pretty_print() -> bool Check if colored printing enabled Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"pretty_print()"},{"location":"api/#freetensor.core.config.print_all_id","text":"pretty_print() -> bool Check if colored printing enabled Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"print_all_id()"},{"location":"api/#freetensor.core.config.set_backend_compiler_cxx","text":"set_backend_compiler_cxx(path: List[str]) -> None Set backend compiler used to compile generated C++ code, unescaped raw path expected Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"set_backend_compiler_cxx()"},{"location":"api/#freetensor.core.config.set_backend_compiler_nvcc","text":"set_backend_compiler_nvcc(path: List[str]) -> None Set backend compiler used to compile generated CUDA code, unescaped raw path expected Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"set_backend_compiler_nvcc()"},{"location":"api/#freetensor.core.config.set_debug_binary","text":"set_debug_binary(flag: bool = True) -> None Compile with -g at backend. Do not delete the binary file after loaded Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"set_debug_binary()"},{"location":"api/#freetensor.core.config.set_default_device","text":"set_default_device(device: freetensor_ffi.Device) -> None Set default device (internal implementation of with Device ) Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"set_default_device()"},{"location":"api/#freetensor.core.config.set_default_target","text":"set_default_target(target: freetensor_ffi.Target) -> None Set default target (internal implementation of with Target ) Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"set_default_target()"},{"location":"api/#freetensor.core.config.set_pretty_print","text":"set_pretty_print(flag: bool = True) -> None Set colored printing Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"set_pretty_print()"},{"location":"api/#freetensor.core.config.set_print_all_id","text":"set_pretty_print(flag: bool = True) -> None Set colored printing Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"set_print_all_id()"},{"location":"api/#freetensor.core.config.set_werror","text":"set_werror(flag: bool = True) -> None Error on warning Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"set_werror()"},{"location":"api/#freetensor.core.config.werror","text":"werror() -> bool Check if error-on-warning enabled Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"werror()"},{"location":"api/#freetensor.core.config.with_cuda","text":"with_cuda() -> bool Check if FreeTensor is built with CUDA Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"with_cuda()"},{"location":"api/#freetensor.core.config.with_mkl","text":"with_mkl() -> str Check if FreeTensor is built with MKL Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"with_mkl()"},{"location":"api/#freetensor.core.config.with_pytorch","text":"with_pytorch() -> bool Check if FreeTensor is built with PyTorch interface Source code in freetensor/core/config.py def g ( * args , ** kvs ): return f ( * args , ** kvs )","title":"with_pytorch()"},{"location":"api/#freetensor.core.context","text":"Facility to pick statements to build an AST Classes and functions in this module are internally used by transformer to construct ASTs. They are also used by some internal tests. API of these classes and functions are subject to changes. End users are encouraged to use transformer , instead of this module.","title":"context"},{"location":"api/#freetensor.core.context.pop_ast","text":"Get AST and reset context Internally used by transformer and tests Source code in freetensor/core/context.py def pop_ast ( verbose : bool = False ): \"\"\" Get AST and reset context Internally used by `transformer` and tests \"\"\" ret = ctx_stack . pop () . make_stmt () ctx_stack . reset () if verbose : print ( \"The popped AST is:\" , file = sys . stderr ) print ( ret , file = sys . stderr ) print ( file = sys . stderr ) return ret","title":"pop_ast()"},{"location":"api/#freetensor.core.driver","text":"","title":"driver"},{"location":"api/#freetensor.core.driver.Device","text":"A computing device can be constructed from (TargetType, DeviceNumber) (TargetType, getDeviceByName): cuda uses best matches criteria. (TargetType, FullName, nth): get nth(from 0) device named Fullname . E.g. Device(TargetType::GPU, 0) means the 0-th GPU (device) Device(TargetType::GPU, \"V100\") means a GPU which best matches \"V100\" Device(TargetType::GPU, \"NVIDIA GeForce RTX 3060 Laptop GPU\", 0) A Device can be used as a \"with\" scope, then all the Array s and Driver s will use it by default. In this style, it also sets the default Target. E.g: with Device(...): ast = lower(ast) # Use the Target of the Device above by default a = Array(...) # Use the Device above by default Source code in freetensor/core/driver.py class Device ( ffi . Device ): ''' A computing device can be constructed from 1. (TargetType, DeviceNumber) 2. (TargetType, getDeviceByName): cuda uses best matches criteria. 3. (TargetType, FullName, nth): get nth(from 0) device named `Fullname`. E.g. Device(TargetType::GPU, 0) means the 0-th GPU (device) Device(TargetType::GPU, \"V100\") means a GPU which best matches \"V100\" Device(TargetType::GPU, \"NVIDIA GeForce RTX 3060 Laptop GPU\", 0) A Device can be used as a \"with\" scope, then all the `Array`s and `Driver`s will use it by default. In this style, it also sets the default Target. E.g: ``` with Device(...): ast = lower(ast) # Use the Target of the Device above by default a = Array(...) # Use the Device above by default ``` ''' def __enter__ ( self ): _old_target_device_stack . append ( ( config . default_target (), config . default_device ())) config . set_default_target ( self . target ()) config . set_default_device ( self ) return self def __exit__ ( self , exc_type , exc_value , traceback ): old_target , old_device = _old_target_device_stack . pop () config . set_default_target ( old_target ) config . set_default_device ( old_device )","title":"Device"},{"location":"api/#freetensor.core.driver.Driver","text":"Source code in freetensor/core/driver.py class Driver ( ffi . Driver ): def __init__ ( self , func : ffi . Func , src : str , device : Optional [ Device ] = None , host_device : Optional [ Device ] = None , verbose : Optional [ bool ] = None ): ''' Compile a program using a backend compiler and load it into memory This class is for internal use. Please consider using `build_binary` Parameters ---------- func : ffi.Func AST of the function, where the function signature is needed to determine the parameters and return values src : str Native code generated from codegen device : Device (Optional) The device to run the program. If omitted, use the default device in config verbose : bool (Optional) True to print extra infomation ''' src = str ( src ) if device is None : device = config . default_device () if verbose is None : verbose = False if host_device is None : super ( Driver , self ) . __init__ ( func , src , device , verbose ) else : super ( Driver , self ) . __init__ ( func , src , device , host_device , verbose ) self . func = func # When we pass numpy or pytorch tensors to `set_args`, they are # converted to `Array` objects by reference. In `Array`'s FFI, we # keep these tensors alive whenever the `Array`'s PYTHON objects # alive. We need to also keep the `Array`'s PYTHON objects here. # Please note that we cannot hold the reference count in `Driver`'s # C++ implementation, where we can only hold the `Array`'s C++ # objects alive. self . args_ref_cnt_holder = [] def set_args ( self , * args , ** kws ): ''' Set argument for an invocation ''' # No need to hold reference of the last run any more self . args_ref_cnt_holder = [] args = list ( args ) kws = dict ( kws ) for i in range ( len ( args )): args [ i ] = array ( args [ i ]) for key in kws : kws [ key ] = array ( kws [ key ]) for arg in args : self . args_ref_cnt_holder . append ( arg ) for key in kws : self . args_ref_cnt_holder . append ( kws [ key ]) super ( Driver , self ) . set_args ( args , kws ) def collect_returns ( self , always_return_pack : bool = False ): ''' Collect return values from an invocation Return values must be collect. Otherwise there will be memory leaks If there is only one return value, it is returned directly. Otherwise, or if `always_return_pack` is set, the return values are packed in a ReturnValuesPack ''' values = super ( Driver , self ) . collect_returns () if len ( values ) == 0 and not always_return_pack : return None elif len ( values ) == 1 and not always_return_pack : return values [ 0 ] else : return ReturnValuesPack ( map ( lambda r : r . name , filter ( lambda r : not r . is_in_closure or r . return_closure , self . func . returns )), values ) def __call__ ( self , * args , ** kws ): ''' Set argument, execute the binary code, and collect the returns If there is only one return value, it is returned directly. Otherwise, the return values are packed in a ReturnValuesPack This function will introduce some overhaed handling arguments and return values. For an accurate execution time measurement, plase call `self.set_args` first, then `self.time`, and finally `self.collect_returns` ''' self . set_args ( * args , ** kws ) self . run () return self . collect_returns ()","title":"Driver"},{"location":"api/#freetensor.core.driver.Driver.__call__","text":"Set argument, execute the binary code, and collect the returns If there is only one return value, it is returned directly. Otherwise, the return values are packed in a ReturnValuesPack This function will introduce some overhaed handling arguments and return values. For an accurate execution time measurement, plase call self.set_args first, then self.time , and finally self.collect_returns Source code in freetensor/core/driver.py def __call__ ( self , * args , ** kws ): ''' Set argument, execute the binary code, and collect the returns If there is only one return value, it is returned directly. Otherwise, the return values are packed in a ReturnValuesPack This function will introduce some overhaed handling arguments and return values. For an accurate execution time measurement, plase call `self.set_args` first, then `self.time`, and finally `self.collect_returns` ''' self . set_args ( * args , ** kws ) self . run () return self . collect_returns ()","title":"__call__()"},{"location":"api/#freetensor.core.driver.Driver.__init__","text":"Compile a program using a backend compiler and load it into memory This class is for internal use. Please consider using build_binary Parameters: func ( Func ) \u2013 AST of the function, where the function signature is needed to determine the parameters and return values src ( str ) \u2013 Native code generated from codegen device ( Optional[freetensor.core.driver.Device] ) \u2013 The device to run the program. If omitted, use the default device in config verbose ( Optional[bool] ) \u2013 True to print extra infomation Source code in freetensor/core/driver.py def __init__ ( self , func : ffi . Func , src : str , device : Optional [ Device ] = None , host_device : Optional [ Device ] = None , verbose : Optional [ bool ] = None ): ''' Compile a program using a backend compiler and load it into memory This class is for internal use. Please consider using `build_binary` Parameters ---------- func : ffi.Func AST of the function, where the function signature is needed to determine the parameters and return values src : str Native code generated from codegen device : Device (Optional) The device to run the program. If omitted, use the default device in config verbose : bool (Optional) True to print extra infomation ''' src = str ( src ) if device is None : device = config . default_device () if verbose is None : verbose = False if host_device is None : super ( Driver , self ) . __init__ ( func , src , device , verbose ) else : super ( Driver , self ) . __init__ ( func , src , device , host_device , verbose ) self . func = func # When we pass numpy or pytorch tensors to `set_args`, they are # converted to `Array` objects by reference. In `Array`'s FFI, we # keep these tensors alive whenever the `Array`'s PYTHON objects # alive. We need to also keep the `Array`'s PYTHON objects here. # Please note that we cannot hold the reference count in `Driver`'s # C++ implementation, where we can only hold the `Array`'s C++ # objects alive. self . args_ref_cnt_holder = []","title":"__init__()"},{"location":"api/#freetensor.core.driver.Driver.collect_returns","text":"Collect return values from an invocation Return values must be collect. Otherwise there will be memory leaks If there is only one return value, it is returned directly. Otherwise, or if always_return_pack is set, the return values are packed in a ReturnValuesPack Source code in freetensor/core/driver.py def collect_returns ( self , always_return_pack : bool = False ): ''' Collect return values from an invocation Return values must be collect. Otherwise there will be memory leaks If there is only one return value, it is returned directly. Otherwise, or if `always_return_pack` is set, the return values are packed in a ReturnValuesPack ''' values = super ( Driver , self ) . collect_returns () if len ( values ) == 0 and not always_return_pack : return None elif len ( values ) == 1 and not always_return_pack : return values [ 0 ] else : return ReturnValuesPack ( map ( lambda r : r . name , filter ( lambda r : not r . is_in_closure or r . return_closure , self . func . returns )), values )","title":"collect_returns()"},{"location":"api/#freetensor.core.driver.Driver.set_args","text":"Set argument for an invocation Source code in freetensor/core/driver.py def set_args ( self , * args , ** kws ): ''' Set argument for an invocation ''' # No need to hold reference of the last run any more self . args_ref_cnt_holder = [] args = list ( args ) kws = dict ( kws ) for i in range ( len ( args )): args [ i ] = array ( args [ i ]) for key in kws : kws [ key ] = array ( kws [ key ]) for arg in args : self . args_ref_cnt_holder . append ( arg ) for key in kws : self . args_ref_cnt_holder . append ( kws [ key ]) super ( Driver , self ) . set_args ( args , kws )","title":"set_args()"},{"location":"api/#freetensor.core.driver.ReturnValuesPack","text":"Hold return values from a Driver invocation Return values can be retrieved in an anonymous manner: x, y, z = pack , or in a named manner: pack['x'] Please note that a ReturnValuesPack is different from a OrderedDict, as OrderedDict unpacks to keys rather than values Source code in freetensor/core/driver.py class ReturnValuesPack : ''' Hold return values from a Driver invocation Return values can be retrieved in an anonymous manner: `x, y, z = pack`, or in a named manner: `pack['x']` Please note that a ReturnValuesPack is different from a OrderedDict, as OrderedDict unpacks to keys rather than values ''' def __init__ ( self , keys : Sequence [ str ], values : Sequence [ Array ]): keys = list ( keys ) values = list ( values ) assert len ( keys ) == len ( values ) self . keys = keys self . values = values def __iter__ ( self ): ''' Get all return values in the order declared in Func ''' yield from self . values def __getitem__ ( self , key ) -> Array : ''' Get a return value with a name. Tuple is supported for multiple values ''' if type ( key ) is tuple or type ( key ) is list : ret = [] for k in key : ret . append ( self [ k ]) return ret for k , v in zip ( self . keys , self . values ): if k == key : return v raise ffi . DriverError ( \"No such return value named \" + key ) def __contains__ ( self , key ): ''' Test if a return value exists ''' for k , v in zip ( self . keys , self . values ): if k == key : return True return False","title":"ReturnValuesPack"},{"location":"api/#freetensor.core.driver.ReturnValuesPack.__contains__","text":"Test if a return value exists Source code in freetensor/core/driver.py def __contains__ ( self , key ): ''' Test if a return value exists ''' for k , v in zip ( self . keys , self . values ): if k == key : return True return False","title":"__contains__()"},{"location":"api/#freetensor.core.driver.ReturnValuesPack.__getitem__","text":"Get a return value with a name. Tuple is supported for multiple values Source code in freetensor/core/driver.py def __getitem__ ( self , key ) -> Array : ''' Get a return value with a name. Tuple is supported for multiple values ''' if type ( key ) is tuple or type ( key ) is list : ret = [] for k in key : ret . append ( self [ k ]) return ret for k , v in zip ( self . keys , self . values ): if k == key : return v raise ffi . DriverError ( \"No such return value named \" + key )","title":"__getitem__()"},{"location":"api/#freetensor.core.driver.ReturnValuesPack.__iter__","text":"Get all return values in the order declared in Func Source code in freetensor/core/driver.py def __iter__ ( self ): ''' Get all return values in the order declared in Func ''' yield from self . values","title":"__iter__()"},{"location":"api/#freetensor.core.driver.array","text":"Factory function for Array It converts more data format to Array Source code in freetensor/core/driver.py def array ( data ): ''' Factory function for Array It converts more data format to Array ''' if type ( data ) is Array : return data # For NumPy, Although Pybind11's `array_t` type provides a flag `forcecast` to # cast from a strided array to a contiguous one. But it always casts to a specific # type, e.g. float64. I have no idea how to support multiple types. Therfore, # we have to call NumPy's `.copy(order='C')` to make a new NumPy array. This # function can only be called from Python side (not from PyBind11's `py::array` # type). if type ( data ) is np . ndarray : if not data . flags [ 'C_CONTIGUOUS' ]: data = data . copy ( order = 'C' ) return Array ( data ) if data . __class__ . __module__ == 'torch' : import torch if type ( data ) is torch . Tensor : if not config . with_pytorch (): raise ffi . DriverError ( \"FreeTensor should be built with WITH_PYTORCH to accept a PyTorch tensor\" ) if not data . is_contiguous (): data = data . contiguous () return Array ( data ) raise ffi . DriverError ( f \"Unsupported data type { type ( data ) } for Array\" )","title":"array()"},{"location":"api/#freetensor.core.driver.build_binary","text":"Compile a program using a backend compiler and load it into memory Parameters: code ( Optional[freetensor.core.codegen.NativeCode] ) \u2013 Native code generated by codegen . If not specified, a partial function is returned, which can be used as a decorator device ( Optional[freetensor.core.driver.Device] ) \u2013 The device to run the program. If omitted, use the default device in config Source code in freetensor/core/driver.py def build_binary ( code : Optional [ NativeCode ] = None , device : Optional [ Device ] = None , host_device : Optional [ Device ] = None , verbose : Optional [ bool ] = None ): ''' Compile a program using a backend compiler and load it into memory Parameters ---------- code : NativeCode Native code generated by `codegen`. If not specified, a partial function is returned, which can be used as a decorator device : Device (Optional) The device to run the program. If omitted, use the default device in config ''' if code is not None : if device is None : device = config . default_device () if device . target () != code . target : raise ffi . DriverError ( f \"Codegen target ( { code . target } ) is inconsistent with device target ( { device . target () } )\" ) return Driver ( code . func , code . code , device , host_device , verbose ) else : f = build_binary if device is not None : f = functools . partial ( f , device = device ) if host_device is not None : f = functools . partial ( f , host_device = host_device ) if verbose is not None : f = functools . partial ( f , verbose = verbose ) return f","title":"build_binary()"},{"location":"api/#freetensor.core.expr","text":"Facility to build AST expressions Classes and functions in this module are not only used internally for constructing AST nodes, and also exposed to users via multi-stage programming","title":"expr"},{"location":"api/#freetensor.core.expr.AlreadyMadeReduceTo","text":"A single-value type that marks a ReduceTo node is already made, and there is no need to make another Store node In standard Python data model, functions like iadd returns the modified self, and setitem does a self-assignment. We do the augmenting assignment directly in iadd and return AlreadyMadeReduceTo, so we do not have to Store it again Source code in freetensor/core/expr.py class AlreadyMadeReduceTo : \"\"\" A single-value type that marks a ReduceTo node is already made, and there is no need to make another Store node In standard Python data model, functions like __iadd__ returns the modified self, and __setitem__ does a self-assignment. We do the augmenting assignment directly in __iadd__ and return AlreadyMadeReduceTo, so we do not have to Store it again \"\"\" pass","title":"AlreadyMadeReduceTo"},{"location":"api/#freetensor.core.expr.VarRef","text":"Variable of FreeTensor All variables in FreeTensor DSL (declared via Var , created by empty or var , returned by libop , etc.), and their slices, are VarRef objects. Operations on VarRef objects generates AST nodes Source code in freetensor/core/expr.py class VarRef ( ffi . FrontendVar ): ''' Variable of FreeTensor All variables in FreeTensor DSL (declared via `Var`, created by `empty` or `var`, returned by `libop`, etc.), and their slices, are `VarRef` objects. Operations on `VarRef` objects generates AST nodes ''' def __init__ ( self , name : str , vardef , full_shape : Sequence , dtype : ffi . DataType , mtype : ffi . MemType , indices : Sequence = []): super ( VarRef , self ) . __init__ ( name , full_shape , dtype , mtype , indices ) self . vardef = vardef from .stmt import find_borrowed_vardefs self . borrowed_vardefs = find_borrowed_vardefs ( indices ) for item in self . borrowed_vardefs : item . lend_out () def __del__ ( self ): for item in self . borrowed_vardefs : item . reclaim () def __getitem__ ( self , key ): return VarRef ( self . name , self . vardef , self . full_shape , self . dtype , self . mtype , self . chain_indices ( self . _parse_key ( key ))) def __setitem__ ( self , key , value ): var = VarRef ( self . name , self . vardef , self . full_shape , self . dtype , self . mtype , self . chain_indices ( self . _parse_key ( key ))) if var . ndim > 0 : if value is AlreadyMadeReduceTo : return from .. import libop libop . assign ( var , value ) return if var . vardef . atype == ffi . AccessType ( \"input\" ): raise ffi . InvalidProgram ( \"Cannot modify an \\\" input \\\" tensor `\" + self . name + \"`\" ) if var . vardef . borrower_cnt > 0 : raise ffi . InvalidProgram ( \"Cannot modify tensor `\" + self . name + \"` becuase it has been borrowed in another tensor's shape, \" \"a tensor slice, or a range of a loop\" ) if value is AlreadyMadeReduceTo : # Following the checks above return top = ctx_stack . top () top . append_stmt ( var . as_store ( top . get_metadata (), value )) def as_store ( self , metadata , value ): if ( not isinstance ( value , ffi . AnyExpr ) and ffi . up_cast ( dtype ( value ), self . vardef . dtype ) != self . vardef . dtype ): # Add explicit cast node, to avoid confusion after propagation value = cast ( value , self . vardef . dtype ) return super ( VarRef , self ) . as_store ( metadata , value ) def as_reduce_to ( self , reduce_op , metadata , value , atomic = False ): if ( not isinstance ( value , ffi . AnyExpr ) and ffi . up_cast ( dtype ( value ), self . vardef . dtype ) != self . vardef . dtype ): # Add explicit cast node, to avoid confusion after propagation value = cast ( value , self . vardef . dtype ) return super ( VarRef , self ) . as_reduce_to ( reduce_op , metadata , value , atomic ) def select ( self , idx , dim ): assert isinstance ( dim , int ) assert dim >= 0 and dim < self . ndim indices = [ slice ( None , None ) if d != dim else idx for d in range ( self . ndim ) ] return self [ indices ] def shape ( self , dim = None ): ''' Return lengths of all dimensions or the length of one dimension `.shape()` -> list of lengths of all dimensions `.shape(dim)` -> length of dimension `dim`, where `dim` can be `int` or `Expr` All lengths can be `Expr` (if the length is dynamically decided) or `int` (if statically decided) ''' intOrExpr = lambda x : x . val if isinstance ( x , ffi . IntConst ) else x if dim is None : return [ intOrExpr ( d ) for d in super ( VarRef , self ) . shape ()] else : return intOrExpr ( super ( VarRef , self ) . shape ( dim )) def _parse_key ( self , key ): if key is None or key is ... : key = () if not isinstance ( key , collections . abc . Sequence ): key = ( key ,) ffiIdx = [] for idx , length in zip ( key , self . shape ()): if isinstance ( idx , slice ): start = idx . start if idx . start is not None else 0 stop = idx . stop if idx . stop is not None else length assert idx . step is None or idx . step == 1 ffiIdx . append ( ffi . FrontendVarIdx ( start , stop )) elif isinstance ( idx , VarRef ): if len ( idx . full_shape ) == len ( idx . indices ): ffiIdx . append ( ffi . FrontendVarIdx ( idx . as_load ())) else : assert len ( key ) == 1 , f \"Shape of an index of { self . name } should be 1-D, instead of { idx . name } \" assert type ( idx . full_shape [ 0 ] ) is ffi . IntConst , \"Dynamic number of dimensions is not supported\" ndim = idx . full_shape [ 0 ] . val ffiIdx += [ ffi . FrontendVarIdx ( idx [ i ] . as_load ()) for i in range ( ndim ) ] else : ffiIdx . append ( ffi . FrontendVarIdx ( idx )) return ffiIdx def __add__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . add ( self , other ) return self . as_load () + other def __radd__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . add ( other , self ) return other + self . as_load () def __iadd__ ( self , other ): if self . ndim > 0 : from .. import libop libop . add_to ( self , other ) return AlreadyMadeReduceTo top = ctx_stack . top () top . append_stmt ( self . as_reduce_to ( ffi . ReduceOp . Add , top . get_metadata (), other )) return AlreadyMadeReduceTo def __sub__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . sub ( self , other ) return self . as_load () - other def __rsub__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . sub ( other , self ) return other - self . as_load () def __isub__ ( self , other ): if self . ndim > 0 : from .. import libop libop . sub_to ( self , other ) return AlreadyMadeReduceTo top = ctx_stack . top () top . append_stmt ( self . as_reduce_to ( ffi . ReduceOp . Sub , top . get_metadata (), other )) return AlreadyMadeReduceTo def __mul__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . mul ( self , other ) return self . as_load () * other def __rmul__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . mul ( other , self ) return other * self . as_load () def __imul__ ( self , other ): if self . ndim > 0 : from .. import libop libop . mul_to ( self , other ) return AlreadyMadeReduceTo top = ctx_stack . top () top . append_stmt ( self . as_reduce_to ( ffi . ReduceOp . Mul , top . get_metadata (), other )) return AlreadyMadeReduceTo def __truediv__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . truediv ( self , other ) return self . as_load () / other def __rtruediv__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . truediv ( other , self ) return other / self . as_load () def __itruediv__ ( self , other ): if self . ndim > 0 : from .. import libop libop . truediv_to ( self , other ) return AlreadyMadeReduceTo return NotImplemented # Fallback to x = x / y def __floordiv__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . floordiv ( self , other ) return self . as_load () // other def __rfloordiv__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . floordiv ( other , self ) return other // self . as_load () def __ifloordiv__ ( self , other ): if self . ndim > 0 : from .. import libop libop . floordiv_to ( self , other ) return AlreadyMadeReduceTo return NotImplemented # Fallback to x = x // y def __mod__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . mod ( self , other ) return self . as_load () % other def __rmod__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . mod ( other , self ) return other % self . as_load () def __imod__ ( self , other ): if self . ndim > 0 : from .. import libop libop . mod_to ( self , other ) return AlreadyMadeReduceTo return NotImplemented # Fallback to x = x % y def __lt__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . lt ( self , other ) return self . as_load () < other def __le__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . le ( self , other ) return self . as_load () <= other def __gt__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . gt ( self , other ) return self . as_load () > other def __ge__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . ge ( self , other ) return self . as_load () >= other def __eq__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . eq ( self , other ) return self . as_load () == other def __ne__ ( self , other ): if self . ndim > 0 : from .. import libop return libop . ne ( self , other ) return self . as_load () != other def __neg__ ( self ): if self . ndim > 0 : from .. import libop return libop . neg ( self ) return 0 - self . as_load () def __matmul__ ( self , other ): from .. import libop return libop . matmul ( self , other ) def __rmatmul__ ( self , other ): from .. import libop return libop . matmul ( other , self )","title":"VarRef"},{"location":"api/#freetensor.core.expr.VarRef.as_reduce_to","text":"as_reduce_to(self: freetensor_ffi.FrontendVar, op: freetensor_ffi.ReduceOp, metadata: freetensor_ffi.Metadata, value: freetensor_ffi.Expr, atomic: bool = False) -> freetensor_ffi.Stmt Source code in freetensor/core/expr.py def as_reduce_to ( self , reduce_op , metadata , value , atomic = False ): if ( not isinstance ( value , ffi . AnyExpr ) and ffi . up_cast ( dtype ( value ), self . vardef . dtype ) != self . vardef . dtype ): # Add explicit cast node, to avoid confusion after propagation value = cast ( value , self . vardef . dtype ) return super ( VarRef , self ) . as_reduce_to ( reduce_op , metadata , value , atomic )","title":"as_reduce_to()"},{"location":"api/#freetensor.core.expr.VarRef.as_store","text":"as_store(self: freetensor_ffi.FrontendVar, metadata: freetensor_ffi.Metadata, value: freetensor_ffi.Expr) -> freetensor_ffi.Stmt Source code in freetensor/core/expr.py def as_store ( self , metadata , value ): if ( not isinstance ( value , ffi . AnyExpr ) and ffi . up_cast ( dtype ( value ), self . vardef . dtype ) != self . vardef . dtype ): # Add explicit cast node, to avoid confusion after propagation value = cast ( value , self . vardef . dtype ) return super ( VarRef , self ) . as_store ( metadata , value )","title":"as_store()"},{"location":"api/#freetensor.core.expr.VarRef.shape","text":"Return lengths of all dimensions or the length of one dimension .shape() -> list of lengths of all dimensions .shape(dim) -> length of dimension dim , where dim can be int or Expr All lengths can be Expr (if the length is dynamically decided) or int (if statically decided) Source code in freetensor/core/expr.py def shape ( self , dim = None ): ''' Return lengths of all dimensions or the length of one dimension `.shape()` -> list of lengths of all dimensions `.shape(dim)` -> length of dimension `dim`, where `dim` can be `int` or `Expr` All lengths can be `Expr` (if the length is dynamically decided) or `int` (if statically decided) ''' intOrExpr = lambda x : x . val if isinstance ( x , ffi . IntConst ) else x if dim is None : return [ intOrExpr ( d ) for d in super ( VarRef , self ) . shape ()] else : return intOrExpr ( super ( VarRef , self ) . shape ( dim ))","title":"shape()"},{"location":"api/#freetensor.core.expr.abs","text":"Absolute value For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.abs Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The absolute value Source code in freetensor/core/expr.py def abs ( expr ): ''' Absolute value For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.abs Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The absolute value ''' if _istensor ( expr ): from .. import libop return libop . abs ( expr ) if isinstance ( expr , Number ): return builtins . abs ( expr ) return ffi . makeAbs ( expr )","title":"abs()"},{"location":"api/#freetensor.core.expr.add","text":"lhs + rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.add Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The sum Source code in freetensor/core/expr.py def add ( lhs , rhs ): ''' `lhs + rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.add Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The sum ''' return lhs + rhs","title":"add()"},{"location":"api/#freetensor.core.expr.any","text":"Create an AnyExpr node (only for testing) Any nodes matches any expression nodes in ast.match Source code in freetensor/core/expr.py def any (): ''' Create an AnyExpr node (only for testing) Any nodes matches any expression nodes in `ast.match` ''' return ffi . makeAnyExpr ()","title":"any()"},{"location":"api/#freetensor.core.expr.cast","text":"Cast to another type Parameters: expr ( VarRef or Number ) \u2013 The operand dtype ( DataTypr or str ) \u2013 The target data type Returns: VarRef or Number \u2013 The result Source code in freetensor/core/expr.py def cast ( expr , dtype ): ''' Cast to another type Parameters ---------- expr : VarRef or Number The operand dtype : DataTypr or str The target data type Returns ------- VarRef or Number The result ''' return ffi . makeCast ( expr , ffi . DataType ( dtype ))","title":"cast()"},{"location":"api/#freetensor.core.expr.ceil","text":"Round a float up to an interger (towards +inf) For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ceil Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result Source code in freetensor/core/expr.py def ceil ( expr ): ''' Round a float up to an interger (towards +inf) For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ceil Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The result ''' if _istensor ( expr ): from .. import libop return libop . ceil ( expr ) return ffi . makeCeil ( expr )","title":"ceil()"},{"location":"api/#freetensor.core.expr.ceildiv","text":"Ceiling integer division of lhs dividing by rhs The result rounds towards positive infinity For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ceildiv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient Source code in freetensor/core/expr.py def ceildiv ( lhs , rhs ): ''' Ceiling integer division of `lhs` dividing by `rhs` The result rounds towards positive infinity For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ceildiv Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The quotient ''' if _istensor ( lhs ) or _istensor ( rhs ): from .. import libop return libop . ceildiv ( lhs , rhs ) if type ( lhs ) is int and type ( rhs ) is int : return lhs // rhs + ( lhs % rhs > 0 ) return ffi . makeCeilDiv ( lhs , rhs )","title":"ceildiv()"},{"location":"api/#freetensor.core.expr.dtype","text":"Get element data type of a variable Source code in freetensor/core/expr.py def dtype ( var ): ''' Get element data type of a variable ''' if isinstance ( var , VarRef ): return var . dtype elif isinstance ( var , ffi . Expr ): return var . dtype else : # TODO: Config default type if isinstance ( var , bool ): # NOTE: before int, because bool in Python is a sub-class of int return ffi . DataType ( \"bool\" ) elif isinstance ( var , float ): return ffi . DataType ( \"float32\" ) elif isinstance ( var , int ): return ffi . DataType ( \"int32\" ) else : raise Exception ( 'Unknown scalar type: ' + str ( type ( var )))","title":"dtype()"},{"location":"api/#freetensor.core.expr.eq","text":"lhs == rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.eq Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison Source code in freetensor/core/expr.py def eq ( lhs , rhs ): ''' `lhs == rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.eq Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The comparison ''' return lhs == rhs","title":"eq()"},{"location":"api/#freetensor.core.expr.exp","text":"Natural exponent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.exp Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The exponent Source code in freetensor/core/expr.py def exp ( expr ): ''' Natural exponent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.exp Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The exponent ''' if _istensor ( expr ): from .. import libop return libop . exp ( expr ) if isinstance ( expr , Number ): return math . exp ( expr ) return ffi . makeExp ( expr )","title":"exp()"},{"location":"api/#freetensor.core.expr.floor","text":"Round a float down to an interger (towards -inf) For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.floor Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result Source code in freetensor/core/expr.py def floor ( expr ): ''' Round a float down to an interger (towards -inf) For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.floor Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The result ''' if _istensor ( expr ): from .. import libop return libop . floor ( expr ) return ffi . makeFloor ( expr )","title":"floor()"},{"location":"api/#freetensor.core.expr.floordiv","text":"Floored integer division of lhs dividing by rhs The result rounds towards negative infinity (following Python convention, instead of C) This function is recommended over round_towards_0_div , as it enjoys more optimizations For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.floordiv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient Source code in freetensor/core/expr.py def floordiv ( lhs , rhs ): ''' Floored integer division of `lhs` dividing by `rhs` The result rounds towards negative infinity (following Python convention, instead of C) This function is recommended over `round_towards_0_div`, as it enjoys more optimizations For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.floordiv Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The quotient ''' return lhs // rhs","title":"floordiv()"},{"location":"api/#freetensor.core.expr.ge","text":"lhs >= rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ge Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison Source code in freetensor/core/expr.py def ge ( lhs , rhs ): ''' `lhs >= rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ge Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The comparison ''' return lhs >= rhs","title":"ge()"},{"location":"api/#freetensor.core.expr.gt","text":"lhs > rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.gt Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison Source code in freetensor/core/expr.py def gt ( lhs , rhs ): ''' `lhs > rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.gt Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The comparison ''' return lhs > rhs","title":"gt()"},{"location":"api/#freetensor.core.expr.if_then_else","text":"Similar to then_case if cond else else_case NOTE: there is NO guarantee that only one branch will be executed. In some cases, both branches will be executed and the result of one of them will be picked. Therefore, please do NOT use if_then_else to guard an out-of-bound array indexing Parameters: cond ( VarRef of Number ) \u2013 Condition lhs ( VarRef or Number ) \u2013 Then-case experssion rhs ( VarRef or Number ) \u2013 Else-case expression Returns: VarRef or Number \u2013 The result Source code in freetensor/core/expr.py def if_then_else ( cond , then_case , else_case ): ''' Similar to `then_case if cond else else_case` NOTE: there is NO guarantee that only one branch will be executed. In some cases, both branches will be executed and the result of one of them will be picked. Therefore, please do NOT use `if_then_else` to guard an out-of-bound array indexing Parameters ---------- cond : VarRef of Number Condition lhs : VarRef or Number Then-case experssion rhs : VarRef or Number Else-case expression Returns ------- VarRef or Number The result ''' if type ( cond ) is bool : return then_case if cond else else_case return ffi . makeIfExpr ( cond , then_case , else_case )","title":"if_then_else()"},{"location":"api/#freetensor.core.expr.intrinsic","text":"Invoke whatever target code Parameters: fmt ( str ) \u2013 What to run. \"%\" is filled by parameters one by one. E.g. sinf(%) The following variadic arguments ( Expr ) \u2013 Parameters to fmt ret_type ( DataType or str ) \u2013 (Keyword argument only) The return type. Void for no return type. Defaults to Void has_side_effect ( bool ) \u2013 (Keyword argument only) True to indicate the intrinsic modifes something other than the return value. Defaults to false Source code in freetensor/core/expr.py def intrinsic ( fmt , * params , ** kws ): \"\"\" Invoke whatever target code Parameters ---------- fmt : str What to run. \"%\" is filled by parameters one by one. E.g. sinf(%) The following variadic arguments : Expr Parameters to `fmt` ret_type : DataType or str (Keyword argument only) The return type. Void for no return type. Defaults to Void has_side_effect: bool (Keyword argument only) True to indicate the intrinsic modifes something other than the return value. Defaults to false \"\"\" ret_type = ffi . DataType ( \"void\" ) has_side_effect = False if \"ret_type\" in kws : ret_type = ffi . DataType ( kws [ \"ret_type\" ]) del kws [ \"ret_type\" ] if \"has_side_effect\" in kws : has_side_effect = kws [ \"has_side_effect\" ] del kws [ \"has_side_effect\" ] assert len ( kws ) == 0 , \"Unrecognized keyword arguments: %s \" % kws return ffi . makeIntrinsic ( fmt , params , ret_type , has_side_effect )","title":"intrinsic()"},{"location":"api/#freetensor.core.expr.l_and","text":"Logical and of lhs and rhs NOTE: Short-circuit evaluation is NOT supported For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_and Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The logical and Source code in freetensor/core/expr.py def l_and ( lhs , rhs ): ''' Logical and of `lhs` and `rhs` NOTE: Short-circuit evaluation is NOT supported For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_and Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The logical and ''' if _istensor ( lhs ) or _istensor ( rhs ): from .. import libop return libop . l_and ( lhs , rhs ) if type ( lhs ) is bool and type ( rhs ) is bool : return lhs and rhs else : return ffi . makeLAnd ( lhs , rhs )","title":"l_and()"},{"location":"api/#freetensor.core.expr.l_not","text":"Logical not For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_not Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The logical not Source code in freetensor/core/expr.py def l_not ( expr ): ''' Logical not For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_not Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The logical not ''' if _istensor ( expr ): from .. import libop return libop . l_not ( expr ) if type ( expr ) is bool : return not expr else : return ffi . makeLNot ( expr )","title":"l_not()"},{"location":"api/#freetensor.core.expr.l_or","text":"Logical or of lhs and rhs NOTE: Short-circuit evaluation is NOT supported For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_or Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The logical or Source code in freetensor/core/expr.py def l_or ( lhs , rhs ): ''' Logical or of `lhs` and `rhs` NOTE: Short-circuit evaluation is NOT supported For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_or Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The logical or ''' if _istensor ( lhs ) or _istensor ( rhs ): from .. import libop return libop . l_or ( lhs , rhs ) if type ( lhs ) is bool and type ( rhs ) is bool : return lhs or rhs else : return ffi . makeLOr ( lhs , rhs )","title":"l_or()"},{"location":"api/#freetensor.core.expr.le","text":"lhs <= rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.le Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison Source code in freetensor/core/expr.py def le ( lhs , rhs ): ''' `lhs <= rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.le Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The comparison ''' return lhs <= rhs","title":"le()"},{"location":"api/#freetensor.core.expr.lt","text":"lhs < rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.lt Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison Source code in freetensor/core/expr.py def lt ( lhs , rhs ): ''' `lhs < rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.lt Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The comparison ''' return lhs < rhs","title":"lt()"},{"location":"api/#freetensor.core.expr.max","text":"Maximum of lhs and rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.max Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The maximum Source code in freetensor/core/expr.py def max ( lhs , rhs ): ''' Maximum of `lhs` and `rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.max Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The maximum ''' if _istensor ( lhs ) or _istensor ( rhs ): from .. import libop return libop . max ( lhs , rhs ) if isinstance ( lhs , Number ) and isinstance ( rhs , Number ): return builtins . max ( lhs , rhs ) return ffi . makeMax ( lhs , rhs )","title":"max()"},{"location":"api/#freetensor.core.expr.min","text":"Minimum of lhs and rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.min Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The minimum Source code in freetensor/core/expr.py def min ( lhs , rhs ): ''' Minimum of `lhs` and `rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.min Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The minimum ''' if _istensor ( lhs ) or _istensor ( rhs ): from .. import libop return libop . min ( lhs , rhs ) if isinstance ( lhs , Number ) and isinstance ( rhs , Number ): return builtins . min ( lhs , rhs ) return ffi . makeMin ( lhs , rhs )","title":"min()"},{"location":"api/#freetensor.core.expr.mod","text":"lhs modulus rhs The result is always non-negative (following Python convention, instead of C). This function is recommended over remainder , as it enjoys more optimizations For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.mod Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The modulo Source code in freetensor/core/expr.py def mod ( lhs , rhs ): ''' `lhs` modulus `rhs` The result is always non-negative (following Python convention, instead of C). This function is recommended over `remainder`, as it enjoys more optimizations For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.mod Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The modulo ''' return lhs % rhs","title":"mod()"},{"location":"api/#freetensor.core.expr.mtype","text":"Get memory type of a variable Source code in freetensor/core/expr.py def mtype ( var ): ''' Get memory type of a variable ''' if isinstance ( var , VarRef ): return var . mtype else : return 'byvalue'","title":"mtype()"},{"location":"api/#freetensor.core.expr.mul","text":"lhs * rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.mul Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The product Source code in freetensor/core/expr.py def mul ( lhs , rhs ): ''' `lhs * rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.mul Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The product ''' return lhs * rhs","title":"mul()"},{"location":"api/#freetensor.core.expr.ndim","text":"Get the number of dimensions of a variable Source code in freetensor/core/expr.py def ndim ( var ): ''' Get the number of dimensions of a variable ''' if isinstance ( var , VarRef ): return var . ndim else : return 0","title":"ndim()"},{"location":"api/#freetensor.core.expr.ne","text":"lhs != rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ne Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison Source code in freetensor/core/expr.py def ne ( lhs , rhs ): ''' `lhs != rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ne Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The comparison ''' return lhs != rhs","title":"ne()"},{"location":"api/#freetensor.core.expr.remainder","text":"Remainder of lhs dividing rhs The result can be positive or negative (following C convention, instead of Python). End users are encouraged to use lhs % rhs instead, which follows Python convetion, and enjoys better optimization in FreeTensor For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.remainder Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The remainder Source code in freetensor/core/expr.py def remainder ( lhs , rhs ): ''' Remainder of `lhs` dividing `rhs` The result can be positive or negative (following C convention, instead of Python). End users are encouraged to use `lhs % rhs` instead, which follows Python convetion, and enjoys better optimization in FreeTensor For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.remainder Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The remainder ''' if _istensor ( lhs ) or _istensor ( rhs ): from .. import libop return libop . remainder ( lhs , rhs ) return ffi . makeRemainder ( lhs , rhs )","title":"remainder()"},{"location":"api/#freetensor.core.expr.round_towards_0_div","text":"C-style integer division of lhs dividing by rhs The result rounds towards 0 (following C convention, instead of Python) End users are encouraged to use lhs // rhs instead, which follows Python convetion, and enjoys better optimization in FreeTensor For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.round_towards_0_div Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient Source code in freetensor/core/expr.py def round_towards_0_div ( lhs , rhs ): ''' C-style integer division of `lhs` dividing by `rhs` The result rounds towards 0 (following C convention, instead of Python) End users are encouraged to use `lhs // rhs` instead, which follows Python convetion, and enjoys better optimization in FreeTensor For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.round_towards_0_div Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The quotient ''' if _istensor ( lhs ) or _istensor ( rhs ): from .. import libop return libop . round_towards_0_div ( lhs , rhs ) return ffi . makeRoundTowards0Div ( lhs , rhs )","title":"round_towards_0_div()"},{"location":"api/#freetensor.core.expr.shape","text":"shape(var, i): Get size of specified dimension of a variable shape(var): Get sizes of all dimensions of a variable Source code in freetensor/core/expr.py def shape ( var , i = None ): ''' shape(var, i): Get size of specified dimension of a variable shape(var): Get sizes of all dimensions of a variable ''' if isinstance ( var , VarRef ): return var . shape ( i ) else : if i is None : return () else : raise Exception ( f 'Getting size of dimension { i } of scalar { var } ' )","title":"shape()"},{"location":"api/#freetensor.core.expr.sigmoid","text":"Sigmoid For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sigmoid Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result Source code in freetensor/core/expr.py def sigmoid ( expr ): ''' Sigmoid For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sigmoid Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The result ''' if _istensor ( expr ): from .. import libop return libop . sigmoid ( expr ) return ffi . makeSigmoid ( expr )","title":"sigmoid()"},{"location":"api/#freetensor.core.expr.sqrt","text":"Square root For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sqrt Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The square root Source code in freetensor/core/expr.py def sqrt ( expr ): ''' Square root For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sqrt Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The square root ''' if _istensor ( expr ): from .. import libop return libop . sqrt ( expr ) if isinstance ( expr , Number ): return math . sqrt ( expr ) return ffi . makeSqrt ( expr )","title":"sqrt()"},{"location":"api/#freetensor.core.expr.square","text":"Square For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.square Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The square Source code in freetensor/core/expr.py def square ( expr ): ''' Square For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.square Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The square ''' if _istensor ( expr ): from .. import libop return libop . square ( expr ) if isinstance ( expr , Number ): return expr * expr return ffi . makeSquare ( expr )","title":"square()"},{"location":"api/#freetensor.core.expr.sub","text":"lhs - rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sub Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The difference Source code in freetensor/core/expr.py def sub ( lhs , rhs ): ''' `lhs - rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sub Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The difference ''' return lhs - rhs","title":"sub()"},{"location":"api/#freetensor.core.expr.tanh","text":"Hyperbolic tangent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.tanh Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result Source code in freetensor/core/expr.py def tanh ( expr ): ''' Hyperbolic tangent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.tanh Parameters ---------- expr : VarRef or Number The operand Returns ------- VarRef or Number The result ''' if _istensor ( expr ): from .. import libop return libop . tanh ( expr ) if isinstance ( expr , Number ): return math . tanh ( expr ) return ffi . makeTanh ( expr )","title":"tanh()"},{"location":"api/#freetensor.core.expr.truediv","text":"Floating point division of lhs dividing by rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.truediv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient Source code in freetensor/core/expr.py def truediv ( lhs , rhs ): ''' Floating point division of `lhs` dividing by `rhs` For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.truediv Parameters ---------- lhs : VarRef or Number The left-hand-side operand rhs : VarRef or Number The right-hand-side operand Returns ------- VarRef or Number The quotient ''' return lhs / rhs","title":"truediv()"},{"location":"api/#freetensor.core.frontend","text":"A frontend transforming user Python functions to ASTs via staging.","title":"frontend"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload","text":"Helper class managing context in IR staging. Source code in freetensor/core/frontend.py class FreeTensorOverload ( StagingOverload ): '''Helper class managing context in IR staging.''' def __init__ ( self ): super () . __init__ () self . lifetime_stack : List [ LifetimeScope ] = [] self . closure : Dict [ str , Any ] = {} self . name_dict : Dict [ str , int ] = {} def register_vardef ( self , name , shape , dtype , atype , mtype = None , capture = None ): fullname = self . fullname ( name ) if capture : self . closure [ fullname ] = capture return self . lifetime_stack [ - 1 ] . register_inner_scope ( _VarDef ( fullname , shape , dtype , atype , mtype )) def register_inlined_invoke ( self , ret_names : Sequence [ str ], func : ffi . Func , args , kvs ): ret_names = [ self . fullname ( name ) for name in ret_names ] return self . lifetime_stack [ - 1 ] . register_inner_scope ( Invoke ( ret_names , func , args , kvs )) def register_assert ( self , pred ): self . lifetime_stack [ - 1 ] . register_inner_scope ( Assert ( pred )) def fullname ( self , name : str ) -> str : '''Get distinct name.''' if name in self . name_dict : self . name_dict [ name ] += 1 return f ' { name } _ { self . name_dict [ name ] } ' else : self . name_dict [ name ] = 0 return name def in_staging ( self ,): return len ( self . lifetime_stack ) > 0 def custom_attr ( self , obj : Any , attr : str ) -> Any : if attr == \"ndim\" : return ndim ( obj ) if attr == \"shape\" : return lambda i = None : shape ( obj , i ) if attr == \"dtype\" : return dtype ( obj ) if attr == \"mtype\" : return mtype ( obj ) raise AttributeError () def functiondef_wrapper ( self , filename : str , func ): basic_wrapped = super () . functiondef_wrapper ( filename , func ) def wrapped ( * args , __freetensor_transform_outermost__ = False , ** kwargs ): if __freetensor_transform_outermost__ : call_metadata = None else : call_metadata = ctx_stack . top () . get_metadata () ctx_stack . top () . clear_metadata () prev = ctx_stack . top () . caller_metadata ctx_stack . top () . set_caller_metadata ( call_metadata ) result = basic_wrapped ( * args , ** kwargs ) ctx_stack . top () . set_caller_metadata ( prev ) return result return wrapped def metadata ( self , entry : str ) -> None : parts = entry . split () if len ( parts ) == 0 : return key = parts [ 0 ] if len ( parts ) > 1 : key = key [: - 1 ] val = parts [ 1 ] if key == 'label' : ctx_stack . top () . add_label ( val ) elif key == 'no_deps' : back = inspect . currentframe () . f_back if val in back . f_locals : var = back . f_locals [ val ] elif val in back . f_globals : var = back . f_globals [ val ] else : raise self . error ( f 'Variable { val } not found for annotating comment ( { key } : { val } )' ) if not isinstance ( var , VarRef ): raise self . error ( f 'Variable { val } = { var } is not a VarRef, which is required by annotating comment ( { key } : { val } )' ) ctx_stack . top () . add_next_no_deps ( var . name ) elif key == 'prefer_libs' : ctx_stack . top () . set_next_prefer_libs () def at_position ( self , filename : str , lineno : int ) -> None : ctx_stack . top () . set_next_location ( filename , lineno )","title":"FreeTensorOverload"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.allow_shortcut_scope","text":"Opens a scope that allows shortcut control flows in a statically deterministic context. Need to be closed by with statement. Source code in freetensor/core/frontend.py def allow_shortcut_scope ( self , allow : bool ): '''Opens a scope that allows shortcut control flows in a statically deterministic context. Need to be closed by `with` statement.''' return AllowShortcutScope ( self , allow )","title":"allow_shortcut_scope()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.assert_stmt","text":"Assert staging tool. Source code in freetensor/core/frontend.py def assert_stmt ( self , test ): '''Assert staging tool.''' if isinstance ( test , StagedPredicate ): test . assert_stmt () else : assert test","title":"assert_stmt()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.assign_stmt","text":"Customized assign wrapper. If value is instance of StagedAssignable , it's regarded as a customized assign behavior and gets executed with the assigned target variable name. This wrapper is used for initializing a variable. Source code in freetensor/core/frontend.py def assign_stmt ( self , name : str , value ): '''Customized assign wrapper. If `value` is instance of `StagedAssignable`, it's regarded as a customized assign behavior and gets executed with the assigned target variable name. This wrapper is used for initializing a variable. ''' if isinstance ( value , StagedAssignable ): return value . assign ( name ) else : return value","title":"assign_stmt()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.at_position","text":"Code position handler. Defaults to a no-op. Can be overridden by subclasses. Parameters: filename ( str ) \u2013 Name of the file containing code for the next statement. lineno ( int ) \u2013 Line number of the next statement. Source code in freetensor/core/frontend.py def at_position ( self , filename : str , lineno : int ) -> None : ctx_stack . top () . set_next_location ( filename , lineno )","title":"at_position()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.break_stmt","text":"Break staging tool. Only allow break in static control flow. Source code in freetensor/core/frontend.py def break_stmt ( self ): '''Break staging tool. Only allow break in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Break is only allowed in statically deterministic control flow.' ) raise BreakException ()","title":"break_stmt()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.continue_stmt","text":"Continue staging tool. Only allow continue in static control flow. Source code in freetensor/core/frontend.py def continue_stmt ( self ): '''Continue staging tool. Only allow continue in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Continue is only allowed in statically deterministic control flow.' ) raise ContinueException ()","title":"continue_stmt()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.custom_attr","text":"Customized attribute accessor. The framework first looks for a Python native attribute. If not found, it looks for this overloaded custom attribute resolver. The default implementation provides no custom attribute. Can be overridden by subclasses. Parameters: obj ( Any ) \u2013 Object to access attribute. attr ( str ) \u2013 Attribute name. Returns: Any \u2013 The attribute value. Source code in freetensor/core/frontend.py def custom_attr ( self , obj : Any , attr : str ) -> Any : if attr == \"ndim\" : return ndim ( obj ) if attr == \"shape\" : return lambda i = None : shape ( obj , i ) if attr == \"dtype\" : return dtype ( obj ) if attr == \"mtype\" : return mtype ( obj ) raise AttributeError ()","title":"custom_attr()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.foreach","text":"Customized foreach wrapper. If value is instance of StagedIterable , its regarded as a customized foreach behavior and used to generate code for the python for loop. Otherwise, we try to execute the loop as usual. Source code in freetensor/core/frontend.py def foreach ( self , names , iter , body : Callable [[ Any ], None ]) -> None : '''Customized foreach wrapper. If `value` is instance of `StagedIterable`, its regarded as a customized foreach behavior and used to generate code for the python for loop. Otherwise, we try to execute the loop as usual. ''' if isinstance ( iter , StagedIterable ): iter . foreach ( names , body ) else : for iter_var in iter : try : body ( iter_var ) except BreakException : break except ContinueException : continue","title":"foreach()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.fullname","text":"Get distinct name. Source code in freetensor/core/frontend.py def fullname ( self , name : str ) -> str : '''Get distinct name.''' if name in self . name_dict : self . name_dict [ name ] += 1 return f ' { name } _ { self . name_dict [ name ] } ' else : self . name_dict [ name ] = 0 return name","title":"fullname()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.functiondef_wrapper","text":"Function definition wrapper. This wrapper performs extra initialization and cleanup for function definition. Source code in freetensor/core/frontend.py def functiondef_wrapper ( self , filename : str , func ): basic_wrapped = super () . functiondef_wrapper ( filename , func ) def wrapped ( * args , __freetensor_transform_outermost__ = False , ** kwargs ): if __freetensor_transform_outermost__ : call_metadata = None else : call_metadata = ctx_stack . top () . get_metadata () ctx_stack . top () . clear_metadata () prev = ctx_stack . top () . caller_metadata ctx_stack . top () . set_caller_metadata ( call_metadata ) result = basic_wrapped ( * args , ** kwargs ) ctx_stack . top () . set_caller_metadata ( prev ) return result return wrapped","title":"functiondef_wrapper()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.if_then_else_expr","text":"If-then-else expression staging tool. Source code in freetensor/core/frontend.py def if_then_else_expr ( self , predicate , then_expr , else_expr ): '''If-then-else expression staging tool.''' if isinstance ( predicate , StagedPredicate ): return predicate . if_then_else_expr ( then_expr , else_expr ) else : if predicate : return then_expr () else : return else_expr ()","title":"if_then_else_expr()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.if_then_else_stmt","text":"If-then-else statement staging tool. When predicate is deterministic in staging, only one branch is generated. Otherwise, a If node in IR is generated. Source code in freetensor/core/frontend.py def if_then_else_stmt ( self , predicate , then_body , else_body = None ): '''If-then-else statement staging tool. When predicate is deterministic in staging, only one branch is generated. Otherwise, a If node in IR is generated. ''' if isinstance ( predicate , StagedPredicate ): predicate . if_then_else_stmt ( then_body , else_body ) else : if predicate : then_body () elif else_body : else_body ()","title":"if_then_else_stmt()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.load_attr","text":"Load attribute staging tool. Allows customization of reading attributes. Source code in freetensor/core/frontend.py def load_attr ( self , obj , attr : str ): '''Load attribute staging tool. Allows customization of reading attributes.''' try : return getattr ( obj , attr ) except AttributeError : try : # Have to use AttributeError again, since a custom attribute might have # a None value result = self . custom_attr ( obj , attr ) successful = True except AttributeError : successful = False if successful : return result else : raise","title":"load_attr()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.metadata","text":"Metadata handler. A metadata line is a comment starting with #! and followed by a metadata, represented as a string parameter. Defaults to a no-op. Can be overridden by subclasses. Parameters: content ( str ) \u2013 The metadata content. Source code in freetensor/core/frontend.py def metadata ( self , entry : str ) -> None : parts = entry . split () if len ( parts ) == 0 : return key = parts [ 0 ] if len ( parts ) > 1 : key = key [: - 1 ] val = parts [ 1 ] if key == 'label' : ctx_stack . top () . add_label ( val ) elif key == 'no_deps' : back = inspect . currentframe () . f_back if val in back . f_locals : var = back . f_locals [ val ] elif val in back . f_globals : var = back . f_globals [ val ] else : raise self . error ( f 'Variable { val } not found for annotating comment ( { key } : { val } )' ) if not isinstance ( var , VarRef ): raise self . error ( f 'Variable { val } = { var } is not a VarRef, which is required by annotating comment ( { key } : { val } )' ) ctx_stack . top () . add_next_no_deps ( var . name ) elif key == 'prefer_libs' : ctx_stack . top () . set_next_prefer_libs ()","title":"metadata()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.return_stmt","text":"Return staging tool. Only allow return in static control flow. Source code in freetensor/core/frontend.py def return_stmt ( self , value , funcname ): '''Return staging tool. Only allow return in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Return is only allowed in statically deterministic control flow.' ) if isinstance ( value , StagedUnpackAssignable ): # We don't know how many items are there, so no unpacking value = value . assign ( funcname ) if isinstance ( value , StagedAssignable ): value = value . assign ( funcname ) raise ReturnException ( value )","title":"return_stmt()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.unpack_assign_stmt","text":"Customized assign wrapper for one or more targets. If values is instance of StagedUnpackAssignable , it's regarded as a customized assign behavior and gets executed with all the assigned targets' names. Otherwise, it calls assign_stmt with each sub-assignments. Please note that names can be nested tuples like (\"a\", (\"b\", \"c\")) . Please also note that names can also be a single string like \"a\" even if values is a tuple. There is no unpacking in this case Source code in freetensor/core/frontend.py def unpack_assign_stmt ( self , names , values ): '''Customized assign wrapper for one or more targets. If `values` is instance of `StagedUnpackAssignable`, it's regarded as a customized assign behavior and gets executed with all the assigned targets' names. Otherwise, it calls `assign_stmt` with each sub-assignments. Please note that `names` can be nested tuples like `(\"a\", (\"b\", \"c\"))`. Please also note that `names` can also be a single string like \"a\" even if `values` is a tuple. There is no unpacking in this case ''' if isinstance ( values , StagedUnpackAssignable ): return values . assign ( names ) elif isinstance ( names , str ): return self . assign_stmt ( names , values ) else : assert isinstance ( names , Sequence ) values = tuple ( values ) if len ( names ) != len ( values ): raise self . error ( \"Number of return values does not match when unpacking\" ) returns = [] for name , value in zip ( names , values ): returns . append ( self . unpack_assign_stmt ( name , value )) return tuple ( returns )","title":"unpack_assign_stmt()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.while_stmt","text":"While statement staging tool. Source code in freetensor/core/frontend.py def while_stmt ( self , fpred , body ): '''While statement staging tool.''' first_pred = fpred () if isinstance ( first_pred , StagedPredicate ): first_pred . while_stmt ( body ) else : if first_pred : try : body () except BreakException : return except ContinueException : pass while fpred (): try : body () except BreakException : break except ContinueException : continue","title":"while_stmt()"},{"location":"api/#freetensor.core.frontend.LifetimeScope","text":"This scope is used to register multiple scopes inside a single lifetime scope. The inner scopes might be used to register variables, etc. They will be exited in reverse order of their registration. Source code in freetensor/core/frontend.py class LifetimeScope : '''This scope is used to register multiple scopes inside a single lifetime scope. The inner scopes might be used to register variables, etc. They will be exited in reverse order of their registration. ''' def __init__ ( self ): self . inner_scopes = [] def __enter__ ( self ): _overload . lifetime_stack . append ( self ) def __exit__ ( self , exc_type , exc_val , exc_tb ): for scope in reversed ( self . inner_scopes ): scope . __exit__ ( exc_type , exc_val , exc_tb ) popped = _overload . lifetime_stack . pop () if popped != self : raise _overload . error ( 'LifetimeScope enter/exit not match, must be FILO' ) def register_inner_scope ( self , scope ): self . inner_scopes . append ( scope ) return scope . __enter__ ()","title":"LifetimeScope"},{"location":"api/#freetensor.core.frontend.PredefinedVarCreator","text":"Source code in freetensor/core/frontend.py class PredefinedVarCreator ( VarCreator ): def __init__ ( self , initializer : List [ Any ], dtype : str , mtype : str ): def get_shape ( lst ): if not isinstance ( lst , list ): assert ndim ( lst ) == 0 return () if len ( lst ) == 0 : return ( 0 ,) shape_ = get_shape ( lst [ 0 ]) for x in lst [ 1 :]: assert shape_ == get_shape ( x ) return ( len ( lst ),) + shape_ super () . __init__ ( get_shape ( initializer ), dtype , mtype ) self . initializer = initializer def assign ( self , name : str ) -> VarRef : var = super () . assign ( name ) def impl ( var_slice , init_slice ): if not isinstance ( init_slice , list ): var_slice [()] = init_slice else : for i , x in enumerate ( init_slice ): impl ( var_slice [ i ], x ) impl ( var , self . initializer ) return var","title":"PredefinedVarCreator"},{"location":"api/#freetensor.core.frontend.PredefinedVarCreator.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in freetensor/core/frontend.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls )","title":"__class__"},{"location":"api/#freetensor.core.frontend.PredefinedVarCreator.__class__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in freetensor/core/frontend.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/#freetensor.core.frontend.PredefinedVarCreator.__class__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in freetensor/core/frontend.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/#freetensor.core.frontend.PredefinedVarCreator.__class__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in freetensor/core/frontend.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/#freetensor.core.frontend.PredefinedVarCreator.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/frontend.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/#freetensor.core.frontend.PredefinedVarCreator.__init__","text":"Initialize self. See help(type(self)) for accurate signature. Source code in freetensor/core/frontend.py def __init__ ( self , initializer : List [ Any ], dtype : str , mtype : str ): def get_shape ( lst ): if not isinstance ( lst , list ): assert ndim ( lst ) == 0 return () if len ( lst ) == 0 : return ( 0 ,) shape_ = get_shape ( lst [ 0 ]) for x in lst [ 1 :]: assert shape_ == get_shape ( x ) return ( len ( lst ),) + shape_ super () . __init__ ( get_shape ( initializer ), dtype , mtype ) self . initializer = initializer","title":"__init__()"},{"location":"api/#freetensor.core.frontend.PredefinedVarCreator.assign","text":"Customized assign behavior. Creates a VarDef with its full name. Source code in freetensor/core/frontend.py def assign ( self , name : str ) -> VarRef : var = super () . assign ( name ) def impl ( var_slice , init_slice ): if not isinstance ( init_slice , list ): var_slice [()] = init_slice else : for i , x in enumerate ( init_slice ): impl ( var_slice [ i ], x ) impl ( var , self . initializer ) return var","title":"assign()"},{"location":"api/#freetensor.core.frontend.Var","text":"Source code in freetensor/core/frontend.py class Var ( StagedTypeAnnotation ): def __init__ ( self , shape , dtype , atype = \"input\" , mtype = None ): ''' Declare a variable Parameters ---------- name : str Name of the variable shape : Sequence[Expr] or Var Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype : str or DataType Data type of the variable atype : str or AccessType Access type of the variable. It specifies whether (and how) the variable is an I/O variable of the function it belongs to. Defaults to \"input\" mtype : str or MemType (Optional) Memory type of the variable. If omitted, the main memory type of the default Target in config will be used ''' self . shape , self . dtype , self . atype , self . mtype = shape , dtype , atype , mtype def annotate ( self , name : str ) -> VarRef : return _overload . register_vardef ( name , self . shape , self . dtype , self . atype , self . mtype )","title":"Var"},{"location":"api/#freetensor.core.frontend.Var.__class__","text":"Source code in freetensor/core/frontend.py class StagedTypeAnnotationMeta ( abc . ABCMeta ): def __getitem__ ( self , args ): return self ( * args )","title":"__class__"},{"location":"api/#freetensor.core.frontend.Var.__class__.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in freetensor/core/frontend.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls ) __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). Source code in freetensor/core/frontend.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. Source code in freetensor/core/frontend.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). Source code in freetensor/core/frontend.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/frontend.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"__base__"},{"location":"api/#freetensor.core.frontend.Var.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/frontend.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/#freetensor.core.frontend.Var.__init__","text":"Declare a variable Parameters: name ( str ) \u2013 Name of the variable shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable atype ( str or AccessType ) \u2013 Access type of the variable. It specifies whether (and how) the variable is an I/O variable of the function it belongs to. Defaults to \"input\" mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used Source code in freetensor/core/frontend.py def __init__ ( self , shape , dtype , atype = \"input\" , mtype = None ): ''' Declare a variable Parameters ---------- name : str Name of the variable shape : Sequence[Expr] or Var Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype : str or DataType Data type of the variable atype : str or AccessType Access type of the variable. It specifies whether (and how) the variable is an I/O variable of the function it belongs to. Defaults to \"input\" mtype : str or MemType (Optional) Memory type of the variable. If omitted, the main memory type of the default Target in config will be used ''' self . shape , self . dtype , self . atype , self . mtype = shape , dtype , atype , mtype","title":"__init__()"},{"location":"api/#freetensor.core.frontend.VarCreator","text":"VarCreator(shape: Union[Sequence, freetensor.core.expr.VarRef], dtype: str, mtype: str, assigned: bool = False) Source code in freetensor/core/frontend.py @dataclass class VarCreator ( StagedAssignable ): shape : Union [ Sequence , VarRef ] dtype : str mtype : str assigned : bool = False def assign ( self , name : str ) -> VarRef : '''Customized assign behavior. Creates a VarDef with its full name.''' if not self . assigned : self . assigned = True return _overload . register_vardef ( name , self . shape , self . dtype , 'cache' , self . mtype ) else : raise _overload . error ( \"Create new tensors in an `a = b = c`-like multi-assignment \" \"is not supported\" )","title":"VarCreator"},{"location":"api/#freetensor.core.frontend.VarCreator.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in freetensor/core/frontend.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls )","title":"__class__"},{"location":"api/#freetensor.core.frontend.VarCreator.__class__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in freetensor/core/frontend.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/#freetensor.core.frontend.VarCreator.__class__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in freetensor/core/frontend.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/#freetensor.core.frontend.VarCreator.__class__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in freetensor/core/frontend.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/#freetensor.core.frontend.VarCreator.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/frontend.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/#freetensor.core.frontend.VarCreator.assign","text":"Customized assign behavior. Creates a VarDef with its full name. Source code in freetensor/core/frontend.py def assign ( self , name : str ) -> VarRef : '''Customized assign behavior. Creates a VarDef with its full name.''' if not self . assigned : self . assigned = True return _overload . register_vardef ( name , self . shape , self . dtype , 'cache' , self . mtype ) else : raise _overload . error ( \"Create new tensors in an `a = b = c`-like multi-assignment \" \"is not supported\" )","title":"assign()"},{"location":"api/#freetensor.core.frontend.dynamic_range","text":"Dynamic range that generates For loop in IR tree. Source code in freetensor/core/frontend.py class dynamic_range ( StagedIterable ): '''Dynamic range that generates For loop in IR tree.''' def __init__ ( self , start , stop = None , step = 1 ) -> None : '''Initialize a dynamic range. Arguments semantic identical to builtin `range`.''' if stop : self . start = start self . stop = stop else : self . start = 0 self . stop = start self . step = step def foreach ( self , name , body : Callable [[ Any ], None ]) -> None : '''Customized foreach behavior. Creates a For loop.''' if not isinstance ( name , str ): raise _overload . error ( 'dynamic_range only supports exactly one target variable' ) # Early optimizations if isinstance ( self . start , Number ) and isinstance ( self . stop , Number ) and isinstance ( self . step , Number ): if not range ( self . start , self . stop , self . step ): return if len ( range ( self . start , self . stop , self . step )) == 1 : with LifetimeScope (): body ( self . start ) return with _overload . allow_shortcut_scope ( False ): with For ( _overload . fullname ( name ), self . start , self . stop , self . step ) as iter_var : with LifetimeScope (): body ( iter_var )","title":"dynamic_range"},{"location":"api/#freetensor.core.frontend.dynamic_range.__init__","text":"Initialize a dynamic range. Arguments semantic identical to builtin range . Source code in freetensor/core/frontend.py def __init__ ( self , start , stop = None , step = 1 ) -> None : '''Initialize a dynamic range. Arguments semantic identical to builtin `range`.''' if stop : self . start = start self . stop = stop else : self . start = 0 self . stop = start self . step = step","title":"__init__()"},{"location":"api/#freetensor.core.frontend.dynamic_range.foreach","text":"Customized foreach behavior. Creates a For loop. Source code in freetensor/core/frontend.py def foreach ( self , name , body : Callable [[ Any ], None ]) -> None : '''Customized foreach behavior. Creates a For loop.''' if not isinstance ( name , str ): raise _overload . error ( 'dynamic_range only supports exactly one target variable' ) # Early optimizations if isinstance ( self . start , Number ) and isinstance ( self . stop , Number ) and isinstance ( self . step , Number ): if not range ( self . start , self . stop , self . step ): return if len ( range ( self . start , self . stop , self . step )) == 1 : with LifetimeScope (): body ( self . start ) return with _overload . allow_shortcut_scope ( False ): with For ( _overload . fullname ( name ), self . start , self . stop , self . step ) as iter_var : with LifetimeScope (): body ( iter_var )","title":"foreach()"},{"location":"api/#freetensor.core.frontend.capture_var","text":"Capture external array as tensor variable. Source code in freetensor/core/frontend.py def impl ( * args , ** kwargs ): if _overload . in_staging (): return staging ( * args , ** kwargs ) else : return original ( * args , ** kwargs )","title":"capture_var()"},{"location":"api/#freetensor.core.frontend.empty","text":"Create an empty variable Parameters: shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used Source code in freetensor/core/frontend.py def impl ( * args , ** kwargs ): if _overload . in_staging (): return staging ( * args , ** kwargs ) else : return original ( * args , ** kwargs )","title":"empty()"},{"location":"api/#freetensor.core.frontend.inline","text":"Enable a user function to be called by a transformed function at run time Parameters: func ( Python function ) \u2013 The user function src ( str (Optional) ) \u2013 The source code of func . This parameter is only required if the source code cannot be get automatically, e.g., if func is generated from a exec default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( bool ) \u2013 True to print the generated Python code that is used for transforming Source code in freetensor/core/frontend.py def inline ( func = None , src = None , fallback = None , default_dynamic_range = True , verbose = False ): ''' Enable a user function to be called by a transformed function at run time Parameters ---------- func : Python function The user function src : str (Optional) The source code of `func`. This parameter is only required if the source code cannot be get automatically, e.g., if `func` is generated from a `exec` default_dynamic_range : bool If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose : bool True to print the generated Python code that is used for transforming ''' if func is None : return functools . partial ( inline , src = src , fallback = fallback , default_dynamic_range = default_dynamic_range , verbose = verbose ) extra_locals = _prepare_extra_locals ( default_dynamic_range ) # Do not initialize _overload here, since `into_staging` does not use the context. # Keep the context as-is to support adding new inline functions during transforming. # Such a case occurs when a transformed function dynamically imports a new inline. transformed = _overload . into_staging ( func , extra_locals , src , verbose = verbose ) return functools . wraps ( func )( staged_callable ( transformed , fallback or func ))","title":"inline()"},{"location":"api/#freetensor.core.frontend.transform","text":"Transform a user function to an AST Parameters: func ( Python function ) \u2013 The user function to transform. If not specified, a partial function will be returend, which can be used as a decorator default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( int ) \u2013 0 = print nothing. 1 = print the resulting AST. 2 = 1 + print the generated Python code that is used for transforming Source code in freetensor/core/frontend.py def transform ( func = None , default_dynamic_range = True , verbose : int = 0 ): ''' Transform a user function to an AST Parameters ---------- func : Python function The user function to transform. If not specified, a partial function will be returend, which can be used as a decorator default_dynamic_range : bool If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose : int 0 = print nothing. 1 = print the resulting AST. 2 = 1 + print the generated Python code that is used for transforming ''' if func is None : return functools . partial ( transform , default_dynamic_range = default_dynamic_range , verbose = verbose ) if verbose is None : verbose = 0 extra_locals = _prepare_extra_locals ( default_dynamic_range ) params = list ( inspect . signature ( func ) . parameters ) staging_func = _overload . into_staging ( func , extra_locals , verbose = verbose >= 2 ) try : # Initialize _overload to prepare for staging. _overload . __init__ () # Create a new scope for the function with LifetimeScope (): # Run staging function with the tensor program arguments' names as parameters returns = staging_func ( * params , __freetensor_transform_outermost__ = True ) # Check returned vardefs (if any) if isinstance ( returns , VarRef ): returns = [ returns ] elif isinstance ( returns , tuple ): for ret in returns : if not isinstance ( ret , VarRef ): raise _overload . error ( 'Illegal return at top level, need to be a `VarRef` or a tuple of `VarRef`s' ) returns = list ( returns ) elif returns is None : returns = [] else : raise _overload . error ( 'Illegal return at top level, need to be a `VarRef` or a tuple of `VarRef`s' ) # Set returned vardefs' access type to inout/output according to whether it was an input for ret in returns : if ret . vardef . atype == 'input' or ret . vardef . atype == 'inout' : ret . vardef . set_atype ( 'inout' ) else : ret . vardef . set_atype ( 'output' ) returns = [( ret . vardef . name , ret . vardef . dtype ) for ret in returns ] # Set closure; they are from captured Arrays. closure = _overload . closure except StagingError : raise except TransformError : raise except Exception as e : raise _overload . error ( 'Exception occurred in staging' ) from e finally : # Despite whether the exception is raised, we need to clean up the ctx_stack staged_ast = pop_ast () staged = None # Enable invoking a transformed AST in another function being transformed, # via `inlined_invoke` def prepare_inlined_invoke ( * args , ** kvs ): nonlocal staged if _overload . in_staging (): if len ( returns ) == 1 : names = ( func . __name__ ,) else : names = tuple ( f \" { func . __name__ } . { i } \" for i in range ( len ( returns ))) return _overload . register_inlined_invoke ( names , staged , args , kvs ) else : raise _overload . error ( 'Unexpected call on a transformed AST. A transformed AST can only ' 'be called in the following two ways: 1) called with actual data ' 'after `@optimize`, and 2) called from another function to be ' '`@transform`ed' ) staged = Func ( func . __name__ , params + list ( closure . keys ()), returns , staged_ast , closure , custom_callback = prepare_inlined_invoke ) if verbose >= 1 : print ( \"The transformed AST is:\" , file = sys . stderr ) print ( staged , file = sys . stderr ) print ( file = sys . stderr ) return staged","title":"transform()"},{"location":"api/#freetensor.core.frontend.var","text":"Create an with variable a given initializer Parameters: initializer ( Sequence[Sequence[...Sequence[Expr]...]] ) \u2013 (Multi-level of) sequence of expressions. Will be data of the variable shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used Source code in freetensor/core/frontend.py def impl ( * args , ** kwargs ): if _overload . in_staging (): return staging ( * args , ** kwargs ) else : return original ( * args , ** kwargs )","title":"var()"},{"location":"api/#freetensor.core.optimize","text":"","title":"optimize"},{"location":"api/#freetensor.core.optimize.optimize","text":"An one-click optimization from Python function to binary executable Usage: @optimize def f(...): ... It is equivalent to: @build_binary @codegen @lower @transform def f(...): ... Parameters: func ( Python function or AST ) \u2013 The user function to optimize. If not specified, a partial function will be returend, which can be used as a decorator schedule_callback ( Optional[Callable[[freetensor.core.schedule.Schedule], NoneType]] ) \u2013 Schedule(s) to apply target ( Optional[freetensor_ffi.Target] ) \u2013 The target architecture. You don't have to set target if you set device device ( Optional[freetensor.core.driver.Device] ) \u2013 Where to run the program default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( Optional[int] ) \u2013 Verbosity level. Can be 0, 1 or 2 Source code in freetensor/core/optimize.py def optimize ( func = None , schedule_callback : Optional [ Callable [[ Schedule ], None ]] = None , target : Optional [ Target ] = None , device : Optional [ Device ] = None , default_dynamic_range : bool = True , verbose : Optional [ int ] = None ): ''' An one-click optimization from Python function to binary executable Usage: ``` @optimize def f(...): ... ``` It is equivalent to: ``` @build_binary @codegen @lower @transform def f(...): ... ``` Parameters ---------- func : Python function or AST The user function to optimize. If not specified, a partial function will be returend, which can be used as a decorator schedule_callback : Callable (Optional) Schedule(s) to apply target : Target (Optional) The target architecture. You don't have to set target if you set device device : Device (Optional) Where to run the program default_dynamic_range : bool If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose : int (Optional) Verbosity level. Can be 0, 1 or 2 ''' if func is not None : if target is None and device is not None : target = device . target () if not issubclass ( type ( func ), ffi . AST ): ast = transform ( func , default_dynamic_range = default_dynamic_range , verbose = verbose ) else : ast = func ast = schedule ( ast , schedule_callback , verbose = verbose ) ast = lower ( ast , target , verbose = verbose ) code = codegen ( ast , target , verbose = verbose ) exe = build_binary ( code , device , verbose = verbose ) return exe else : return functools . partial ( optimize , schedule_callback = schedule_callback , target = target , device = device , default_dynamic_range = default_dynamic_range , verbose = verbose )","title":"optimize()"},{"location":"api/#freetensor.core.optimize.optimize_to_pytorch","text":"Compile a FreeTensor function to a PyTorch call, whose gradient can be recognized by PyTorch The compiled function will be a typical PyTorch's \"function\" (rather than a PyTorch's \"module\"). Technically, this means it is a wrapper function around a PyTorch's Function 's apply method Schedules (if any) must be applied to the forward function and the backward function separated. For this reason, currently only first-order gradient is supported Parameters: func ( Python function or AST ) \u2013 The user function to optimize. If not specified, a partial function will be returend, which can be used as a decorator tapes ( Union[Sequence, freetensor_ffi.GradTapeMode] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history forward_schedule_callback ( Optional[Callable[[freetensor.core.schedule.Schedule], NoneType]] ) \u2013 Schedule(s) to apply to the forward function backward_schedule_callback ( Optional[Callable[[freetensor.core.schedule.Schedule], NoneType]] ) \u2013 Schedule(s) to apply to the backward function target ( Optional[freetensor_ffi.Target] ) \u2013 The target architecture. You don't have to set target if you set device device ( Optional[freetensor.core.driver.Device] ) \u2013 Where to run the program default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( Optional[int] ) \u2013 Verbosity level. Can be 0, 1 or 2 Source code in freetensor/core/optimize.py def optimize_to_pytorch ( func = None , tapes : Union [ Sequence , GradTapeMode ] = GradTapeMode . NoReuseOnly , forward_schedule_callback : Optional [ Callable [[ Schedule ], None ]] = None , backward_schedule_callback : Optional [ Callable [[ Schedule ], None ]] = None , target : Optional [ Target ] = None , device : Optional [ Device ] = None , default_dynamic_range : bool = True , verbose : Optional [ int ] = None ): ''' Compile a FreeTensor function to a PyTorch call, whose gradient can be recognized by PyTorch The compiled function will be a typical PyTorch's \"function\" (rather than a PyTorch's \"module\"). Technically, this means it is a wrapper function around a PyTorch's `Function`'s `apply` method Schedules (if any) must be applied to the forward function and the backward function separated. For this reason, currently only first-order gradient is supported Parameters ---------- func : Python function or AST The user function to optimize. If not specified, a partial function will be returend, which can be used as a decorator tapes : Union[Sequence, GradTapeMode] Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a `GradTapeMode`, then it will determine which intermediate variables to be stored by heuristics. Avail `GradTapeMode`s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history forward_schedule_callback : Callable (Optional) Schedule(s) to apply to the forward function backward_schedule_callback : Callable (Optional) Schedule(s) to apply to the backward function target : Target (Optional) The target architecture. You don't have to set target if you set device device : Device (Optional) Where to run the program default_dynamic_range : bool If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose : int (Optional) Verbosity level. Can be 0, 1 or 2 ''' if func is not None : import torch # Transform from Python source to AST if not issubclass ( type ( func ), ffi . AST ): ast = transform ( func , default_dynamic_range = default_dynamic_range , verbose = verbose ) else : ast = func # Compile lazily because we know `requires` and `provides` only when # executing. Re-compile when gradient requirements changes saved_requires = set () saved_provides = set () cur_requires = None cur_provides = None fwd_exe = None bwd_exe = None input_grad_map = None output_grad_map = None tape_rets = None def lazy_compile (): nonlocal saved_requires , saved_provides , cur_requires , cur_provides nonlocal fwd_exe , bwd_exe , input_grad_map , output_grad_map , tape_rets if saved_requires == cur_requires and saved_provides == cur_provides : return saved_requires = cur_requires saved_provides = cur_provides if len ( cur_requires ) != 0 : fwd_ast , bwd_ast , input_grad_map , output_grad_map = grad ( ast , requires = saved_requires , provides = saved_provides , tapes = tapes , # PyTorch requires explicitly marking saved states via # `save_for_backward()` tape_in_closure = False , verbose = verbose ) tape_rets = fwd_ast . returns [ len ( ast . returns ):] fwd_exe = optimize ( fwd_ast , forward_schedule_callback , target , device , default_dynamic_range , verbose ) bwd_exe = optimize ( bwd_ast , backward_schedule_callback , target , device , default_dynamic_range , verbose ) else : # No one needs grad. No need to do autograd fwd_ast = ast fwd_exe = optimize ( fwd_ast , forward_schedule_callback , target , device , default_dynamic_range , verbose ) bwd_exe = None input_grad_map = {} output_grad_map = {} tape_rets = [] # Generate a PyTorch Function class GeneratedPyTorchFunction ( torch . autograd . Function ): @staticmethod def forward ( ctx , * args , ** kvs ): nonlocal cur_requires , cur_provides # We only get to know provided gradients of output tensors when we # run `backward`, but we need to run autograd and compile the program # here in `forward`. We can only assume gradients are provided for # every output tensors, even if they are unrelated to the inputs. # Setting this option to True makes PyTorch generate zero gradient # for such outputs. (TODO: better solution?) ctx . set_materialize_grads ( True ) # Gather required gradients of the inputs cur_requires = set () for param , arg in zip ( ast . params , args ): if arg . requires_grad : cur_requires . add ( param . name ) for key , value in kvs . items (): if value . requires_grad : cur_requires . add ( key ) # For the reason above, we assume gradients are provided for every # output tensors cur_provides = set () for ret in ast . returns : cur_provides . add ( ret . name ) lazy_compile () fwd_exe . set_args ( * args , ** kvs ) fwd_exe . run () returns = fwd_exe . collect_returns ( always_return_pack = True ) returns = tuple ( item . torch () for item in returns ) # Save states for 1) all inputs and 2) all taped tensors (taped # outputs are also taped tensors). For taped tensors, we need to # make them output tensors, so PyTorch can recognize them. This is # an officially recommanded trick at # https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html#saving-intermediate-results # So, please be aware that only the first part in `returns` are real # return tensors saved_tensors = [] for arg in args : # 1) saved_tensors . append ( arg ) for ret in returns : # 2) and maybe other junks saved_tensors . append ( ret ) ctx . save_for_backward ( * saved_tensors ) return returns [ 0 ] if len ( returns ) == 1 else returns @staticmethod @torch . autograd . function . once_differentiable def backward ( ctx , * args , ** kvs ): saved_tensors = ctx . saved_tensors internal_kvs = {} for ret , arg in zip ( ast . returns , args ): internal_kvs [ output_grad_map [ ret . name ]] = arg for key , value in kvs : internal_kvs [ output_grad_map [ key ]] = value for param , saved in zip ( ast . params , saved_tensors ): # NOTE: Now we only support \"input\" parameters for PyTorch # interface (no \"inout\" or \"output\"), so we can forward all # parameters. If we support \"inout\" or \"output\" in the future, # we need to filter only \"input\" parameters here internal_kvs [ param . name ] = saved for tape_ret , saved in zip ( tape_rets , saved_tensors [ len ( ast . params ) + len ( ast . returns ):]): internal_kvs [ tape_ret . name ] = saved bwd_exe . set_args ( ** internal_kvs ) bwd_exe . run () input_grads = bwd_exe . collect_returns ( always_return_pack = True ) # PyTorch requires returning gradient of inputs in their original # order. If no gradient is required for an input, set it to None returns = tuple ( input_grads [ input_grad_map [ param . name ]] . torch ( ) if param . name in input_grad_map else None for param in ast . params ) return returns [ 0 ] if len ( returns ) == 1 else returns # Wrap around the PyTorch `Function`, to be a real Python \"function\", and # remove our extra tape outputs def generatedPyTorchFunction ( * args , ** kvs ): returns = GeneratedPyTorchFunction . apply ( * args , ** kvs ) returns_tuple = returns if isinstance ( returns , Sequence ) else ( returns ,) returns_tuple = returns_tuple [: len ( ast . returns )] return returns_tuple [ 0 ] if len ( returns_tuple ) == 1 else returns_tuple # If called inside a FreeTensor funcion, don't care about PyTorch, just # inline the transformed AST return staged_callable ( ast , generatedPyTorchFunction ) else : return functools . partial ( optimize_to_pytorch , tapes = tapes , forward_schedule_callback = forward_schedule_callback , backward_schedule_callback = backward_schedule_callback , target = target , device = device , default_dynamic_range = default_dynamic_range , verbose = verbose )","title":"optimize_to_pytorch()"},{"location":"api/#freetensor.core.passes","text":"","title":"passes"},{"location":"api/#freetensor.core.passes.lower","text":"Lower an AST using a series of passes Parameters: ast ( AST ) \u2013 The AST to be lowered. Can be a Func or a Stmt . If not specified, a partial function of lower will be returned, which can be used as a decorator target ( Optional[freetensor_ffi.Target] ) \u2013 Lower the AST to a target with target-specific passes, then the AST can be used for codegen. If not set, use the default Target in Config skip_passes ( Optional[Sequence[str]] ) \u2013 Skip some pass for testing or debugging. Names in skip_passes are in underscore_style, as in Python. Please note that some passes will not be skipped even specified in these parameter, because they are indirectly called in some other passes verbose ( Optional[int] ) \u2013 0 = print nothing. 1 = print the lowered AST. 2 = print AST after every single passes Source code in freetensor/core/passes.py def lower ( ast = None , target : Optional [ ffi . Target ] = None , skip_passes : Optional [ Sequence [ str ]] = None , verbose : Optional [ int ] = None ): ''' Lower an AST using a series of passes Parameters ---------- ast : AST The AST to be lowered. Can be a `Func` or a `Stmt`. If not specified, a partial function of `lower` will be returned, which can be used as a decorator target : Target (Optional) Lower the AST to a target with target-specific passes, then the AST can be used for codegen. If not set, use the default Target in Config skip_passes : Sequence[str] (Optional) Skip some pass for testing or debugging. Names in `skip_passes` are in underscore_style, as in Python. Please note that some passes will not be skipped even specified in these parameter, because they are indirectly called in some other passes verbose : int (Optional) 0 = print nothing. 1 = print the lowered AST. 2 = print AST after every single passes ''' if ast is not None : return ffi . lower ( ast , target , set () if skip_passes is None else set ( skip_passes ), 0 if verbose is None else verbose ) else : _lower = lower if target is not None : _lower = functools . partial ( _lower , target = target ) if skip_passes is not None : _lower = functools . partial ( _lower , skip_passes = skip_passes ) if verbose is not None : _lower = functools . partial ( _lower , verbose = verbose ) return _lower","title":"lower()"},{"location":"api/#freetensor.core.schedule","text":"","title":"schedule"},{"location":"api/#freetensor.core.schedule.IDMap","text":"A dict-like container recording an ID-to-ID mapping, representing what IDs become what IDs after a schedule An IDMap can be looked up by numerical ID, or by Stmt instances or Selector strings of the original (before applying schedule) AST Source code in freetensor/core/schedule.py class IDMap : ''' A dict-like container recording an ID-to-ID mapping, representing what IDs become what IDs after a schedule An IDMap can be looked up by numerical ID, or by Stmt instances or Selector strings of the original (before applying schedule) AST ''' def __init__ ( self , old_ast , id_map : Dict [ ID , ID ]): self . old_ast = old_ast self . id_map = id_map def _lookup ( self , pattern : Union [ ID , ffi . Stmt , Selector , str ]) -> ID : if isinstance ( pattern , ID ): return pattern elif isinstance ( pattern , ffi . Stmt ): return pattern . id else : return find_stmt ( self . old_ast , Selector ( pattern )) . id def __contains__ ( self , key ): return self . _lookup ( key ) in self . id_map def __getitem__ ( self , key ): return self . id_map [ self . _lookup ( key )] def __iter__ ( self ): return iter ( self . map )","title":"IDMap"},{"location":"api/#freetensor.core.schedule.Schedule","text":"Source code in freetensor/core/schedule.py class Schedule ( ffi . Schedule ): def _lookup ( self , pattern : Union [ ID , ffi . Stmt , Selector , str ]) -> ID : if isinstance ( pattern , ID ): return pattern elif isinstance ( pattern , ffi . Stmt ): return pattern . id else : return self . find ( Selector ( pattern )) . id def _lookup_list ( self , pattern : Union [ ID , List [ ID ], ffi . Stmt , List [ ffi . Stmt ], Selector , List [ Selector ], str , List [ str ]] ) -> List [ ID ]: if isinstance ( pattern , Sequence ) and not isinstance ( pattern , str ): return functools . reduce ( lambda x , y : x + y , map ( self . _lookup_list , pattern )) elif isinstance ( pattern , ID ): return [ pattern ] elif isinstance ( pattern , ffi . Stmt ): return [ pattern . id ] else : return [ item . id for item in self . find_at_least_one ( Selector ( pattern )) ] def __init__ ( self , arg , verbose : int = 0 ): if isinstance ( arg , ffi . Schedule ): # from native Schedule object super () . __init__ ( arg ) else : # create a new schedule from a program super () . __init__ ( arg , verbose ) def ast ( self ): \"\"\" Get the scheduled AST without function signature This is mainly for debugging and testting purpose \"\"\" ret = super () . ast () if self . verbose >= 1 : print ( f \"The scheduled AST is: \\n { ret } \" ) return ret def func ( self ): \"\"\" Get the scheduled function \"\"\" ret = super () . func () if self . verbose >= 1 : print ( f \"The scheduled Func is: \\n { ret } \" ) return ret def fork ( self ): return Schedule ( super () . fork ()) def split ( self , node , factor =- 1 , nparts =- 1 , shift = 0 ): \"\"\" Split a loop into two nested loops To fission a loop into two consecutive loops, use `fission` instead Two modes are provided: 1. Specify `factor` and leave `nparts` to -1. It will result in an outer loop with length `ceil(n / factor)`, and an inner loop with length `factor`, where `n` is the original loop length added by `shift`. The original iterator `i` will be transformed to `i0 * factor + i1`, where `i0` and `i1` are the iterators of the new outer and inner loops, respectively 2. Specify `nparts` and leave `factor` to -1. It will result in an outer loop with length `nparts`, and an inner loop with length `ceil(n / nparts)`, where `n` is the original loop length added by `shift`. The original iterator `i` will be transformed to `i0 * ceil(n / nparts) + i1`, where `i0` and `i1` are the iterators of the new outer and inner loops, respectively Please note that the second mode will introduce an `i0 * ceil(n / nparts)` factor into the program, which cannot be recognized by polyhedral analysis, which may hinder some following schedules. If possible, plese use the first mode, and then reorder the inner and outer loops Suppose the original loop is labeled \"L\", the split two loops can be selected by \"$split.0{L}\" (the outer loop) and \"$split.1{L}\" (the inner loop). If one of the resulting loop is proved to have only a single iteration, it will be removed Parameters ---------- node : str, ID or Stmt The loop to be split factor : int Length of the inner loop. Set to -1 if using `nparts` nparts : int Length of the outer loop. Set to -1 if using `factor` Raises ------ InvalidSchedule if the loop is not found Returns ------- (Optional[ID], Optional[ID]) (outer loop ID, inner loop ID), either ID can be None if the loop is proved to have only a single iteration \"\"\" return ( i if i else None for i in super () . split ( self . _lookup ( node ), factor , nparts , shift )) def reorder ( self , order ): \"\"\" Reorder directly nested loops To swap consecutive loops, use `swap` instead Parameters ---------- order : array like of str, ID or Stmt Vector of loops. The requested order of the loops Raises ------ InvalidSchedule if the input is invalid or there are breaking dependences \"\"\" super () . reorder ( list ( map ( self . _lookup , order ))) def merge ( self , loop1 , loop2 ): \"\"\" Merge two directly nested loops into one To fuse consecutive loops, use `fuse` instead `parallelize`, `unroll` and `vectorize` properties will be reset on the merged loop Suppose the original loops are labeled \"L1\" and \"L2\", the merged loop can be selected by \"$merge{L1, L2}\" Parameters ---------- loop1, loop2 : str, ID or Stmt loops to be merged, can be in any order Raises ------ InvalidSchedule if the loops are not directly nested Returns ------- ID ID of the merged loop \"\"\" return super () . merge ( self . _lookup ( loop1 ), self . _lookup ( loop2 )) def permute ( self , loops , transform_func ): \"\"\" Permute perfectly nested loops (directly nested loops without statements in between) with the given loop space transformation function The transformed loops follow ascending lexical order of the transformed terms returned by `transformFunc` when called with original iteration Parameters ---------- loops : array like of str, ID or Stmt the list of perfectly nested loops to be permuted transform_func : Callable[[Expr], Expr] the loop space transformation function, should be bijective Returns ------- list of ID the list of IDs of permuted loops \"\"\" return super () . permute ([ self . _lookup ( l ) for l in loops ], transform_func ) def fission ( self , loop , side , splitter ): \"\"\" Fission a loop into two loops each containing part of the statements, one followed by another To split loop into two nested loops, use `split` instead Statements inside the original loop will be distributed to one or both (happening if they are scope statements) loops. If a statement is originally labeled \"S\", it can be selected by \"$fission.0{S}\" (from the first loop) or \"$fission.1{S}\" (from the second loop) after fission. If one of the resulting loop has an empty body, it will be removed Parameters ---------- loop : str, ID or Stmt The loop to be fissioned side : FissionSide If `After`, `splitter` is the last statement of the first loop. If `Before`, `splitter` is the first statement of the second loop splitter : str (Selector string), ID, Stmt, or list of them Where to fission the loop. If multiple statement are selected, fission the look before or after all of them Raises ------ InvalidSchedule if any dependence cannot be resolved Returns ------- (IDMap, IDMap) ({old ID -> new ID in 1st loop}, {old ID -> new ID in 2nd loop}). If a loop is removed because it has an empty body, it will not be in the returned map \"\"\" old_ast = self . ast () splitter_list = self . _lookup_list ( splitter ) # In DFS order if side == FissionSide . Before : splitter = splitter_list [ 0 ] else : splitter = splitter_list [ - 1 ] map1 , map2 = super () . fission ( self . _lookup ( loop ), side , splitter ) return IDMap ( old_ast , map1 ), IDMap ( old_ast , map2 ) def fuse ( self , loop0 , loop1 = None , strict = False ): \"\"\" Fuse two directly following loops with the same length into one To merge nested loops into one, use `merge` instead `parallelize`, `unroll` and `vectorize` properties will be reset on the fused loop Suppose the original loops are labeled \"L1\" and \"L2\", the fused loop can be selected by \"$fuse{L1, L2}\" Parameters ---------- loop0 : str, ID or Stmt The leading loop loop1 : str, ID or Stmt, Optional The following loop. If omitted, it will try to find a following loop of `loop0` strict : bool False by default. If set to True, throw an error if unable to determine whether the two loops are of the same length Raises ------ InvalidSchedule if the two loops are not directly following, the two loops are not of the same length, or there is any dependence cannot be resolved Returns ------- ID ID of the result loop \"\"\" if loop1 is None : return super () . fuse ( self . _lookup ( loop0 ), strict ) else : return super () . fuse ( self . _lookup ( loop0 ), self . _lookup ( loop1 ), strict ) def swap ( self , order ): \"\"\" Swap statements in the same block To reorder nested loops, use `reorder` instead Parameters ---------- order : List[str (Selector string), ID, List[ID], Stmt, or List[Stmt]] The statements. If one item of the `order` list contains multiple statements, the `order` list will be flattened Raises ------ InvalidSchedule if the statements are not found or the dependences cannot be solved \"\"\" super () . swap ( self . _lookup_list ( order )) def blend ( self , loop ): \"\"\" Unroll a loop and interleave statements from each iteration E.g. ``` for i = 0 to 2 { f(i); g(i); } ``` will be transformed to be ``` f(0); f(1); g(0); g(1); ``` Virtual threads in TVM can be implemented via blend Parameters ---------- loop : str, ID or Stmt The loop being transformed Raises ------ InvalidSchedule if the loop is not found, the loop length is not a constant, or the dependences cannot be solved \"\"\" super () . blend ( self . _lookup ( loop )) def cache ( self , stmt , var , mtype ): \"\"\" Cache a variable into a new local variable All needed data will be filled into the cache first, then all reads and writes will be directed to the cache, and finally all needed data will be flushed from the cache Note for reduction: This transformation preserves the computation order. It will transform ``` a += x a += y ``` to ``` a.cache = a + x + y a = a.cache ``` If you need a \"real\" cache for reduction, which reorders the computation, use `cache_reduction` instead Parameters ---------- stmt : str, ID or Stmt The statement or block (e.g. an If or a For) to be modified var : str Name of the variable to be cached mtype : MemType Where to cache Raises ------ InvalidSchedule if the ID or name is not found Returns ------- (ID, ID, ID, ID) (ID of the statement that fills the cache, ID of the statement that flushes from the cache, name of the cache variable, ID of the VarDef node of the cache variable) \"\"\" return super () . cache ( self . _lookup ( stmt ), var , MemType ( mtype )) def cache_reduction ( self , stmt , var , mtype ): \"\"\" Perform local reductions (e.g. sum) in a local variable first, and then reduce the local result to the global variable E.g. ``` a += x a += y ``` will be transformed to be ``` a.cache = x + y a += a.cache ``` Parameters ---------- stmt : str, ID or Stmt The statement or block (e.g. an If or a For) to be modified var : str Name of the variable to be cached. Only reductions are allowed on `var` in `stmt`. Plain reads or writes are not allowed mtype : MemType Where to cache Raises ------ InvalidSchedule if the ID or name is not found, or there are unsupported reads or writes Returns ------- (ID, ID, ID, ID) (ID of the statement that initialize the cache, ID of the statement that reduces the local result to the global result, name of the cache variable, ID of the VarDef node of the cache variable) \"\"\" return super () . cache_reduction ( self . _lookup ( stmt ), var , MemType ( mtype )) def set_mem_type ( self , vardef , mtype ): \"\"\" Change where a variable is stored Parameters ---------- vardef : str, ID or Stmt ID of the VarDef statement of the specific variable mtype : MemType Where the variable should be stored Raises ------ InvalidSchedule if the variable is not found \"\"\" super () . set_mem_type ( self . _lookup ( vardef ), MemType ( mtype )) def var_split ( self , vardef , dim , mode , factor =- 1 , nparts =- 1 ): \"\"\" Split a dimension of a variable into two Parameters ---------- vardef : str, ID or Stmt ID of the VarDef statement of the specific variable dim : int which dimension to be split mode : VarSplitMode When the dimension to split is not divisible by `factor` or `nparts`, the resulting shape may become larger. In `FixedSize` mode, the actual buffer size will not be changed, and gurads will be added to prevent out-of-bound accesses. In `RelaxedSize` mode, the buffer size may increase. The `RelaxedSize` mode cannot be applied to I/O variables factor : int Length of the inner (higher no.) dimension. Set to -1 if using `nparts` nparts : int Length of the outer (lower no.) loop. Set to -1 if using `factor` Raises ------ InvalidSchedule if the variable or the dimension is not found \"\"\" return super () . var_split ( self . _lookup ( vardef ), dim , mode , factor , nparts ) def var_merge ( self , vardef , dim ): \"\"\" Merge two dimensions of a variable Parameters ---------- vardef : str, ID or Stmt ID of the VarDef statement of the specific variable dim : int Merge the `dim`-th and the `(dim + 1)`-th dimension \"\"\" return super () . var_merge ( self . _lookup ( vardef ), dim ) def var_reorder ( self , vardef , order ): \"\"\" Reorder the dimensions of a variable Parameters ---------- vardef : str, ID or Stmt ID of the VarDef statement of the specific variable order : array like of str, ID or Stmt Vector of integers. The new order of the dimensions Raises ------ InvalidSchedule if the variable or the order is illegal \"\"\" return super () . var_reorder ( self . _lookup ( vardef ), order ) def move_to ( self , stmt , side , dst ): \"\"\" Move a statement to a new position This is a composite schedule command, which is implemented with other commands If moving a statement out of some loops, identical loops will be added around the moved statement, which is equivalent to fission these loops Parameters ---------- stmt : str, ID or Stmt The statement to be moved side : MoveToSide Whether `stmt` will be BEFORE or AFTER `dst dst : str (Selector string), ID, Stmt, or list of them Insert `stmt` to be directly after this statement. If multiple statements are selected, move to before or after all of them Raises ------ InvalidSchedule if there is no feasible path to move Returns ------- (ID, ID) (The new ID of the moved statement, The out-most newly introduced statments including the added loops) \"\"\" dst_list = self . _lookup_list ( dst ) # In DFS order if side == MoveToSide . Before : dst = dst_list [ 0 ] else : dst = dst_list [ - 1 ] return super () . move_to ( self . _lookup ( stmt ), side , dst ) def inline ( self , vardef ): \"\"\" Remove a variable. When the variable is used, recompute its value Parameters ---------- vardef : str, ID or Stmt The VarDef statement of the specific variable. It can not be an I/O varible Raises ------ InvalidSchedule if the variable cannot be completely removed \"\"\" return super () . inline ( self . _lookup ( vardef )) def parallelize ( self , loop , parallel ): \"\"\" Mark a loop with a parallel implementation This schedule follows a fork-join model: multiple workers (abstract threads) are created (but physically the threads may be cached in a thread pool) when the loop begins, do their jobs in parallel, and join when the loop ends OpenMP threads follow a typical fork-join model. CUDA threads run in a bulk-synchronous parallel (BSP) model, which can also be mimiked by the fork-join model: All threads start when the kernel get launched, but they only begin to do their jobs when the parallel loop begins. Nevertheless, the fork-join model needs the following extension to fully mimic a BSP model: Taking CUDA as an example, we allow binding a loop to `threadIdx.x` inside another loop bound to `threadIdx.x`, which is illegal in a classic fork-join model. For example, we may implement a matmul with collaborative fetch as below: ``` for i : threadIdx.x # Li for j : threadIdx.y # Lj local_sum = 0 # In gpu/local memory, unique to (i, j) for k0 # Lk0 for k : threadIdx.y # Lk1_a A_cache[k] = A[i, k] # In gpu/shared, shared by different j for k : threadIdx.x # Lk1_b B_cache[k] = B[k, j] # In gpu/shared, shared by different i for k # Lk1_c sum += A_cache[k] * B_cache[k] C[i, j] = local_sum ``` A seemingly plausible solution to avoid this extension is to reorder `Lk0` to outer-most, and then move `Lk1_a` and `Lk1_b` out of `Li` or `Lj`. This resolves the nested `threadIdx.x` and `threadIdx.y` binding problem by running `Li+Lk1_a`, `Lj+Lk1_b` and `Li+Lj` interleavingly, instead of running `Lk1_a` and `Lk1_b` inside `Li+Lj`. However, this approach is illegal, because the local variable `local_sum` can no longer be kept inside the body of `Li` and `Lj`: It has to be reused across multiple runs of `Li` and `Lj` Please also note that we can bind one `threadIdx.x` to two loops only when the body statement is loop-invariant to one of them. For example, the following binding is still illegal, even in our extended fork-join model, because it violates its serial semantics: ``` for i : threadIdx.x for j : threadIdx.x A[i, j] ++ ``` Parameters ---------- loop : str, ID or Stmt The loop parallel : ParallelScope Parallel scope \"\"\" super () . parallelize ( self . _lookup ( loop ), ParallelScope ( parallel )) def unroll ( self , loop , immediate = False ): \"\"\" Unroll a loop Parameters ---------- loop : str, ID or Stmt ID of the loop immediate : bool If false (by default), postpone the unroll procedure to the backend compiler, which saves scheduling time. If true, unroll the loop immediately, which may help further simplifications based on the unrolled result. If your purpose is just to fill the instruction cache, set it to false. If you are unrolling a loop that computes array indices, set it to true Raises ------ InvalidSchedule if the loop is not found or length of the loop is not a constant \"\"\" super () . unroll ( self . _lookup ( loop ), immediate ) def vectorize ( self , loop ): \"\"\" Vectorize a loop Please note that, as vectorization is different from architecture to achitecture, the scheduler may or may not postpone it to the backend compiler. The vectorization is a best-effort schedule Parameters ---------- loop : str, ID or Stmt ID of the loop Raises ------ InvalidSchedule if the ID or name is not found, or the dependence requirement is not met \"\"\" super () . vectorize ( self . _lookup ( loop )) def separate_tail ( self , noDuplicateVarDefs = False ): \"\"\" Seperate main iterations and tail iterations of a loop E.g. ``` for i = 0 -> 3 { for j = 0 -> 4 { if (i * 4 + j < 10) { ... } } } ``` Each loop will be separated into 2 parts: the body and the tail. After simplification, the program will finally be transformed to ``` for i = 0 -> 2 { for j = 0 -> 4 { ... } } for j = 0 -> 2 { ... } ``` Ideally, all programs can benefit from this schedule. However, this schedule may greatly increase the program size and make the compiling time way too long. Therefore, this transformation is implemented as a schedule, which can be applied optionally. (TODO: Optionally apply this schedule to part of the program) Parameters ---------- noDuplicateVarDefs : bool If there is two VarDef nodes in two branches, it may result in doubled memory use, since different thread may go to different branch. Set this parameter to true to stop duplicating VarDef nodes. \"\"\" super () . separate_tail ( noDuplicateVarDefs ) def as_matmul ( self , loop ): \"\"\" Transform nested loops to be a external call to a matrix multiplication Parameters ---------- loop : str, ID or Stmt ID of the loop Raises ------ InvalidSchedule if the loop cannot be transformed to be a matrix multiplication \"\"\" super () . as_matmul ( self . _lookup ( loop )) def pluto_fuse ( self , loop0 , loop1 , nest_level_0 = 0 , nest_level_1 = 0 , fusable_overlap_threshold = 1 , do_simplify = True ): \"\"\" Use Pluto+ algorithm to permute and fuse two loops, with as most parallelizable loops as possible at outermost levels. The two loops are required to be consequent; all directly nested levels are detected and subject to permutation. Remaining levels that cannot be fused are left inside the fused loops as two statements Parameters ---------- loop0 : str, ID or Stmt The first loop to fuse loop1 : str, ID or Stmt The second loop to fuse nest_level_0 : int The number of nesting levels of loop 0 to be considered, defaults to maximum possible nest_level_1 : int The number of nesting levels of loop 1 to be considered, defaults to maximum possible fusableOverlapThreshold : int The minimum overlapping size of two loops to be regarded fusable. Defaults to 1 do_simplify : bool Whether the result is simplified by the way, defaults to true Returns ------- (ID, int) The ID of fused loop and level of parallelizable loops Raises ------ InvalidSchedule if the loops are not consequent \"\"\" return super () . pluto_fuse ( self . _lookup ( loop0 ), self . _lookup ( loop1 ), nest_level_0 , nest_level_1 , fusable_overlap_threshold , do_simplify ) def pluto_permute ( self , loop , nest_level = 0 , do_simplify = True ): \"\"\" Use Pluto+ algorithm to permute a single loop, with as most parallelizable loops as possible at outermost levels. Parameters ---------- loop : str, ID or Stmt The loop to permute nest_level : int The number of nesting levels to be considered, defaults to maximum possible do_simplify : bool Whether the result is simplified by the way, defaults to true Returns ------- (ID, int) The ID of permuted loop and level of parallelizable loops \"\"\" return super () . pluto_permute ( self . _lookup ( loop ), nest_level , do_simplify ) def auto_schedule ( self , target ): \"\"\" (Experimental) Automatic scheduling using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_schedule ( target ) def auto_use_lib ( self , target ): \"\"\" (Experimental) Automatically use external libs using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_use_lib ( target ) def auto_fuse ( self , target ): \"\"\" (Experimental) Automatically fuse consecutive loops using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_fuse ( target ) def auto_parallelize ( self , target ): \"\"\" (Experimental) Automatically parallelize some loops using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_parallelize ( target ) def auto_set_mem_type ( self , target ): \"\"\" (Experimental) Automatically set memory types using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_set_mem_type ( target ) def auto_unroll ( self , target ): \"\"\" (Experimental) Automatically unroll loops using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_unroll ( target )","title":"Schedule"},{"location":"api/#freetensor.core.schedule.Schedule.as_matmul","text":"Transform nested loops to be a external call to a matrix multiplication Parameters: loop ( str, ID or Stmt ) \u2013 ID of the loop Exceptions: InvalidSchedule \u2013 if the loop cannot be transformed to be a matrix multiplication Source code in freetensor/core/schedule.py def as_matmul ( self , loop ): \"\"\" Transform nested loops to be a external call to a matrix multiplication Parameters ---------- loop : str, ID or Stmt ID of the loop Raises ------ InvalidSchedule if the loop cannot be transformed to be a matrix multiplication \"\"\" super () . as_matmul ( self . _lookup ( loop ))","title":"as_matmul()"},{"location":"api/#freetensor.core.schedule.Schedule.ast","text":"Get the scheduled AST without function signature This is mainly for debugging and testting purpose Source code in freetensor/core/schedule.py def ast ( self ): \"\"\" Get the scheduled AST without function signature This is mainly for debugging and testting purpose \"\"\" ret = super () . ast () if self . verbose >= 1 : print ( f \"The scheduled AST is: \\n { ret } \" ) return ret","title":"ast()"},{"location":"api/#freetensor.core.schedule.Schedule.auto_fuse","text":"(Experimental) Automatically fuse consecutive loops using some heuristics Parameters: target ( Target ) \u2013 Target architecture Source code in freetensor/core/schedule.py def auto_fuse ( self , target ): \"\"\" (Experimental) Automatically fuse consecutive loops using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_fuse ( target )","title":"auto_fuse()"},{"location":"api/#freetensor.core.schedule.Schedule.auto_parallelize","text":"(Experimental) Automatically parallelize some loops using some heuristics Parameters: target ( Target ) \u2013 Target architecture Source code in freetensor/core/schedule.py def auto_parallelize ( self , target ): \"\"\" (Experimental) Automatically parallelize some loops using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_parallelize ( target )","title":"auto_parallelize()"},{"location":"api/#freetensor.core.schedule.Schedule.auto_schedule","text":"(Experimental) Automatic scheduling using some heuristics Parameters: target ( Target ) \u2013 Target architecture Source code in freetensor/core/schedule.py def auto_schedule ( self , target ): \"\"\" (Experimental) Automatic scheduling using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_schedule ( target )","title":"auto_schedule()"},{"location":"api/#freetensor.core.schedule.Schedule.auto_set_mem_type","text":"(Experimental) Automatically set memory types using some heuristics Parameters: target ( Target ) \u2013 Target architecture Source code in freetensor/core/schedule.py def auto_set_mem_type ( self , target ): \"\"\" (Experimental) Automatically set memory types using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_set_mem_type ( target )","title":"auto_set_mem_type()"},{"location":"api/#freetensor.core.schedule.Schedule.auto_unroll","text":"(Experimental) Automatically unroll loops using some heuristics Parameters: target ( Target ) \u2013 Target architecture Source code in freetensor/core/schedule.py def auto_unroll ( self , target ): \"\"\" (Experimental) Automatically unroll loops using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_unroll ( target )","title":"auto_unroll()"},{"location":"api/#freetensor.core.schedule.Schedule.auto_use_lib","text":"(Experimental) Automatically use external libs using some heuristics Parameters: target ( Target ) \u2013 Target architecture Source code in freetensor/core/schedule.py def auto_use_lib ( self , target ): \"\"\" (Experimental) Automatically use external libs using some heuristics Parameters ---------- target : Target Target architecture \"\"\" super () . auto_use_lib ( target )","title":"auto_use_lib()"},{"location":"api/#freetensor.core.schedule.Schedule.blend","text":"Unroll a loop and interleave statements from each iteration E.g. for i = 0 to 2 { f(i); g(i); } will be transformed to be f(0); f(1); g(0); g(1); Virtual threads in TVM can be implemented via blend Parameters: loop ( str, ID or Stmt ) \u2013 The loop being transformed Exceptions: InvalidSchedule \u2013 if the loop is not found, the loop length is not a constant, or the dependences cannot be solved Source code in freetensor/core/schedule.py def blend ( self , loop ): \"\"\" Unroll a loop and interleave statements from each iteration E.g. ``` for i = 0 to 2 { f(i); g(i); } ``` will be transformed to be ``` f(0); f(1); g(0); g(1); ``` Virtual threads in TVM can be implemented via blend Parameters ---------- loop : str, ID or Stmt The loop being transformed Raises ------ InvalidSchedule if the loop is not found, the loop length is not a constant, or the dependences cannot be solved \"\"\" super () . blend ( self . _lookup ( loop ))","title":"blend()"},{"location":"api/#freetensor.core.schedule.Schedule.cache","text":"Cache a variable into a new local variable All needed data will be filled into the cache first, then all reads and writes will be directed to the cache, and finally all needed data will be flushed from the cache Note for reduction: This transformation preserves the computation order. It will transform a += x a += y to a.cache = a + x + y a = a.cache If you need a \"real\" cache for reduction, which reorders the computation, use cache_reduction instead Parameters: stmt ( str, ID or Stmt ) \u2013 The statement or block (e.g. an If or a For) to be modified var ( str ) \u2013 Name of the variable to be cached mtype ( MemType ) \u2013 Where to cache Exceptions: InvalidSchedule \u2013 if the ID or name is not found Returns: (ID, ID, ID, ID) \u2013 (ID of the statement that fills the cache, ID of the statement that flushes from the cache, name of the cache variable, ID of the VarDef node of the cache variable) Source code in freetensor/core/schedule.py def cache ( self , stmt , var , mtype ): \"\"\" Cache a variable into a new local variable All needed data will be filled into the cache first, then all reads and writes will be directed to the cache, and finally all needed data will be flushed from the cache Note for reduction: This transformation preserves the computation order. It will transform ``` a += x a += y ``` to ``` a.cache = a + x + y a = a.cache ``` If you need a \"real\" cache for reduction, which reorders the computation, use `cache_reduction` instead Parameters ---------- stmt : str, ID or Stmt The statement or block (e.g. an If or a For) to be modified var : str Name of the variable to be cached mtype : MemType Where to cache Raises ------ InvalidSchedule if the ID or name is not found Returns ------- (ID, ID, ID, ID) (ID of the statement that fills the cache, ID of the statement that flushes from the cache, name of the cache variable, ID of the VarDef node of the cache variable) \"\"\" return super () . cache ( self . _lookup ( stmt ), var , MemType ( mtype ))","title":"cache()"},{"location":"api/#freetensor.core.schedule.Schedule.cache_reduction","text":"Perform local reductions (e.g. sum) in a local variable first, and then reduce the local result to the global variable E.g. a += x a += y will be transformed to be a.cache = x + y a += a.cache Parameters: stmt ( str, ID or Stmt ) \u2013 The statement or block (e.g. an If or a For) to be modified var ( str ) \u2013 Name of the variable to be cached. Only reductions are allowed on var in stmt . Plain reads or writes are not allowed mtype ( MemType ) \u2013 Where to cache Exceptions: InvalidSchedule \u2013 if the ID or name is not found, or there are unsupported reads or writes Returns: (ID, ID, ID, ID) \u2013 (ID of the statement that initialize the cache, ID of the statement that reduces the local result to the global result, name of the cache variable, ID of the VarDef node of the cache variable) Source code in freetensor/core/schedule.py def cache_reduction ( self , stmt , var , mtype ): \"\"\" Perform local reductions (e.g. sum) in a local variable first, and then reduce the local result to the global variable E.g. ``` a += x a += y ``` will be transformed to be ``` a.cache = x + y a += a.cache ``` Parameters ---------- stmt : str, ID or Stmt The statement or block (e.g. an If or a For) to be modified var : str Name of the variable to be cached. Only reductions are allowed on `var` in `stmt`. Plain reads or writes are not allowed mtype : MemType Where to cache Raises ------ InvalidSchedule if the ID or name is not found, or there are unsupported reads or writes Returns ------- (ID, ID, ID, ID) (ID of the statement that initialize the cache, ID of the statement that reduces the local result to the global result, name of the cache variable, ID of the VarDef node of the cache variable) \"\"\" return super () . cache_reduction ( self . _lookup ( stmt ), var , MemType ( mtype ))","title":"cache_reduction()"},{"location":"api/#freetensor.core.schedule.Schedule.fission","text":"Fission a loop into two loops each containing part of the statements, one followed by another To split loop into two nested loops, use split instead Statements inside the original loop will be distributed to one or both (happening if they are scope statements) loops. If a statement is originally labeled \"S\", it can be selected by \"$fission.0{S}\" (from the first loop) or \"$fission.1{S}\" (from the second loop) after fission. If one of the resulting loop has an empty body, it will be removed Parameters: loop ( str, ID or Stmt ) \u2013 The loop to be fissioned side ( FissionSide ) \u2013 If After , splitter is the last statement of the first loop. If Before , splitter is the first statement of the second loop splitter ( str (Selector string), ID, Stmt, or list of them ) \u2013 Where to fission the loop. If multiple statement are selected, fission the look before or after all of them Exceptions: InvalidSchedule \u2013 if any dependence cannot be resolved Returns: (IDMap, IDMap) \u2013 ({old ID -> new ID in 1st loop}, {old ID -> new ID in 2nd loop}). If a loop is removed because it has an empty body, it will not be in the returned map Source code in freetensor/core/schedule.py def fission ( self , loop , side , splitter ): \"\"\" Fission a loop into two loops each containing part of the statements, one followed by another To split loop into two nested loops, use `split` instead Statements inside the original loop will be distributed to one or both (happening if they are scope statements) loops. If a statement is originally labeled \"S\", it can be selected by \"$fission.0{S}\" (from the first loop) or \"$fission.1{S}\" (from the second loop) after fission. If one of the resulting loop has an empty body, it will be removed Parameters ---------- loop : str, ID or Stmt The loop to be fissioned side : FissionSide If `After`, `splitter` is the last statement of the first loop. If `Before`, `splitter` is the first statement of the second loop splitter : str (Selector string), ID, Stmt, or list of them Where to fission the loop. If multiple statement are selected, fission the look before or after all of them Raises ------ InvalidSchedule if any dependence cannot be resolved Returns ------- (IDMap, IDMap) ({old ID -> new ID in 1st loop}, {old ID -> new ID in 2nd loop}). If a loop is removed because it has an empty body, it will not be in the returned map \"\"\" old_ast = self . ast () splitter_list = self . _lookup_list ( splitter ) # In DFS order if side == FissionSide . Before : splitter = splitter_list [ 0 ] else : splitter = splitter_list [ - 1 ] map1 , map2 = super () . fission ( self . _lookup ( loop ), side , splitter ) return IDMap ( old_ast , map1 ), IDMap ( old_ast , map2 )","title":"fission()"},{"location":"api/#freetensor.core.schedule.Schedule.fork","text":"fork(self: freetensor_ffi.Schedule) -> freetensor_ffi.Schedule Source code in freetensor/core/schedule.py def fork ( self ): return Schedule ( super () . fork ())","title":"fork()"},{"location":"api/#freetensor.core.schedule.Schedule.func","text":"Get the scheduled function Source code in freetensor/core/schedule.py def func ( self ): \"\"\" Get the scheduled function \"\"\" ret = super () . func () if self . verbose >= 1 : print ( f \"The scheduled Func is: \\n { ret } \" ) return ret","title":"func()"},{"location":"api/#freetensor.core.schedule.Schedule.fuse","text":"Fuse two directly following loops with the same length into one To merge nested loops into one, use merge instead parallelize , unroll and vectorize properties will be reset on the fused loop Suppose the original loops are labeled \"L1\" and \"L2\", the fused loop can be selected by \"$fuse{L1, L2}\" Parameters: loop0 ( str, ID or Stmt ) \u2013 The leading loop loop1 ( str, ID or Stmt, Optional ) \u2013 The following loop. If omitted, it will try to find a following loop of loop0 strict ( bool ) \u2013 False by default. If set to True, throw an error if unable to determine whether the two loops are of the same length Exceptions: InvalidSchedule \u2013 if the two loops are not directly following, the two loops are not of the same length, or there is any dependence cannot be resolved Returns: ID \u2013 ID of the result loop Source code in freetensor/core/schedule.py def fuse ( self , loop0 , loop1 = None , strict = False ): \"\"\" Fuse two directly following loops with the same length into one To merge nested loops into one, use `merge` instead `parallelize`, `unroll` and `vectorize` properties will be reset on the fused loop Suppose the original loops are labeled \"L1\" and \"L2\", the fused loop can be selected by \"$fuse{L1, L2}\" Parameters ---------- loop0 : str, ID or Stmt The leading loop loop1 : str, ID or Stmt, Optional The following loop. If omitted, it will try to find a following loop of `loop0` strict : bool False by default. If set to True, throw an error if unable to determine whether the two loops are of the same length Raises ------ InvalidSchedule if the two loops are not directly following, the two loops are not of the same length, or there is any dependence cannot be resolved Returns ------- ID ID of the result loop \"\"\" if loop1 is None : return super () . fuse ( self . _lookup ( loop0 ), strict ) else : return super () . fuse ( self . _lookup ( loop0 ), self . _lookup ( loop1 ), strict )","title":"fuse()"},{"location":"api/#freetensor.core.schedule.Schedule.inline","text":"Remove a variable. When the variable is used, recompute its value Parameters: vardef ( str, ID or Stmt ) \u2013 The VarDef statement of the specific variable. It can not be an I/O varible Exceptions: InvalidSchedule \u2013 if the variable cannot be completely removed Source code in freetensor/core/schedule.py def inline ( self , vardef ): \"\"\" Remove a variable. When the variable is used, recompute its value Parameters ---------- vardef : str, ID or Stmt The VarDef statement of the specific variable. It can not be an I/O varible Raises ------ InvalidSchedule if the variable cannot be completely removed \"\"\" return super () . inline ( self . _lookup ( vardef ))","title":"inline()"},{"location":"api/#freetensor.core.schedule.Schedule.merge","text":"Merge two directly nested loops into one To fuse consecutive loops, use fuse instead parallelize , unroll and vectorize properties will be reset on the merged loop Suppose the original loops are labeled \"L1\" and \"L2\", the merged loop can be selected by \"$merge{L1, L2}\" Parameters: loop1, loop2 ( str, ID or Stmt ) \u2013 loops to be merged, can be in any order Exceptions: InvalidSchedule \u2013 if the loops are not directly nested Returns: ID \u2013 ID of the merged loop Source code in freetensor/core/schedule.py def merge ( self , loop1 , loop2 ): \"\"\" Merge two directly nested loops into one To fuse consecutive loops, use `fuse` instead `parallelize`, `unroll` and `vectorize` properties will be reset on the merged loop Suppose the original loops are labeled \"L1\" and \"L2\", the merged loop can be selected by \"$merge{L1, L2}\" Parameters ---------- loop1, loop2 : str, ID or Stmt loops to be merged, can be in any order Raises ------ InvalidSchedule if the loops are not directly nested Returns ------- ID ID of the merged loop \"\"\" return super () . merge ( self . _lookup ( loop1 ), self . _lookup ( loop2 ))","title":"merge()"},{"location":"api/#freetensor.core.schedule.Schedule.move_to","text":"Move a statement to a new position This is a composite schedule command, which is implemented with other commands If moving a statement out of some loops, identical loops will be added around the moved statement, which is equivalent to fission these loops Parameters: stmt ( str, ID or Stmt ) \u2013 The statement to be moved side ( MoveToSide ) \u2013 Whether stmt will be BEFORE or AFTER `dst dst ( str (Selector string), ID, Stmt, or list of them ) \u2013 Insert stmt to be directly after this statement. If multiple statements are selected, move to before or after all of them Exceptions: InvalidSchedule \u2013 if there is no feasible path to move Returns: (ID, ID) \u2013 (The new ID of the moved statement, The out-most newly introduced statments including the added loops) Source code in freetensor/core/schedule.py def move_to ( self , stmt , side , dst ): \"\"\" Move a statement to a new position This is a composite schedule command, which is implemented with other commands If moving a statement out of some loops, identical loops will be added around the moved statement, which is equivalent to fission these loops Parameters ---------- stmt : str, ID or Stmt The statement to be moved side : MoveToSide Whether `stmt` will be BEFORE or AFTER `dst dst : str (Selector string), ID, Stmt, or list of them Insert `stmt` to be directly after this statement. If multiple statements are selected, move to before or after all of them Raises ------ InvalidSchedule if there is no feasible path to move Returns ------- (ID, ID) (The new ID of the moved statement, The out-most newly introduced statments including the added loops) \"\"\" dst_list = self . _lookup_list ( dst ) # In DFS order if side == MoveToSide . Before : dst = dst_list [ 0 ] else : dst = dst_list [ - 1 ] return super () . move_to ( self . _lookup ( stmt ), side , dst )","title":"move_to()"},{"location":"api/#freetensor.core.schedule.Schedule.parallelize","text":"Mark a loop with a parallel implementation This schedule follows a fork-join model: multiple workers (abstract threads) are created (but physically the threads may be cached in a thread pool) when the loop begins, do their jobs in parallel, and join when the loop ends OpenMP threads follow a typical fork-join model. CUDA threads run in a bulk-synchronous parallel (BSP) model, which can also be mimiked by the fork-join model: All threads start when the kernel get launched, but they only begin to do their jobs when the parallel loop begins. Nevertheless, the fork-join model needs the following extension to fully mimic a BSP model: Taking CUDA as an example, we allow binding a loop to threadIdx.x inside another loop bound to threadIdx.x , which is illegal in a classic fork-join model. For example, we may implement a matmul with collaborative fetch as below: for i : threadIdx.x # Li for j : threadIdx.y # Lj local_sum = 0 # In gpu/local memory, unique to (i, j) for k0 # Lk0 for k : threadIdx.y # Lk1_a A_cache[k] = A[i, k] # In gpu/shared, shared by different j for k : threadIdx.x # Lk1_b B_cache[k] = B[k, j] # In gpu/shared, shared by different i for k # Lk1_c sum += A_cache[k] * B_cache[k] C[i, j] = local_sum A seemingly plausible solution to avoid this extension is to reorder Lk0 to outer-most, and then move Lk1_a and Lk1_b out of Li or Lj . This resolves the nested threadIdx.x and threadIdx.y binding problem by running Li+Lk1_a , Lj+Lk1_b and Li+Lj interleavingly, instead of running Lk1_a and Lk1_b inside Li+Lj . However, this approach is illegal, because the local variable local_sum can no longer be kept inside the body of Li and Lj : It has to be reused across multiple runs of Li and Lj Please also note that we can bind one threadIdx.x to two loops only when the body statement is loop-invariant to one of them. For example, the following binding is still illegal, even in our extended fork-join model, because it violates its serial semantics: for i : threadIdx.x for j : threadIdx.x A[i, j] ++ Parameters: loop ( str, ID or Stmt ) \u2013 The loop parallel ( ParallelScope ) \u2013 Parallel scope Source code in freetensor/core/schedule.py def parallelize ( self , loop , parallel ): \"\"\" Mark a loop with a parallel implementation This schedule follows a fork-join model: multiple workers (abstract threads) are created (but physically the threads may be cached in a thread pool) when the loop begins, do their jobs in parallel, and join when the loop ends OpenMP threads follow a typical fork-join model. CUDA threads run in a bulk-synchronous parallel (BSP) model, which can also be mimiked by the fork-join model: All threads start when the kernel get launched, but they only begin to do their jobs when the parallel loop begins. Nevertheless, the fork-join model needs the following extension to fully mimic a BSP model: Taking CUDA as an example, we allow binding a loop to `threadIdx.x` inside another loop bound to `threadIdx.x`, which is illegal in a classic fork-join model. For example, we may implement a matmul with collaborative fetch as below: ``` for i : threadIdx.x # Li for j : threadIdx.y # Lj local_sum = 0 # In gpu/local memory, unique to (i, j) for k0 # Lk0 for k : threadIdx.y # Lk1_a A_cache[k] = A[i, k] # In gpu/shared, shared by different j for k : threadIdx.x # Lk1_b B_cache[k] = B[k, j] # In gpu/shared, shared by different i for k # Lk1_c sum += A_cache[k] * B_cache[k] C[i, j] = local_sum ``` A seemingly plausible solution to avoid this extension is to reorder `Lk0` to outer-most, and then move `Lk1_a` and `Lk1_b` out of `Li` or `Lj`. This resolves the nested `threadIdx.x` and `threadIdx.y` binding problem by running `Li+Lk1_a`, `Lj+Lk1_b` and `Li+Lj` interleavingly, instead of running `Lk1_a` and `Lk1_b` inside `Li+Lj`. However, this approach is illegal, because the local variable `local_sum` can no longer be kept inside the body of `Li` and `Lj`: It has to be reused across multiple runs of `Li` and `Lj` Please also note that we can bind one `threadIdx.x` to two loops only when the body statement is loop-invariant to one of them. For example, the following binding is still illegal, even in our extended fork-join model, because it violates its serial semantics: ``` for i : threadIdx.x for j : threadIdx.x A[i, j] ++ ``` Parameters ---------- loop : str, ID or Stmt The loop parallel : ParallelScope Parallel scope \"\"\" super () . parallelize ( self . _lookup ( loop ), ParallelScope ( parallel ))","title":"parallelize()"},{"location":"api/#freetensor.core.schedule.Schedule.permute","text":"Permute perfectly nested loops (directly nested loops without statements in between) with the given loop space transformation function The transformed loops follow ascending lexical order of the transformed terms returned by transformFunc when called with original iteration Parameters: loops ( array like of str, ID or Stmt ) \u2013 the list of perfectly nested loops to be permuted transform_func ( Callable[[Expr], Expr] ) \u2013 the loop space transformation function, should be bijective Returns: list of ID \u2013 the list of IDs of permuted loops Source code in freetensor/core/schedule.py def permute ( self , loops , transform_func ): \"\"\" Permute perfectly nested loops (directly nested loops without statements in between) with the given loop space transformation function The transformed loops follow ascending lexical order of the transformed terms returned by `transformFunc` when called with original iteration Parameters ---------- loops : array like of str, ID or Stmt the list of perfectly nested loops to be permuted transform_func : Callable[[Expr], Expr] the loop space transformation function, should be bijective Returns ------- list of ID the list of IDs of permuted loops \"\"\" return super () . permute ([ self . _lookup ( l ) for l in loops ], transform_func )","title":"permute()"},{"location":"api/#freetensor.core.schedule.Schedule.pluto_fuse","text":"Use Pluto+ algorithm to permute and fuse two loops, with as most parallelizable loops as possible at outermost levels. The two loops are required to be consequent; all directly nested levels are detected and subject to permutation. Remaining levels that cannot be fused are left inside the fused loops as two statements Parameters: loop0 ( str, ID or Stmt ) \u2013 The first loop to fuse loop1 ( str, ID or Stmt ) \u2013 The second loop to fuse nest_level_0 ( int ) \u2013 The number of nesting levels of loop 0 to be considered, defaults to maximum possible nest_level_1 ( int ) \u2013 The number of nesting levels of loop 1 to be considered, defaults to maximum possible fusableOverlapThreshold ( int ) \u2013 The minimum overlapping size of two loops to be regarded fusable. Defaults to 1 do_simplify ( bool ) \u2013 Whether the result is simplified by the way, defaults to true Exceptions: InvalidSchedule \u2013 if the loops are not consequent Returns: (ID, int) \u2013 The ID of fused loop and level of parallelizable loops Source code in freetensor/core/schedule.py def pluto_fuse ( self , loop0 , loop1 , nest_level_0 = 0 , nest_level_1 = 0 , fusable_overlap_threshold = 1 , do_simplify = True ): \"\"\" Use Pluto+ algorithm to permute and fuse two loops, with as most parallelizable loops as possible at outermost levels. The two loops are required to be consequent; all directly nested levels are detected and subject to permutation. Remaining levels that cannot be fused are left inside the fused loops as two statements Parameters ---------- loop0 : str, ID or Stmt The first loop to fuse loop1 : str, ID or Stmt The second loop to fuse nest_level_0 : int The number of nesting levels of loop 0 to be considered, defaults to maximum possible nest_level_1 : int The number of nesting levels of loop 1 to be considered, defaults to maximum possible fusableOverlapThreshold : int The minimum overlapping size of two loops to be regarded fusable. Defaults to 1 do_simplify : bool Whether the result is simplified by the way, defaults to true Returns ------- (ID, int) The ID of fused loop and level of parallelizable loops Raises ------ InvalidSchedule if the loops are not consequent \"\"\" return super () . pluto_fuse ( self . _lookup ( loop0 ), self . _lookup ( loop1 ), nest_level_0 , nest_level_1 , fusable_overlap_threshold , do_simplify )","title":"pluto_fuse()"},{"location":"api/#freetensor.core.schedule.Schedule.pluto_permute","text":"Use Pluto+ algorithm to permute a single loop, with as most parallelizable loops as possible at outermost levels. Parameters: loop ( str, ID or Stmt ) \u2013 The loop to permute nest_level ( int ) \u2013 The number of nesting levels to be considered, defaults to maximum possible do_simplify ( bool ) \u2013 Whether the result is simplified by the way, defaults to true Returns: (ID, int) \u2013 The ID of permuted loop and level of parallelizable loops Source code in freetensor/core/schedule.py def pluto_permute ( self , loop , nest_level = 0 , do_simplify = True ): \"\"\" Use Pluto+ algorithm to permute a single loop, with as most parallelizable loops as possible at outermost levels. Parameters ---------- loop : str, ID or Stmt The loop to permute nest_level : int The number of nesting levels to be considered, defaults to maximum possible do_simplify : bool Whether the result is simplified by the way, defaults to true Returns ------- (ID, int) The ID of permuted loop and level of parallelizable loops \"\"\" return super () . pluto_permute ( self . _lookup ( loop ), nest_level , do_simplify )","title":"pluto_permute()"},{"location":"api/#freetensor.core.schedule.Schedule.reorder","text":"Reorder directly nested loops To swap consecutive loops, use swap instead Parameters: order ( array like of str, ID or Stmt ) \u2013 Vector of loops. The requested order of the loops Exceptions: InvalidSchedule \u2013 if the input is invalid or there are breaking dependences Source code in freetensor/core/schedule.py def reorder ( self , order ): \"\"\" Reorder directly nested loops To swap consecutive loops, use `swap` instead Parameters ---------- order : array like of str, ID or Stmt Vector of loops. The requested order of the loops Raises ------ InvalidSchedule if the input is invalid or there are breaking dependences \"\"\" super () . reorder ( list ( map ( self . _lookup , order )))","title":"reorder()"},{"location":"api/#freetensor.core.schedule.Schedule.separate_tail","text":"Seperate main iterations and tail iterations of a loop E.g. for i = 0 -> 3 { for j = 0 -> 4 { if (i * 4 + j < 10) { ... } } } Each loop will be separated into 2 parts: the body and the tail. After simplification, the program will finally be transformed to for i = 0 -> 2 { for j = 0 -> 4 { ... } } for j = 0 -> 2 { ... } Ideally, all programs can benefit from this schedule. However, this schedule may greatly increase the program size and make the compiling time way too long. Therefore, this transformation is implemented as a schedule, which can be applied optionally. (TODO: Optionally apply this schedule to part of the program) Parameters: noDuplicateVarDefs ( bool ) \u2013 If there is two VarDef nodes in two branches, it may result in doubled memory use, since different thread may go to different branch. Set this parameter to true to stop duplicating VarDef nodes. Source code in freetensor/core/schedule.py def separate_tail ( self , noDuplicateVarDefs = False ): \"\"\" Seperate main iterations and tail iterations of a loop E.g. ``` for i = 0 -> 3 { for j = 0 -> 4 { if (i * 4 + j < 10) { ... } } } ``` Each loop will be separated into 2 parts: the body and the tail. After simplification, the program will finally be transformed to ``` for i = 0 -> 2 { for j = 0 -> 4 { ... } } for j = 0 -> 2 { ... } ``` Ideally, all programs can benefit from this schedule. However, this schedule may greatly increase the program size and make the compiling time way too long. Therefore, this transformation is implemented as a schedule, which can be applied optionally. (TODO: Optionally apply this schedule to part of the program) Parameters ---------- noDuplicateVarDefs : bool If there is two VarDef nodes in two branches, it may result in doubled memory use, since different thread may go to different branch. Set this parameter to true to stop duplicating VarDef nodes. \"\"\" super () . separate_tail ( noDuplicateVarDefs )","title":"separate_tail()"},{"location":"api/#freetensor.core.schedule.Schedule.set_mem_type","text":"Change where a variable is stored Parameters: vardef ( str, ID or Stmt ) \u2013 ID of the VarDef statement of the specific variable mtype ( MemType ) \u2013 Where the variable should be stored Exceptions: InvalidSchedule \u2013 if the variable is not found Source code in freetensor/core/schedule.py def set_mem_type ( self , vardef , mtype ): \"\"\" Change where a variable is stored Parameters ---------- vardef : str, ID or Stmt ID of the VarDef statement of the specific variable mtype : MemType Where the variable should be stored Raises ------ InvalidSchedule if the variable is not found \"\"\" super () . set_mem_type ( self . _lookup ( vardef ), MemType ( mtype ))","title":"set_mem_type()"},{"location":"api/#freetensor.core.schedule.Schedule.split","text":"Split a loop into two nested loops To fission a loop into two consecutive loops, use fission instead Two modes are provided: Specify factor and leave nparts to -1. It will result in an outer loop with length ceil(n / factor) , and an inner loop with length factor , where n is the original loop length added by shift . The original iterator i will be transformed to i0 * factor + i1 , where i0 and i1 are the iterators of the new outer and inner loops, respectively Specify nparts and leave factor to -1. It will result in an outer loop with length nparts , and an inner loop with length ceil(n / nparts) , where n is the original loop length added by shift . The original iterator i will be transformed to i0 * ceil(n / nparts) + i1 , where i0 and i1 are the iterators of the new outer and inner loops, respectively Please note that the second mode will introduce an i0 * ceil(n / nparts) factor into the program, which cannot be recognized by polyhedral analysis, which may hinder some following schedules. If possible, plese use the first mode, and then reorder the inner and outer loops Suppose the original loop is labeled \"L\", the split two loops can be selected by \"$split.0{L}\" (the outer loop) and \"$split.1{L}\" (the inner loop). If one of the resulting loop is proved to have only a single iteration, it will be removed Parameters: node ( str, ID or Stmt ) \u2013 The loop to be split factor ( int ) \u2013 Length of the inner loop. Set to -1 if using nparts nparts ( int ) \u2013 Length of the outer loop. Set to -1 if using factor Exceptions: InvalidSchedule \u2013 if the loop is not found Returns: (Optional[ID], Optional[ID]) \u2013 (outer loop ID, inner loop ID), either ID can be None if the loop is proved to have only a single iteration Source code in freetensor/core/schedule.py def split ( self , node , factor =- 1 , nparts =- 1 , shift = 0 ): \"\"\" Split a loop into two nested loops To fission a loop into two consecutive loops, use `fission` instead Two modes are provided: 1. Specify `factor` and leave `nparts` to -1. It will result in an outer loop with length `ceil(n / factor)`, and an inner loop with length `factor`, where `n` is the original loop length added by `shift`. The original iterator `i` will be transformed to `i0 * factor + i1`, where `i0` and `i1` are the iterators of the new outer and inner loops, respectively 2. Specify `nparts` and leave `factor` to -1. It will result in an outer loop with length `nparts`, and an inner loop with length `ceil(n / nparts)`, where `n` is the original loop length added by `shift`. The original iterator `i` will be transformed to `i0 * ceil(n / nparts) + i1`, where `i0` and `i1` are the iterators of the new outer and inner loops, respectively Please note that the second mode will introduce an `i0 * ceil(n / nparts)` factor into the program, which cannot be recognized by polyhedral analysis, which may hinder some following schedules. If possible, plese use the first mode, and then reorder the inner and outer loops Suppose the original loop is labeled \"L\", the split two loops can be selected by \"$split.0{L}\" (the outer loop) and \"$split.1{L}\" (the inner loop). If one of the resulting loop is proved to have only a single iteration, it will be removed Parameters ---------- node : str, ID or Stmt The loop to be split factor : int Length of the inner loop. Set to -1 if using `nparts` nparts : int Length of the outer loop. Set to -1 if using `factor` Raises ------ InvalidSchedule if the loop is not found Returns ------- (Optional[ID], Optional[ID]) (outer loop ID, inner loop ID), either ID can be None if the loop is proved to have only a single iteration \"\"\" return ( i if i else None for i in super () . split ( self . _lookup ( node ), factor , nparts , shift ))","title":"split()"},{"location":"api/#freetensor.core.schedule.Schedule.swap","text":"Swap statements in the same block To reorder nested loops, use reorder instead Parameters: order ( List[str (Selector string), ID, List[ID], Stmt, or List[Stmt]] ) \u2013 The statements. If one item of the order list contains multiple statements, the order list will be flattened Exceptions: InvalidSchedule \u2013 if the statements are not found or the dependences cannot be solved Source code in freetensor/core/schedule.py def swap ( self , order ): \"\"\" Swap statements in the same block To reorder nested loops, use `reorder` instead Parameters ---------- order : List[str (Selector string), ID, List[ID], Stmt, or List[Stmt]] The statements. If one item of the `order` list contains multiple statements, the `order` list will be flattened Raises ------ InvalidSchedule if the statements are not found or the dependences cannot be solved \"\"\" super () . swap ( self . _lookup_list ( order ))","title":"swap()"},{"location":"api/#freetensor.core.schedule.Schedule.unroll","text":"Unroll a loop Parameters: loop ( str, ID or Stmt ) \u2013 ID of the loop immediate ( bool ) \u2013 If false (by default), postpone the unroll procedure to the backend compiler, which saves scheduling time. If true, unroll the loop immediately, which may help further simplifications based on the unrolled result. If your purpose is just to fill the instruction cache, set it to false. If you are unrolling a loop that computes array indices, set it to true Exceptions: InvalidSchedule \u2013 if the loop is not found or length of the loop is not a constant Source code in freetensor/core/schedule.py def unroll ( self , loop , immediate = False ): \"\"\" Unroll a loop Parameters ---------- loop : str, ID or Stmt ID of the loop immediate : bool If false (by default), postpone the unroll procedure to the backend compiler, which saves scheduling time. If true, unroll the loop immediately, which may help further simplifications based on the unrolled result. If your purpose is just to fill the instruction cache, set it to false. If you are unrolling a loop that computes array indices, set it to true Raises ------ InvalidSchedule if the loop is not found or length of the loop is not a constant \"\"\" super () . unroll ( self . _lookup ( loop ), immediate )","title":"unroll()"},{"location":"api/#freetensor.core.schedule.Schedule.var_merge","text":"Merge two dimensions of a variable Parameters: vardef ( str, ID or Stmt ) \u2013 ID of the VarDef statement of the specific variable dim ( int ) \u2013 Merge the dim -th and the (dim + 1) -th dimension Source code in freetensor/core/schedule.py def var_merge ( self , vardef , dim ): \"\"\" Merge two dimensions of a variable Parameters ---------- vardef : str, ID or Stmt ID of the VarDef statement of the specific variable dim : int Merge the `dim`-th and the `(dim + 1)`-th dimension \"\"\" return super () . var_merge ( self . _lookup ( vardef ), dim )","title":"var_merge()"},{"location":"api/#freetensor.core.schedule.Schedule.var_reorder","text":"Reorder the dimensions of a variable Parameters: vardef ( str, ID or Stmt ) \u2013 ID of the VarDef statement of the specific variable order ( array like of str, ID or Stmt ) \u2013 Vector of integers. The new order of the dimensions Exceptions: InvalidSchedule \u2013 if the variable or the order is illegal Source code in freetensor/core/schedule.py def var_reorder ( self , vardef , order ): \"\"\" Reorder the dimensions of a variable Parameters ---------- vardef : str, ID or Stmt ID of the VarDef statement of the specific variable order : array like of str, ID or Stmt Vector of integers. The new order of the dimensions Raises ------ InvalidSchedule if the variable or the order is illegal \"\"\" return super () . var_reorder ( self . _lookup ( vardef ), order )","title":"var_reorder()"},{"location":"api/#freetensor.core.schedule.Schedule.var_split","text":"Split a dimension of a variable into two Parameters: vardef ( str, ID or Stmt ) \u2013 ID of the VarDef statement of the specific variable dim ( int ) \u2013 which dimension to be split mode ( VarSplitMode ) \u2013 When the dimension to split is not divisible by factor or nparts , the resulting shape may become larger. In FixedSize mode, the actual buffer size will not be changed, and gurads will be added to prevent out-of-bound accesses. In RelaxedSize mode, the buffer size may increase. The RelaxedSize mode cannot be applied to I/O variables factor ( int ) \u2013 Length of the inner (higher no.) dimension. Set to -1 if using nparts nparts ( int ) \u2013 Length of the outer (lower no.) loop. Set to -1 if using factor Exceptions: InvalidSchedule \u2013 if the variable or the dimension is not found Source code in freetensor/core/schedule.py def var_split ( self , vardef , dim , mode , factor =- 1 , nparts =- 1 ): \"\"\" Split a dimension of a variable into two Parameters ---------- vardef : str, ID or Stmt ID of the VarDef statement of the specific variable dim : int which dimension to be split mode : VarSplitMode When the dimension to split is not divisible by `factor` or `nparts`, the resulting shape may become larger. In `FixedSize` mode, the actual buffer size will not be changed, and gurads will be added to prevent out-of-bound accesses. In `RelaxedSize` mode, the buffer size may increase. The `RelaxedSize` mode cannot be applied to I/O variables factor : int Length of the inner (higher no.) dimension. Set to -1 if using `nparts` nparts : int Length of the outer (lower no.) loop. Set to -1 if using `factor` Raises ------ InvalidSchedule if the variable or the dimension is not found \"\"\" return super () . var_split ( self . _lookup ( vardef ), dim , mode , factor , nparts )","title":"var_split()"},{"location":"api/#freetensor.core.schedule.Schedule.vectorize","text":"Vectorize a loop Please note that, as vectorization is different from architecture to achitecture, the scheduler may or may not postpone it to the backend compiler. The vectorization is a best-effort schedule Parameters: loop ( str, ID or Stmt ) \u2013 ID of the loop Exceptions: InvalidSchedule \u2013 if the ID or name is not found, or the dependence requirement is not met Source code in freetensor/core/schedule.py def vectorize ( self , loop ): \"\"\" Vectorize a loop Please note that, as vectorization is different from architecture to achitecture, the scheduler may or may not postpone it to the backend compiler. The vectorization is a best-effort schedule Parameters ---------- loop : str, ID or Stmt ID of the loop Raises ------ InvalidSchedule if the ID or name is not found, or the dependence requirement is not met \"\"\" super () . vectorize ( self . _lookup ( loop ))","title":"vectorize()"},{"location":"api/#freetensor.core.schedule.schedule","text":"Apply any schedule on an AST through a user callback Parameters: ast ( Func or Stmt ) \u2013 The AST to schedule. If not specified, a partial function will be returned that cna be used as a decorator callback ( Callable[[freetensor.core.schedule.Schedule], NoneType] ) \u2013 Specify what schedule(s) to do in this callback verbose ( Optional[int] ) \u2013 0 = print nothing. 1 = print the final AST. 2 = print an AST after each schedule Source code in freetensor/core/schedule.py def schedule ( ast = None , callback : Callable [[ Schedule ], None ] = None , verbose : Optional [ int ] = None ): ''' Apply any schedule on an AST through a user callback Parameters ---------- ast : Func or Stmt The AST to schedule. If not specified, a partial function will be returned that cna be used as a decorator callback : Callable Specify what schedule(s) to do in this callback verbose : int (Optional) 0 = print nothing. 1 = print the final AST. 2 = print an AST after each schedule ''' if ast is not None : if callback is None : return ast if verbose is None : verbose = 0 s = Schedule ( ast , verbose = verbose ) callback ( s ) if ast . type () == ffi . ASTNodeType . Func : return s . func () else : return s . ast () else : f = schedule if callback is not None : f = functools . partial ( f , callback = callback ) if verbose is not None : f = functools . partial ( f , verbose = verbose ) return f","title":"schedule()"},{"location":"api/#freetensor.core.staging","text":"A staging framework to support the FreeTensor frontend.","title":"staging"},{"location":"api/#freetensor.core.staging.AllowShortcutScope","text":"Allow return scope. This is a context manager that allows return in statically deterministic control flow. Source code in freetensor/core/staging.py @dataclass class AllowShortcutScope : '''Allow return scope. This is a context manager that allows return in statically deterministic control flow. ''' overload : StagingOverload should_allow : bool def __enter__ ( self ): self . prev = self . overload . is_shortcut_allowed self . overload . is_shortcut_allowed = self . should_allow def __exit__ ( self , exc_class , exc_value , traceback ): self . overload . is_shortcut_allowed = self . prev","title":"AllowShortcutScope"},{"location":"api/#freetensor.core.staging.BreakException","text":"Exception to be raised by StagingOverload.break_stmt. Breaks from a for loop. Source code in freetensor/core/staging.py class BreakException ( Exception ): '''Exception to be raised by StagingOverload.break_stmt. Breaks from a for loop.''' pass","title":"BreakException"},{"location":"api/#freetensor.core.staging.ContinueException","text":"Exception to be raised by StagingOverload.continue_stmt. Continues a for loop. Source code in freetensor/core/staging.py class ContinueException ( Exception ): '''Exception to be raised by StagingOverload.continue_stmt. Continues a for loop.''' pass","title":"ContinueException"},{"location":"api/#freetensor.core.staging.ReturnException","text":"Exception to be raised by StagingOverload.return_stmt. Holds a return value that will be passed through to the function wrapper. Source code in freetensor/core/staging.py class ReturnException ( Exception ): '''Exception to be raised by StagingOverload.return_stmt. Holds a return value that will be passed through to the function wrapper.''' def __init__ ( self , value : Any ) -> None : self . value = value","title":"ReturnException"},{"location":"api/#freetensor.core.staging.StagedAssignable","text":"Source code in freetensor/core/staging.py class StagedAssignable ( abc . ABC ): @abc . abstractmethod def assign ( self , name : str ): raise NotImplementedError ()","title":"StagedAssignable"},{"location":"api/#freetensor.core.staging.StagedAssignable.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in freetensor/core/staging.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls )","title":"__class__"},{"location":"api/#freetensor.core.staging.StagedAssignable.__class__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in freetensor/core/staging.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/#freetensor.core.staging.StagedAssignable.__class__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in freetensor/core/staging.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/#freetensor.core.staging.StagedAssignable.__class__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in freetensor/core/staging.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/#freetensor.core.staging.StagedAssignable.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/staging.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/#freetensor.core.staging.StagedPredicate","text":"Source code in freetensor/core/staging.py class StagedPredicate ( abc . ABC ): @abc . abstractmethod def logical_and ( self , lazy_other : Callable [[], StagedPredicate ]) -> StagedPredicate : raise NotImplementedError () @abc . abstractmethod def logical_or ( self , lazy_other : Callable [[], StagedPredicate ]) -> StagedPredicate : raise NotImplementedError () @abc . abstractmethod def logical_not ( self ): raise NotImplementedError () @abc . abstractmethod def if_then_else_stmt ( self , then_body : Callable [[], None ], else_body : Optional [ Callable [[], None ]]): raise NotImplementedError () @abc . abstractmethod def if_then_else_expr ( self , then_expr : Callable [[], Any ], else_expr : Callable [[], Any ]): raise NotImplementedError () @abc . abstractmethod def while_stmt ( self , body : Callable [[], None ]): raise NotImplementedError () @abc . abstractmethod def assert_stmt ( self ): raise NotImplementedError ()","title":"StagedPredicate"},{"location":"api/#freetensor.core.staging.StagedPredicate.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in freetensor/core/staging.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls )","title":"__class__"},{"location":"api/#freetensor.core.staging.StagedPredicate.__class__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in freetensor/core/staging.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/#freetensor.core.staging.StagedPredicate.__class__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in freetensor/core/staging.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/#freetensor.core.staging.StagedPredicate.__class__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in freetensor/core/staging.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/#freetensor.core.staging.StagedPredicate.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/staging.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/#freetensor.core.staging.StagedTypeAnnotation","text":"Source code in freetensor/core/staging.py class StagedTypeAnnotation ( metaclass = StagedTypeAnnotationMeta ): @abc . abstractmethod def annotate ( self , name : str ): raise NotImplementedError ()","title":"StagedTypeAnnotation"},{"location":"api/#freetensor.core.staging.StagedTypeAnnotation.__class__","text":"Source code in freetensor/core/staging.py class StagedTypeAnnotationMeta ( abc . ABCMeta ): def __getitem__ ( self , args ): return self ( * args )","title":"__class__"},{"location":"api/#freetensor.core.staging.StagedTypeAnnotation.__class__.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in freetensor/core/staging.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls ) __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). Source code in freetensor/core/staging.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. Source code in freetensor/core/staging.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). Source code in freetensor/core/staging.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/staging.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"__base__"},{"location":"api/#freetensor.core.staging.StagedTypeAnnotation.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/staging.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/#freetensor.core.staging.StagedTypeAnnotationMeta","text":"Source code in freetensor/core/staging.py class StagedTypeAnnotationMeta ( abc . ABCMeta ): def __getitem__ ( self , args ): return self ( * args )","title":"StagedTypeAnnotationMeta"},{"location":"api/#freetensor.core.staging.StagedTypeAnnotationMeta.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in freetensor/core/staging.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls )","title":"__base__"},{"location":"api/#freetensor.core.staging.StagedTypeAnnotationMeta.__base__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in freetensor/core/staging.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/#freetensor.core.staging.StagedTypeAnnotationMeta.__base__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in freetensor/core/staging.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/#freetensor.core.staging.StagedTypeAnnotationMeta.__base__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in freetensor/core/staging.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/#freetensor.core.staging.StagedTypeAnnotationMeta.__base__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/staging.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/#freetensor.core.staging.StagedTypeAnnotationMeta.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/staging.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/#freetensor.core.staging.StagedUnpackAssignable","text":"Source code in freetensor/core/staging.py class StagedUnpackAssignable ( abc . ABC ): @abc . abstractmethod def assign ( self , names ): raise NotImplementedError ()","title":"StagedUnpackAssignable"},{"location":"api/#freetensor.core.staging.StagedUnpackAssignable.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in freetensor/core/staging.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls )","title":"__class__"},{"location":"api/#freetensor.core.staging.StagedUnpackAssignable.__class__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in freetensor/core/staging.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/#freetensor.core.staging.StagedUnpackAssignable.__class__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in freetensor/core/staging.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/#freetensor.core.staging.StagedUnpackAssignable.__class__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in freetensor/core/staging.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/#freetensor.core.staging.StagedUnpackAssignable.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in freetensor/core/staging.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/#freetensor.core.staging.StagingError","text":"Error occurred during staging function execution (i.e. IR tree generation). Source code in freetensor/core/staging.py class StagingError ( Exception ): '''Error occurred during staging function execution (i.e. IR tree generation).''' def __init__ ( self , overload : StagingOverload , message : str ) -> None : # TODO: add output of StagingContext.call_stack super () . __init__ ( f ' { message } : \\n { \"\" . join ( traceback . format_list ( overload . debug_call_stack )) } ' . lstrip ())","title":"StagingError"},{"location":"api/#freetensor.core.staging.StagingOverload","text":"Source code in freetensor/core/staging.py class StagingOverload : def __init__ ( self ) -> None : self . is_shortcut_allowed : bool = True self . debug_call_stack : List [ traceback . FrameSummary ] = [] def custom_attr ( self , obj : Any , attr : str ) -> Any : ''' Customized attribute accessor. The framework first looks for a Python native attribute. If not found, it looks for this overloaded custom attribute resolver. The default implementation provides no custom attribute. Can be overridden by subclasses. Parameters ---------- obj : Any Object to access attribute. attr : str Attribute name. Returns ------- Any : The attribute value. Throws ------ AttributeError : If the attribute is not found. ''' return None def metadata ( self , content ) -> None : ''' Metadata handler. A metadata line is a comment starting with `#! ` and followed by a metadata, represented as a string parameter. Defaults to a no-op. Can be overridden by subclasses. Parameters ---------- content : str The metadata content. ''' pass def at_position ( self , filename : str , lineno : int ) -> None : ''' Code position handler. Defaults to a no-op. Can be overridden by subclasses. Parameters ---------- filename : str Name of the file containing code for the next statement. lineno : int Line number of the next statement. ''' pass def error ( self , content : str ): return StagingError ( self , content ) def allow_shortcut_scope ( self , allow : bool ): '''Opens a scope that allows shortcut control flows in a statically deterministic context. Need to be closed by `with` statement.''' return AllowShortcutScope ( self , allow ) def foreach ( self , names , iter , body : Callable [[ Any ], None ]) -> None : '''Customized foreach wrapper. If `value` is instance of `StagedIterable`, its regarded as a customized foreach behavior and used to generate code for the python for loop. Otherwise, we try to execute the loop as usual. ''' if isinstance ( iter , StagedIterable ): iter . foreach ( names , body ) else : for iter_var in iter : try : body ( iter_var ) except BreakException : break except ContinueException : continue def unpack_assign_stmt ( self , names , values ): '''Customized assign wrapper for one or more targets. If `values` is instance of `StagedUnpackAssignable`, it's regarded as a customized assign behavior and gets executed with all the assigned targets' names. Otherwise, it calls `assign_stmt` with each sub-assignments. Please note that `names` can be nested tuples like `(\"a\", (\"b\", \"c\"))`. Please also note that `names` can also be a single string like \"a\" even if `values` is a tuple. There is no unpacking in this case ''' if isinstance ( values , StagedUnpackAssignable ): return values . assign ( names ) elif isinstance ( names , str ): return self . assign_stmt ( names , values ) else : assert isinstance ( names , Sequence ) values = tuple ( values ) if len ( names ) != len ( values ): raise self . error ( \"Number of return values does not match when unpacking\" ) returns = [] for name , value in zip ( names , values ): returns . append ( self . unpack_assign_stmt ( name , value )) return tuple ( returns ) def assign_stmt ( self , name : str , value ): '''Customized assign wrapper. If `value` is instance of `StagedAssignable`, it's regarded as a customized assign behavior and gets executed with the assigned target variable name. This wrapper is used for initializing a variable. ''' if isinstance ( value , StagedAssignable ): return value . assign ( name ) else : return value def if_then_else_stmt ( self , predicate , then_body , else_body = None ): '''If-then-else statement staging tool. When predicate is deterministic in staging, only one branch is generated. Otherwise, a If node in IR is generated. ''' if isinstance ( predicate , StagedPredicate ): predicate . if_then_else_stmt ( then_body , else_body ) else : if predicate : then_body () elif else_body : else_body () def if_then_else_expr ( self , predicate , then_expr , else_expr ): '''If-then-else expression staging tool.''' if isinstance ( predicate , StagedPredicate ): return predicate . if_then_else_expr ( then_expr , else_expr ) else : if predicate : return then_expr () else : return else_expr () def while_stmt ( self , fpred , body ): '''While statement staging tool.''' first_pred = fpred () if isinstance ( first_pred , StagedPredicate ): first_pred . while_stmt ( body ) else : if first_pred : try : body () except BreakException : return except ContinueException : pass while fpred (): try : body () except BreakException : break except ContinueException : continue def assert_stmt ( self , test ): '''Assert staging tool.''' if isinstance ( test , StagedPredicate ): test . assert_stmt () else : assert test def return_stmt ( self , value , funcname ): '''Return staging tool. Only allow return in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Return is only allowed in statically deterministic control flow.' ) if isinstance ( value , StagedUnpackAssignable ): # We don't know how many items are there, so no unpacking value = value . assign ( funcname ) if isinstance ( value , StagedAssignable ): value = value . assign ( funcname ) raise ReturnException ( value ) def break_stmt ( self ): '''Break staging tool. Only allow break in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Break is only allowed in statically deterministic control flow.' ) raise BreakException () def continue_stmt ( self ): '''Continue staging tool. Only allow continue in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Continue is only allowed in statically deterministic control flow.' ) raise ContinueException () def load_attr ( self , obj , attr : str ): '''Load attribute staging tool. Allows customization of reading attributes.''' try : return getattr ( obj , attr ) except AttributeError : try : # Have to use AttributeError again, since a custom attribute might have # a None value result = self . custom_attr ( obj , attr ) successful = True except AttributeError : successful = False if successful : return result else : raise def and_expr ( self , * lazy_args ): def reducer ( a , fb ): if isinstance ( a , StagedPredicate ): return a . logical_and ( fb ) else : # This is not a simple logical and; it's equivalent to a if-then-else. # Thus, if a is True, fb() is returned, preserving the original value, # which might be a StagedPredicate. return a and fb () return functools . reduce ( reducer , lazy_args , True ) def or_expr ( self , * lazy_args ): def reducer ( a , fb ): if isinstance ( a , StagedPredicate ): return a . logical_or ( fb ) else : return a or fb () return functools . reduce ( reducer , lazy_args , False ) def not_expr ( self , arg ): if isinstance ( arg , StagedPredicate ): return arg . logical_not () else : return not arg def functiondef_decorator ( self , filename ): return functools . partial ( self . functiondef_wrapper , filename ) def functiondef_wrapper ( self , filename , func ): '''Function definition wrapper. This wrapper performs extra initialization and cleanup for function definition. ''' def wrapped ( * args , ** kwargs ): # Push debug call stack with some random line number. # It will be updated by `mark_position` calls in the function. self . debug_call_stack . append ( traceback . FrameSummary ( filename , 1 , func . __name__ )) # The called function can now return from itself, despite what the outer # control flow is. with self . allow_shortcut_scope ( True ): try : func ( * args , ** kwargs ) except ReturnException as e : result = e . value else : # No return_stmt was called, naturally returns None result = None # Pop debug call stack. self . debug_call_stack . pop () return result return wrapped def annotate_stmt ( self , name : str , ty ): if isinstance ( ty , StagedTypeAnnotation ): return ty . annotate ( name ) return None def mark_position ( self , lineno : int ): # FrameSummary is immutable, so we have to initialize a new one with updated # line number. self . debug_call_stack [ - 1 ] = traceback . FrameSummary ( self . debug_call_stack [ - 1 ] . filename , lineno , self . debug_call_stack [ - 1 ] . name ) self . at_position ( self . debug_call_stack [ - 1 ] . filename , self . debug_call_stack [ - 1 ] . lineno ) def into_staging ( self , func , extra_locals : Dict [ str , Any ] = None , src : str = None , verbose = False ): assert inspect . isfunction ( func ) if extra_locals is None : extra_locals = {} if src is None : lines , lineno = ins . getsourcelines ( func ) src = '' . join ( lines ) file = ins . getfile ( func ) else : lineno = 1 file = f '<staging: { func . __name__ } >' # Inject overload to extra_locals. extra_locals [ '__staging_overload__' ] = self # To transform a function, except essential AST transformation, we have to pass # the globals and locals (actually captured outer local variables) to the # transformed function properly. # Note that: # 1. We have to pass both globals and locals to `exec`. # 2. We cannot insert locals to the globals `dict`, otherwise it will pollute # the globals `dict`. # 3. We cannot copy the globals `dict` before passing it to exec, otherwise the # staged function cannot write to globals and get later updates in the global. # Thus, we have to pass the globals and locals to the transformed function # separately. if func . __closure__ : assert len ( func . __code__ . co_freevars ) == len ( func . __closure__ ) func_locals = { name : cell for name , cell in zip ( func . __code__ . co_freevars , func . __closure__ ) } else : func_locals = {} # Translate `#! ` comments to metadata calls. src = process_annotating_comments ( src ) # Wrap the code if it has a indentation. if src [ 0 ] == ' ' or src [ 0 ] == ' \\t ' : src = 'if True: \\n ' + src tree = ast . parse ( src ) assert len ( tree . body ) == 1 and isinstance ( tree . body [ 0 ], ast . If ) # Replace with the real body to eliminate the faked if. tree . body = tree . body [ 0 ] . body # Modify lineno to match with the location. lineno -= 1 else : tree = ast . parse ( src ) tree = Transformer ( file , lineno ) . visit ( tree ) # Instead of passing the `func_local` directly to `exec`, we instead wrap the # staging function. This is to workaround an issue of CPython. (See # https://github.com/python/cpython/issues/86084). # The sketch is: # ``` # def __freetensor_staging_wrapper__(__freetensor_extra_locals__, # __freetensor_local_cells__): # some_extra_local = __freetensor_extra_locals__['some_extra_local'] # some_captured = None # # def original_func(): # nonlocal some_captured # some_captured = __freetensor_local_cells__.some_captured # try: # ... # original function body # finally: # __freetensor_local_cells__.some_captured = some_captured # # return original_func # ``` # Note that `__freetensor_local_cells__` is a `LocalsDictWrapper` object. # It in-turn accesses cell.cell_contents to get/set the value of the local # variable. # The `LocalsDictWrapper` is a helper class to reduce code generation complexity. WRAPPER_NAME = '__freetensor_staging_wrapper__' assert isinstance ( tree , ast . Module ) assert len ( tree . body ) == 1 and isinstance ( tree . body [ 0 ], ast . FunctionDef ) # Modify function body. if len ( func_locals ) > 0 : tree . body [ 0 ] . body = ([ # Declare them as nonlocals to assign to outer scope. ast . Nonlocal ( list ( func_locals . keys ())), ] + [ # Fetch latest values of the closure variables. ast . Assign ([ ast . Name ( name , ast . Store ())], ast . Attribute ( ast . Name ( '__freetensor_local_cells__' , ast . Load ()), name , ast . Load ())) for name in func_locals . keys () ] + [ # Use a try-finally to ensure closure write back. ast . Try ( body = tree . body [ 0 ] . body , handlers = [], orelse = [], finalbody = [ ast . Assign ([ ast . Attribute ( ast . Name ( '__freetensor_local_cells__' , ast . Load ()), name , ast . Store ()) ], ast . Name ( name , ast . Load ())) for name in func_locals . keys () ]) ]) tree . body = [ ast . FunctionDef ( name = WRAPPER_NAME , args = ast . arguments ( posonlyargs = [], args = [ ast . arg ( '__freetensor_extra_locals__' , None ), ast . arg ( '__freetensor_local_cells__' , None ) ], vararg = None , kwonlyargs = [], kw_defaults = [], kwarg = None , defaults = []), body = [ # Captured closure variables are not fetched here, only declared. ast . Assign ([ ast . Name ( name , ast . Store ())], ast . Constant ( None )) for name in func_locals . keys () ] + [ # Extra locals are fetched here. ast . Assign ([ ast . Name ( name , ast . Store ())], ast . Subscript ( ast . Name ( '__freetensor_extra_locals__' , ast . Load ()), ast_index ( ast . Constant ( name )), ast . Load ())) for name in extra_locals . keys () ] + tree . body + [ ast . Return ( value = ast . Name ( id = func . __name__ , ctx = ast . Load ()))], decorator_list = [], returns = None ), ] tree = ast . fix_missing_locations ( tree ) if verbose : import astor source = astor . to_source ( tree ) from pygments import highlight from pygments.lexers import PythonLexer from pygments.formatters import TerminalFormatter print ( highlight ( source , PythonLexer (), TerminalFormatter ( bg = 'dark' , linenos = True )), file = sys . stderr ) tree = source # make debug info match dumped source # Create an empty locals dict to avoid polluting the original globals. empty_locals = {} exec ( compile ( tree , f '<staging: { func . __name__ } >' , 'exec' ), func . __globals__ , empty_locals ) f_wrapper = empty_locals [ WRAPPER_NAME ] # Pass the closure to the wrapper and retrieve the staging function with # correct captured variables. f_staging = f_wrapper ( extra_locals , LocalsDictWrapper ( func_locals )) return f_staging","title":"StagingOverload"},{"location":"api/#freetensor.core.staging.StagingOverload.allow_shortcut_scope","text":"Opens a scope that allows shortcut control flows in a statically deterministic context. Need to be closed by with statement. Source code in freetensor/core/staging.py def allow_shortcut_scope ( self , allow : bool ): '''Opens a scope that allows shortcut control flows in a statically deterministic context. Need to be closed by `with` statement.''' return AllowShortcutScope ( self , allow )","title":"allow_shortcut_scope()"},{"location":"api/#freetensor.core.staging.StagingOverload.assert_stmt","text":"Assert staging tool. Source code in freetensor/core/staging.py def assert_stmt ( self , test ): '''Assert staging tool.''' if isinstance ( test , StagedPredicate ): test . assert_stmt () else : assert test","title":"assert_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.assign_stmt","text":"Customized assign wrapper. If value is instance of StagedAssignable , it's regarded as a customized assign behavior and gets executed with the assigned target variable name. This wrapper is used for initializing a variable. Source code in freetensor/core/staging.py def assign_stmt ( self , name : str , value ): '''Customized assign wrapper. If `value` is instance of `StagedAssignable`, it's regarded as a customized assign behavior and gets executed with the assigned target variable name. This wrapper is used for initializing a variable. ''' if isinstance ( value , StagedAssignable ): return value . assign ( name ) else : return value","title":"assign_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.at_position","text":"Code position handler. Defaults to a no-op. Can be overridden by subclasses. Parameters: filename ( str ) \u2013 Name of the file containing code for the next statement. lineno ( int ) \u2013 Line number of the next statement. Source code in freetensor/core/staging.py def at_position ( self , filename : str , lineno : int ) -> None : ''' Code position handler. Defaults to a no-op. Can be overridden by subclasses. Parameters ---------- filename : str Name of the file containing code for the next statement. lineno : int Line number of the next statement. ''' pass","title":"at_position()"},{"location":"api/#freetensor.core.staging.StagingOverload.break_stmt","text":"Break staging tool. Only allow break in static control flow. Source code in freetensor/core/staging.py def break_stmt ( self ): '''Break staging tool. Only allow break in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Break is only allowed in statically deterministic control flow.' ) raise BreakException ()","title":"break_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.continue_stmt","text":"Continue staging tool. Only allow continue in static control flow. Source code in freetensor/core/staging.py def continue_stmt ( self ): '''Continue staging tool. Only allow continue in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Continue is only allowed in statically deterministic control flow.' ) raise ContinueException ()","title":"continue_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.custom_attr","text":"Customized attribute accessor. The framework first looks for a Python native attribute. If not found, it looks for this overloaded custom attribute resolver. The default implementation provides no custom attribute. Can be overridden by subclasses. Parameters: obj ( Any ) \u2013 Object to access attribute. attr ( str ) \u2013 Attribute name. Returns: Any \u2013 The attribute value. Source code in freetensor/core/staging.py def custom_attr ( self , obj : Any , attr : str ) -> Any : ''' Customized attribute accessor. The framework first looks for a Python native attribute. If not found, it looks for this overloaded custom attribute resolver. The default implementation provides no custom attribute. Can be overridden by subclasses. Parameters ---------- obj : Any Object to access attribute. attr : str Attribute name. Returns ------- Any : The attribute value. Throws ------ AttributeError : If the attribute is not found. ''' return None","title":"custom_attr()"},{"location":"api/#freetensor.core.staging.StagingOverload.foreach","text":"Customized foreach wrapper. If value is instance of StagedIterable , its regarded as a customized foreach behavior and used to generate code for the python for loop. Otherwise, we try to execute the loop as usual. Source code in freetensor/core/staging.py def foreach ( self , names , iter , body : Callable [[ Any ], None ]) -> None : '''Customized foreach wrapper. If `value` is instance of `StagedIterable`, its regarded as a customized foreach behavior and used to generate code for the python for loop. Otherwise, we try to execute the loop as usual. ''' if isinstance ( iter , StagedIterable ): iter . foreach ( names , body ) else : for iter_var in iter : try : body ( iter_var ) except BreakException : break except ContinueException : continue","title":"foreach()"},{"location":"api/#freetensor.core.staging.StagingOverload.functiondef_wrapper","text":"Function definition wrapper. This wrapper performs extra initialization and cleanup for function definition. Source code in freetensor/core/staging.py def functiondef_wrapper ( self , filename , func ): '''Function definition wrapper. This wrapper performs extra initialization and cleanup for function definition. ''' def wrapped ( * args , ** kwargs ): # Push debug call stack with some random line number. # It will be updated by `mark_position` calls in the function. self . debug_call_stack . append ( traceback . FrameSummary ( filename , 1 , func . __name__ )) # The called function can now return from itself, despite what the outer # control flow is. with self . allow_shortcut_scope ( True ): try : func ( * args , ** kwargs ) except ReturnException as e : result = e . value else : # No return_stmt was called, naturally returns None result = None # Pop debug call stack. self . debug_call_stack . pop () return result return wrapped","title":"functiondef_wrapper()"},{"location":"api/#freetensor.core.staging.StagingOverload.if_then_else_expr","text":"If-then-else expression staging tool. Source code in freetensor/core/staging.py def if_then_else_expr ( self , predicate , then_expr , else_expr ): '''If-then-else expression staging tool.''' if isinstance ( predicate , StagedPredicate ): return predicate . if_then_else_expr ( then_expr , else_expr ) else : if predicate : return then_expr () else : return else_expr ()","title":"if_then_else_expr()"},{"location":"api/#freetensor.core.staging.StagingOverload.if_then_else_stmt","text":"If-then-else statement staging tool. When predicate is deterministic in staging, only one branch is generated. Otherwise, a If node in IR is generated. Source code in freetensor/core/staging.py def if_then_else_stmt ( self , predicate , then_body , else_body = None ): '''If-then-else statement staging tool. When predicate is deterministic in staging, only one branch is generated. Otherwise, a If node in IR is generated. ''' if isinstance ( predicate , StagedPredicate ): predicate . if_then_else_stmt ( then_body , else_body ) else : if predicate : then_body () elif else_body : else_body ()","title":"if_then_else_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.load_attr","text":"Load attribute staging tool. Allows customization of reading attributes. Source code in freetensor/core/staging.py def load_attr ( self , obj , attr : str ): '''Load attribute staging tool. Allows customization of reading attributes.''' try : return getattr ( obj , attr ) except AttributeError : try : # Have to use AttributeError again, since a custom attribute might have # a None value result = self . custom_attr ( obj , attr ) successful = True except AttributeError : successful = False if successful : return result else : raise","title":"load_attr()"},{"location":"api/#freetensor.core.staging.StagingOverload.metadata","text":"Metadata handler. A metadata line is a comment starting with #! and followed by a metadata, represented as a string parameter. Defaults to a no-op. Can be overridden by subclasses. Parameters: content ( str ) \u2013 The metadata content. Source code in freetensor/core/staging.py def metadata ( self , content ) -> None : ''' Metadata handler. A metadata line is a comment starting with `#! ` and followed by a metadata, represented as a string parameter. Defaults to a no-op. Can be overridden by subclasses. Parameters ---------- content : str The metadata content. ''' pass","title":"metadata()"},{"location":"api/#freetensor.core.staging.StagingOverload.return_stmt","text":"Return staging tool. Only allow return in static control flow. Source code in freetensor/core/staging.py def return_stmt ( self , value , funcname ): '''Return staging tool. Only allow return in static control flow.''' if not self . is_shortcut_allowed : raise self . error ( 'Return is only allowed in statically deterministic control flow.' ) if isinstance ( value , StagedUnpackAssignable ): # We don't know how many items are there, so no unpacking value = value . assign ( funcname ) if isinstance ( value , StagedAssignable ): value = value . assign ( funcname ) raise ReturnException ( value )","title":"return_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.unpack_assign_stmt","text":"Customized assign wrapper for one or more targets. If values is instance of StagedUnpackAssignable , it's regarded as a customized assign behavior and gets executed with all the assigned targets' names. Otherwise, it calls assign_stmt with each sub-assignments. Please note that names can be nested tuples like (\"a\", (\"b\", \"c\")) . Please also note that names can also be a single string like \"a\" even if values is a tuple. There is no unpacking in this case Source code in freetensor/core/staging.py def unpack_assign_stmt ( self , names , values ): '''Customized assign wrapper for one or more targets. If `values` is instance of `StagedUnpackAssignable`, it's regarded as a customized assign behavior and gets executed with all the assigned targets' names. Otherwise, it calls `assign_stmt` with each sub-assignments. Please note that `names` can be nested tuples like `(\"a\", (\"b\", \"c\"))`. Please also note that `names` can also be a single string like \"a\" even if `values` is a tuple. There is no unpacking in this case ''' if isinstance ( values , StagedUnpackAssignable ): return values . assign ( names ) elif isinstance ( names , str ): return self . assign_stmt ( names , values ) else : assert isinstance ( names , Sequence ) values = tuple ( values ) if len ( names ) != len ( values ): raise self . error ( \"Number of return values does not match when unpacking\" ) returns = [] for name , value in zip ( names , values ): returns . append ( self . unpack_assign_stmt ( name , value )) return tuple ( returns )","title":"unpack_assign_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.while_stmt","text":"While statement staging tool. Source code in freetensor/core/staging.py def while_stmt ( self , fpred , body ): '''While statement staging tool.''' first_pred = fpred () if isinstance ( first_pred , StagedPredicate ): first_pred . while_stmt ( body ) else : if first_pred : try : body () except BreakException : return except ContinueException : pass while fpred (): try : body () except BreakException : break except ContinueException : continue","title":"while_stmt()"},{"location":"api/#freetensor.core.staging.TransformError","text":"Error occurred during AST transforming from python function to staging function that generates IR tree. Source code in freetensor/core/staging.py class TransformError ( Exception ): '''Error occurred during AST transforming from python function to staging function that generates IR tree.''' def __init__ ( self , message : str , filename : str , base_lineno : int , error_node : ast . AST ) -> None : super () . __init__ ( f 'At { filename } : { base_lineno + error_node . lineno } : \\n { message } .' )","title":"TransformError"},{"location":"api/#freetensor.core.staging.Transformer","text":"Transformer(filename: 'str', base_lineno: 'int', curr_func: 'str' = None, nonlocals: 'List[List[str]]' = None) Source code in freetensor/core/staging.py @dataclass class Transformer ( ast . NodeTransformer ): filename : str base_lineno : int curr_func : str = None nonlocals : List [ List [ str ]] = None def visit ( self , node : ast . AST ): new_node = super () . visit ( node ) if isinstance ( node , ast . stmt ) and not isinstance ( node , ast . FunctionDef ): if not isinstance ( new_node , list ): new_node = [ new_node ] return [ ast . Expr ( call_helper ( StagingOverload . mark_position , ast . Constant ( self . base_lineno + node . lineno - 1 ))) ] + new_node return new_node def visit_Assign ( self , old_node : ast . Assign ) -> ast . Assign : '''Rule: `lhs = rhs` -> `lhs = unpack_assign_stmt('lhs', rhs)` `x.lhs = rhs` -> `x.lhs = unpack_assign_stmt('lhs', rhs)` `a, (b, c) = (x, (y, z))` -> `a, (b, c) = unpack_assign_stmt(('a', ('b', 'c')), (x, (y, z)))` `a = b = c` -> `a = unpack_assign_stmt('a', c); b = unpack_assign_stmt('b', c)` If `unpack_assign_stmt` is not overloaded, `assign_stmt` will be called for each item ''' node : ast . Assign = self . generic_visit ( old_node ) class UnoverloadableExcept ( BaseException ): pass def recursive_get_names ( target ): if isinstance ( target , ast . Name ): return ast . Constant ( target . id ) elif isinstance ( target , ast . Attribute ): return ast . Constant ( target . attr ) elif isinstance ( target , ast . Tuple ): # Unpacking: (a, b) = c l = [] for t in target . elts : l . append ( recursive_get_names ( t )) return ast . Tuple ( l , ast . Load ()) else : raise UnoverloadableExcept () def do_visit_assign ( targets ): try : names = recursive_get_names ( targets ) return ast . Assign ([ targets ], call_helper ( StagingOverload . unpack_assign_stmt , names , node . value )) except UnoverloadableExcept : return ast . Assign ([ targets ], node . value ) # If there are more than one item in `node.targets`, it means multiple # assignments like `a = b = c`. For unpacking like `(a, b) = c`, it # is represented as one tuple as a target item new_nodes = [] for target in node . targets : new_nodes . append ( do_visit_assign ( target )) return new_nodes def handleType_AnnAssign ( self , node : ast . AnnAssign ) -> Any : x = node . target assert isinstance ( x , ast . Name ) assert node . value is None x_str = ast . Constant ( x . id ) Ty = node . annotation intermediate = f 'freetensor__annotate__ { x . id } ' intermediate_store = ast . Name ( intermediate , ast . Store ()) intermediate_load = ast . Name ( intermediate , ast . Load ()) node = [ ast . Assign ([ intermediate_store ], call_helper ( StagingOverload . annotate_stmt , x_str , Ty )), ast . If ( intermediate_load , [ ast . Assign ([ x ], intermediate_load )], []) ] return node def visit_AnnAssign ( self , old_node : ast . AnnAssign ) -> Any : '''Rule: `x: Ty` -> ``` freetensor__annotate__x = annotate_stmt('x', Ty) if freetensor__annotate__x: x = freetensor__annotate__x ```: pure annotation ''' node : ast . AnnAssign = self . generic_visit ( old_node ) if isinstance ( node . target , ast . Name ) and node . value is None : node = self . handleType_AnnAssign ( node ) return node def visit_For ( self , old_node : ast . For ): '''Rule: ``` for x in iter: body ``` -> ``` def for_body(x): body foreach('x', iter, for_body) ```''' if len ( old_node . orelse ) == 0 : with NonlocalTransformingScope ( self ) as nonlocals : # While opening a fake function, For loops initiates an iter name as # well. Need to remove it from the outer nonlocals list to implement # shadowing. Only For loops behaves as such, so handle it specially here. nonlocals = set ( nonlocals ) def recursive_remove_id ( target ): if isinstance ( target , ast . Name ): if target . id in nonlocals : nonlocals . remove ( target . id ) else : assert isinstance ( target , ast . Tuple ) for t in target . elts : recursive_remove_id ( t ) recursive_remove_id ( old_node . target ) nonlocals = list ( nonlocals ) def recursive_get_names ( target ): if isinstance ( target , ast . Name ): return ast . Constant ( target . id ) else : l = [] assert isinstance ( target , ast . Tuple ) for t in target . elts : l . append ( recursive_get_names ( t )) return ast . Tuple ( l , ast . Load ()) target_names = recursive_get_names ( old_node . target ) node : ast . For = self . generic_visit ( old_node ) node = [ function_helper ( 'for_body' , [ '__item__' ], [ ast . Assign ([ node . target ], ast . Name ( '__item__' , ast . Load ())) ] + node . body , nonlocals ), ast . Expr ( call_helper ( StagingOverload . foreach , target_names , node . iter , ast . Name ( 'for_body' , ast . Load ()))) ] else : node = self . generic_visit ( old_node ) return node def visit_While ( self , old_node : ast . While ) -> Any : '''Rule: ``` while pred: body ``` -> ``` def while_body(): body while_stmt(lambda: pred, while_body) ```''' with NonlocalTransformingScope ( self ) as nonlocals : node : ast . While = self . generic_visit ( old_node ) node = [ function_helper ( 'while_body' , [], node . body , nonlocals ), ast . Expr ( call_helper ( StagingOverload . while_stmt , ast . Lambda ( _EMPTY_ARGS , node . test ), ast . Name ( 'while_body' , ast . Load ()))) ] return node def visit_If ( self , old_node : ast . If ): '''Rule: ``` if pred: body else: orelse ``` -> ``` def then_body(): body def else_body(): orelse if_then_else_stmt(pred, then_body, else_body) ``` ''' test = self . visit ( old_node . test ) with NonlocalTransformingScope ( self ) as nonlocals : new_node = [ function_helper ( 'then_body' , [], [ z for x in old_node . body for y in [ self . visit ( x )] for z in ( y if isinstance ( y , list ) else [ y ]) ], nonlocals ) ] then_body = ast . Name ( 'then_body' , ast . Load ()) if old_node . orelse : with NonlocalTransformingScope ( self ) as nonlocals : new_node . append ( function_helper ( 'else_body' , [], [ z for x in old_node . orelse for y in [ self . visit ( x )] for z in ( y if isinstance ( y , list ) else [ y ]) ], nonlocals )) else_body = ast . Name ( 'else_body' , ast . Load ()) else : else_body = ast . Constant ( None ) new_node . append ( ast . Expr ( call_helper ( StagingOverload . if_then_else_stmt , test , then_body , else_body ))) return new_node def visit_IfExp ( self , old_node : ast . IfExp ): '''Rule: `body if test else orelse` -> `if_then_else_expr(test, body, orelse)`''' node = self . generic_visit ( old_node ) node = call_helper ( StagingOverload . if_then_else_expr , node . test , ast . Lambda ( _EMPTY_ARGS , node . body ), ast . Lambda ( _EMPTY_ARGS , node . orelse )) return node def visit_FunctionDef ( self , old_node : ast . FunctionDef ) -> Any : prev_func = self . curr_func self . curr_func = old_node . name # nested functions follow original Python (shitty) scoping, # thus backup the nonlocals stack and prepare a clean one. prev_nonlocals = self . nonlocals self . nonlocals = None with NonlocalTransformingScope ( self ): # mark arguments as nonlocal for name in old_node . args . args + old_node . args . kwonlyargs : self . nonlocals [ - 1 ] . append ( name . arg ) if old_node . args . vararg : self . nonlocals [ - 1 ] . append ( old_node . args . vararg . arg ) if old_node . args . kwarg : self . nonlocals [ - 1 ] . append ( old_node . args . kwarg . arg ) # Transform the function body node : ast . FunctionDef = self . generic_visit ( old_node ) # Cleanup the decorators node . decorator_list = [ call_helper ( StagingOverload . functiondef_decorator , ast . Constant ( self . filename )) ] # Handle the type annotations node . body = [ stmt for arg in node . args . posonlyargs + node . args . args if arg . annotation for stmt in self . handleType_AnnAssign ( ast . AnnAssign ( ast . Name ( arg . arg , ast . Store ()), arg . annotation , None , 1 )) ] + node . body # Cleanup annotations; we don't need them anymore for arg in [ node . args . vararg , node . args . kwarg ] + node . args . posonlyargs + node . args . args + node . args . kwonlyargs : if arg is not None : arg . annotation = None self . curr_func = prev_func self . nonlocals = prev_nonlocals return node def visit_Assert ( self , old_node : ast . Assert ) -> Any : node : ast . Assert = self . generic_visit ( old_node ) node = ast . Expr ( call_helper ( StagingOverload . assert_stmt , node . test )) return node def visit_BoolOp ( self , old_node : ast . BoolOp ) -> Any : node : ast . BoolOp = self . generic_visit ( old_node ) if isinstance ( node . op , ast . And ): libfunc = StagingOverload . and_expr elif isinstance ( node . op , ast . Or ): libfunc = StagingOverload . or_expr else : return node node = call_helper ( libfunc , * [ ast . Lambda ( _EMPTY_ARGS , v ) for v in node . values ]) return node def visit_UnaryOp ( self , old_node : ast . UnaryOp ) -> Any : node : ast . UnaryOp = self . generic_visit ( old_node ) if isinstance ( node . op , ast . Not ): node = call_helper ( StagingOverload . not_expr , node . operand ) return node def visit_Compare ( self , old_node : ast . Compare ) -> Any : '''Expand multiple comparison into `and` expression.''' if len ( old_node . comparators ) == 1 : return self . generic_visit ( old_node ) lhs = old_node . left node = ast . BoolOp ( ast . And (), []) for op , rhs in zip ( old_node . ops , old_node . comparators ): node . values . append ( ast . Compare ( lhs , [ op ], [ rhs ])) lhs = rhs return self . visit ( node ) def visit_Attribute ( self , old_node : ast . Attribute ) -> Any : node : ast . Attribute = self . generic_visit ( old_node ) if isinstance ( node . ctx , ast . Load ): if not ( isinstance ( node . value , ast . Name ) and node . value . id == '__staging_overload__' ): node = call_helper ( StagingOverload . load_attr , node . value , ast . Constant ( node . attr )) return node def visit_Return ( self , old_node : ast . Return ) -> Any : node : ast . Return = self . generic_visit ( old_node ) assert self . curr_func is not None node = ast . Expr ( call_helper ( StagingOverload . return_stmt , node . value , ast . Constant ( self . curr_func ))) return node def visit_Lambda ( self , old_node : ast . Lambda ) -> Any : with NonlocalTransformingScope ( self ): node : ast . Lambda = self . generic_visit ( old_node ) return node def visit_comprehension ( self , old_node : ast . comprehension ) -> Any : with NonlocalTransformingScope ( self ): node : ast . comprehension = self . generic_visit ( old_node ) return node def visit_Name ( self , node : ast . Name ) -> Any : if isinstance ( node . ctx , ast . Store ): self . nonlocals [ - 1 ] . append ( node . id ) return self . generic_visit ( node ) def visit_AsyncFunctionDef ( self , node : ast . AsyncFunctionDef ) -> Any : raise TransformError ( 'Async functions not supported.' , self . filename , self . base_lineno , node ) def visit_ClassDef ( self , node : ast . ClassDef ) -> Any : raise TransformError ( 'Class definitions not supported.' , self . filename , self . base_lineno , node ) def visit_Yield ( self , node : ast . Yield ) -> Any : raise NotImplementedError () def visit_YieldFrom ( self , node : ast . YieldFrom ) -> Any : raise NotImplementedError () def visit_Break ( self , node : ast . Break ) -> Any : return ast . Expr ( call_helper ( StagingOverload . break_stmt )) def visit_Continue ( self , node : ast . Continue ) -> Any : return ast . Expr ( call_helper ( StagingOverload . continue_stmt ))","title":"Transformer"},{"location":"api/#freetensor.core.staging.Transformer.generic_visit","text":"Called if no explicit visitor function exists for a node. Source code in freetensor/core/staging.py def generic_visit ( self , node ): for field , old_value in iter_fields ( node ): if isinstance ( old_value , list ): new_values = [] for value in old_value : if isinstance ( value , AST ): value = self . visit ( value ) if value is None : continue elif not isinstance ( value , AST ): new_values . extend ( value ) continue new_values . append ( value ) old_value [:] = new_values elif isinstance ( old_value , AST ): new_node = self . visit ( old_value ) if new_node is None : delattr ( node , field ) else : setattr ( node , field , new_node ) return node","title":"generic_visit()"},{"location":"api/#freetensor.core.staging.Transformer.visit","text":"Visit a node. Source code in freetensor/core/staging.py def visit ( self , node : ast . AST ): new_node = super () . visit ( node ) if isinstance ( node , ast . stmt ) and not isinstance ( node , ast . FunctionDef ): if not isinstance ( new_node , list ): new_node = [ new_node ] return [ ast . Expr ( call_helper ( StagingOverload . mark_position , ast . Constant ( self . base_lineno + node . lineno - 1 ))) ] + new_node return new_node","title":"visit()"},{"location":"api/#freetensor.core.staging.Transformer.visit_AnnAssign","text":"Rule: x: Ty -> freetensor__annotate__x = annotate_stmt('x', Ty) if freetensor__annotate__x: x = freetensor__annotate__x : pure annotation Source code in freetensor/core/staging.py def visit_AnnAssign ( self , old_node : ast . AnnAssign ) -> Any : '''Rule: `x: Ty` -> ``` freetensor__annotate__x = annotate_stmt('x', Ty) if freetensor__annotate__x: x = freetensor__annotate__x ```: pure annotation ''' node : ast . AnnAssign = self . generic_visit ( old_node ) if isinstance ( node . target , ast . Name ) and node . value is None : node = self . handleType_AnnAssign ( node ) return node","title":"visit_AnnAssign()"},{"location":"api/#freetensor.core.staging.Transformer.visit_Assign","text":"Rule: lhs = rhs -> lhs = unpack_assign_stmt('lhs', rhs) x.lhs = rhs -> x.lhs = unpack_assign_stmt('lhs', rhs) a, (b, c) = (x, (y, z)) -> a, (b, c) = unpack_assign_stmt(('a', ('b', 'c')), (x, (y, z))) a = b = c -> a = unpack_assign_stmt('a', c); b = unpack_assign_stmt('b', c) If unpack_assign_stmt is not overloaded, assign_stmt will be called for each item Source code in freetensor/core/staging.py def visit_Assign ( self , old_node : ast . Assign ) -> ast . Assign : '''Rule: `lhs = rhs` -> `lhs = unpack_assign_stmt('lhs', rhs)` `x.lhs = rhs` -> `x.lhs = unpack_assign_stmt('lhs', rhs)` `a, (b, c) = (x, (y, z))` -> `a, (b, c) = unpack_assign_stmt(('a', ('b', 'c')), (x, (y, z)))` `a = b = c` -> `a = unpack_assign_stmt('a', c); b = unpack_assign_stmt('b', c)` If `unpack_assign_stmt` is not overloaded, `assign_stmt` will be called for each item ''' node : ast . Assign = self . generic_visit ( old_node ) class UnoverloadableExcept ( BaseException ): pass def recursive_get_names ( target ): if isinstance ( target , ast . Name ): return ast . Constant ( target . id ) elif isinstance ( target , ast . Attribute ): return ast . Constant ( target . attr ) elif isinstance ( target , ast . Tuple ): # Unpacking: (a, b) = c l = [] for t in target . elts : l . append ( recursive_get_names ( t )) return ast . Tuple ( l , ast . Load ()) else : raise UnoverloadableExcept () def do_visit_assign ( targets ): try : names = recursive_get_names ( targets ) return ast . Assign ([ targets ], call_helper ( StagingOverload . unpack_assign_stmt , names , node . value )) except UnoverloadableExcept : return ast . Assign ([ targets ], node . value ) # If there are more than one item in `node.targets`, it means multiple # assignments like `a = b = c`. For unpacking like `(a, b) = c`, it # is represented as one tuple as a target item new_nodes = [] for target in node . targets : new_nodes . append ( do_visit_assign ( target )) return new_nodes","title":"visit_Assign()"},{"location":"api/#freetensor.core.staging.Transformer.visit_Compare","text":"Expand multiple comparison into and expression. Source code in freetensor/core/staging.py def visit_Compare ( self , old_node : ast . Compare ) -> Any : '''Expand multiple comparison into `and` expression.''' if len ( old_node . comparators ) == 1 : return self . generic_visit ( old_node ) lhs = old_node . left node = ast . BoolOp ( ast . And (), []) for op , rhs in zip ( old_node . ops , old_node . comparators ): node . values . append ( ast . Compare ( lhs , [ op ], [ rhs ])) lhs = rhs return self . visit ( node )","title":"visit_Compare()"},{"location":"api/#freetensor.core.staging.Transformer.visit_For","text":"Rule: for x in iter: body -> def for_body(x): body foreach('x', iter, for_body) Source code in freetensor/core/staging.py def visit_For ( self , old_node : ast . For ): '''Rule: ``` for x in iter: body ``` -> ``` def for_body(x): body foreach('x', iter, for_body) ```''' if len ( old_node . orelse ) == 0 : with NonlocalTransformingScope ( self ) as nonlocals : # While opening a fake function, For loops initiates an iter name as # well. Need to remove it from the outer nonlocals list to implement # shadowing. Only For loops behaves as such, so handle it specially here. nonlocals = set ( nonlocals ) def recursive_remove_id ( target ): if isinstance ( target , ast . Name ): if target . id in nonlocals : nonlocals . remove ( target . id ) else : assert isinstance ( target , ast . Tuple ) for t in target . elts : recursive_remove_id ( t ) recursive_remove_id ( old_node . target ) nonlocals = list ( nonlocals ) def recursive_get_names ( target ): if isinstance ( target , ast . Name ): return ast . Constant ( target . id ) else : l = [] assert isinstance ( target , ast . Tuple ) for t in target . elts : l . append ( recursive_get_names ( t )) return ast . Tuple ( l , ast . Load ()) target_names = recursive_get_names ( old_node . target ) node : ast . For = self . generic_visit ( old_node ) node = [ function_helper ( 'for_body' , [ '__item__' ], [ ast . Assign ([ node . target ], ast . Name ( '__item__' , ast . Load ())) ] + node . body , nonlocals ), ast . Expr ( call_helper ( StagingOverload . foreach , target_names , node . iter , ast . Name ( 'for_body' , ast . Load ()))) ] else : node = self . generic_visit ( old_node ) return node","title":"visit_For()"},{"location":"api/#freetensor.core.staging.Transformer.visit_If","text":"Rule: if pred: body else: orelse -> def then_body(): body def else_body(): orelse if_then_else_stmt(pred, then_body, else_body) Source code in freetensor/core/staging.py def visit_If ( self , old_node : ast . If ): '''Rule: ``` if pred: body else: orelse ``` -> ``` def then_body(): body def else_body(): orelse if_then_else_stmt(pred, then_body, else_body) ``` ''' test = self . visit ( old_node . test ) with NonlocalTransformingScope ( self ) as nonlocals : new_node = [ function_helper ( 'then_body' , [], [ z for x in old_node . body for y in [ self . visit ( x )] for z in ( y if isinstance ( y , list ) else [ y ]) ], nonlocals ) ] then_body = ast . Name ( 'then_body' , ast . Load ()) if old_node . orelse : with NonlocalTransformingScope ( self ) as nonlocals : new_node . append ( function_helper ( 'else_body' , [], [ z for x in old_node . orelse for y in [ self . visit ( x )] for z in ( y if isinstance ( y , list ) else [ y ]) ], nonlocals )) else_body = ast . Name ( 'else_body' , ast . Load ()) else : else_body = ast . Constant ( None ) new_node . append ( ast . Expr ( call_helper ( StagingOverload . if_then_else_stmt , test , then_body , else_body ))) return new_node","title":"visit_If()"},{"location":"api/#freetensor.core.staging.Transformer.visit_IfExp","text":"Rule: body if test else orelse -> if_then_else_expr(test, body, orelse) Source code in freetensor/core/staging.py def visit_IfExp ( self , old_node : ast . IfExp ): '''Rule: `body if test else orelse` -> `if_then_else_expr(test, body, orelse)`''' node = self . generic_visit ( old_node ) node = call_helper ( StagingOverload . if_then_else_expr , node . test , ast . Lambda ( _EMPTY_ARGS , node . body ), ast . Lambda ( _EMPTY_ARGS , node . orelse )) return node","title":"visit_IfExp()"},{"location":"api/#freetensor.core.staging.Transformer.visit_While","text":"Rule: while pred: body -> def while_body(): body while_stmt(lambda: pred, while_body) Source code in freetensor/core/staging.py def visit_While ( self , old_node : ast . While ) -> Any : '''Rule: ``` while pred: body ``` -> ``` def while_body(): body while_stmt(lambda: pred, while_body) ```''' with NonlocalTransformingScope ( self ) as nonlocals : node : ast . While = self . generic_visit ( old_node ) node = [ function_helper ( 'while_body' , [], node . body , nonlocals ), ast . Expr ( call_helper ( StagingOverload . while_stmt , ast . Lambda ( _EMPTY_ARGS , node . test ), ast . Name ( 'while_body' , ast . Load ()))) ] return node","title":"visit_While()"},{"location":"api/#freetensor.core.staging.call_helper","text":"Call helper that generates a python AST Call node with given callee (overload member) and arguments AST node. Source code in freetensor/core/staging.py def call_helper ( callee , * args : ast . expr , ** kwargs : ast . expr ): '''Call helper that generates a python AST Call node with given callee (overload member) and arguments AST node.''' return ast . Call ( ast . Attribute ( ast . Name ( '__staging_overload__' , ast . Load ()), callee . __name__ , ast . Load ()), list ( args ), [ ast . keyword ( k , w ) for k , w in kwargs . items ()])","title":"call_helper()"},{"location":"api/#freetensor.core.staging.function_helper","text":"Function helper that generates a python AST FunctionDef node with given name, arguments name, and body. Source code in freetensor/core/staging.py def function_helper ( name : str , args : Sequence [ str ], body : List [ ast . stmt ], nonlocals : List [ str ]): '''Function helper that generates a python AST FunctionDef node with given name, arguments name, and body.''' nonlocal_body = ([ ast . Nonlocal ( nonlocals )] if len ( nonlocals ) > 0 else []) + body return ast . FunctionDef ( name = name , args = ast . arguments ( args = [], vararg = None , kwarg = None , posonlyargs = [ ast . arg ( a , None ) for a in args ], defaults = [], kwonlyargs = [], kw_defaults = []), body = nonlocal_body , returns = None , decorator_list = [])","title":"function_helper()"},{"location":"api/#freetensor.core.stmt","text":"Facility to build AST statements Classes and functions in this module are internally used by transformer to construct ASTs. They are also used by some internal tests. API of these classes and functions are subject to changes. End users are encouraged to use transformer , instead of this module. Classes and functions in this module are all in BigCamel naming style, to distinguish from expressions in expr.py","title":"stmt"},{"location":"api/#freetensor.core.stmt.Assert","text":"Scope used to create an Assert node This scope is internally used by transformer and tests E.g.: with Assert(i > 0): ... # Assertion body Source code in freetensor/core/stmt.py class Assert : ''' Scope used to create an Assert node This scope is internally used by `transformer` and tests E.g.: ``` with Assert(i > 0): ... # Assertion body ``` ''' def __init__ ( self , cond ): self . cond = cond def __enter__ ( self ): ctx_stack . push () def __exit__ ( self , exc_type , exc_value , traceback ): if exc_value is not None : # Do not generate an AST node return False # Do not suppress the exception body = ctx_stack . pop () . make_stmt () top = ctx_stack . top () top . append_stmt ( ffi . makeAssert ( self . cond , body , top . get_metadata ()))","title":"Assert"},{"location":"api/#freetensor.core.stmt.Else","text":"Scope used to create an else branch of an If node This scope is internally used by transformer and tests E.g.: with If(i > 0): ... # True branch with Else(): ... # Else branch Source code in freetensor/core/stmt.py class Else : ''' Scope used to create an else branch of an If node This scope is internally used by `transformer` and tests E.g.: ``` with If(i > 0): ... # True branch with Else(): ... # Else branch ``` ''' def __init__ ( self ): pass def __enter__ ( self ): ctx_stack . push () def __exit__ ( self , exc_type , exc_value , traceback ): if exc_value is not None : # Do not generate an AST node return False # Do not suppress the exception body = ctx_stack . pop () . make_stmt () ctx_stack . top () . append_if_else_stmt ( body )","title":"Else"},{"location":"api/#freetensor.core.stmt.For","text":"Scope used to create a For node This scope is internally used by transformer and tests E.g.: with For('i', 0, n) as i: ... # Loop body Source code in freetensor/core/stmt.py class For : ''' Scope used to create a For node This scope is internally used by `transformer` and tests E.g.: ``` with For('i', 0, n) as i: ... # Loop body ``` ''' def __init__ ( self , iter_var : str , begin , end , step = 1 , label : Optional [ str ] = None , no_deps : Optional [ Sequence [ str ]] = None , prefer_libs : Optional [ bool ] = None ): self . iter_var = iter_var self . begin = begin self . end = end self . step = step self . label = label self . no_deps = no_deps self . prefer_libs = prefer_libs self . borrowed_vardefs = set () for x in [ begin , end , step ]: for name in ffi . all_reads ( ffi . Expr ( x )): self . borrowed_vardefs . add ( open_vardefs [ name ]) def __enter__ ( self ): for item in self . borrowed_vardefs : item . lend_out () ctx_stack . push () return ffi . makeVar ( self . iter_var ) def __exit__ ( self , exc_type , exc_value , traceback ): for item in self . borrowed_vardefs : item . reclaim () if exc_value is not None : # Do not generate an AST node return False # Do not suppress the exception body = ctx_stack . pop () . make_stmt () top = ctx_stack . top () top . append_for_stmt ( self . iter_var , self . begin , self . end , self . step , body , metadata = ffi . SourceMetadata ([ self . label ]) if self . label is not None else None , no_deps = self . no_deps , prefer_libs = self . prefer_libs )","title":"For"},{"location":"api/#freetensor.core.stmt.If","text":"Scope used to create an If node This scope is internally used by transformer and tests E.g.: with If(i > 0): ... # Branch body Source code in freetensor/core/stmt.py class If : ''' Scope used to create an If node This scope is internally used by `transformer` and tests E.g.: ``` with If(i > 0): ... # Branch body ``` ''' def __init__ ( self , cond ): self . cond = cond def __enter__ ( self ): ctx_stack . push () def __exit__ ( self , exc_type , exc_value , traceback ): if exc_value is not None : # Do not generate an AST node return False # Do not suppress the exception body = ctx_stack . pop () . make_stmt () ctx_stack . top () . append_if_then_stmt ( self . cond , body )","title":"If"},{"location":"api/#freetensor.core.stmt.Invoke","text":"Inlined invocation of another AST Invoke is used as a scope ( with Invoke(...) as returned_vars ), so that variables returned by the callee can be used in the socpe Invoke can be used for invoking a gradient function, which has already been lowered as an AST. Please note that once a user function has been lowered as an AST, the dimensionalities of its tensors get fixed. Therefore, to invoke ordinary user functions, please use inline in transformer instead, which supports generic types Source code in freetensor/core/stmt.py class Invoke : ''' Inlined invocation of another AST `Invoke` is used as a scope (`with Invoke(...) as returned_vars`), so that variables returned by the callee can be used in the socpe `Invoke` can be used for invoking a gradient function, which has already been lowered as an AST. Please note that once a user function has been lowered as an AST, the dimensionalities of its tensors get fixed. Therefore, to invoke ordinary user functions, please use `inline` in `transformer` instead, which supports generic types ''' def __init__ ( self , ret_names : Sequence [ str ], func : ffi . Func , args : Sequence = [], kvs : Mapping = {}): self . args = args self . kvs = kvs self . func , returns = ffi . strip_returns ( func ) self . vardefs = [] # Outer to inner assert len ( ret_names ) == len ( returns ) for name , ret in zip ( ret_names , returns ): self . vardefs . append ( _VarDef ( name , ret . tensor . shape , ret . tensor . dtype , \"cache\" , ret . mtype )) def __enter__ ( self ): varrefs = [] ret_names = [] for vardef in self . vardefs : varref = vardef . __enter__ () varrefs . append ( varref ) ret_names . append ( varref . name ) ctx_stack . top () . append_stmt ( ffi . inlined_invoke ( ctx_stack . top () . get_metadata (), self . func , self . args , self . kvs , ret_names )) return varrefs [ 0 ] if len ( varrefs ) == 1 else tuple ( varrefs ) def __exit__ ( self , exc_type , exc_value , traceback ): for vardef in reversed ( self . vardefs ): vardef . __exit__ ( exc_type , exc_value , traceback )","title":"Invoke"},{"location":"api/#freetensor.core.stmt.NamedScope","text":"Scope used to create an StmtSeq node with an explicit ID E.g.: with NamedScope(): ... # body This scope is used for testing only. StmtSeq nodes can be deleted in many lowering passes Source code in freetensor/core/stmt.py class NamedScope : ''' Scope used to create an StmtSeq node with an explicit ID E.g.: ``` with NamedScope(): ... # body ``` This scope is used for testing only. StmtSeq nodes can be deleted in many lowering passes ''' def __init__ ( self , * labels : str ): self . labels = labels def __enter__ ( self ): ctx_stack . push () def __exit__ ( self , exc_type , exc_value , traceback ): if exc_value is not None : # Do not generate an AST node return False # Do not suppress the exception finished_scope = ctx_stack . pop () metadata = ctx_stack . top () . get_metadata ( self . labels ) body = finished_scope . make_stmt ( metadata ) ctx_stack . top () . append_stmt ( body )","title":"NamedScope"},{"location":"api/#freetensor.core.stmt.Any","text":"Create an Any node (only for testing) Any nodes matches any statement nodes in ast.match Source code in freetensor/core/stmt.py def Any (): ''' Create an Any node (only for testing) Any nodes matches any statement nodes in `ast.match` ''' ctx_stack . top () . append_stmt ( ffi . makeAny ())","title":"Any()"},{"location":"api/#freetensor.core.stmt.Eval","text":"Create an Eval node This scope is internally used by transformer and tests Source code in freetensor/core/stmt.py def Eval ( expr ): ''' Create an Eval node This scope is internally used by `transformer` and tests ''' top = ctx_stack . top () top . append_stmt ( ffi . makeEval ( expr , top . get_metadata ()))","title":"Eval()"},{"location":"api/#freetensor.core.stmt.MarkLabel","text":"Mark the ID of the following statement This scope is internally used by transformer and tests Source code in freetensor/core/stmt.py def MarkLabel ( label : str ): \"\"\" Mark the ID of the following statement This scope is internally used by `transformer` and tests \"\"\" ctx_stack . top () . add_label ( label )","title":"MarkLabel()"},{"location":"api/#freetensor.core.stmt.VarDef","text":"A factory function that creates a VarDef or a series of nested VarDef s This scope is internally used by transformer and tests Source code in freetensor/core/stmt.py def VarDef ( * args , ** kvs ): ''' A factory function that creates a VarDef or a series of nested `VarDef`s This scope is internally used by `transformer` and tests ''' if len ( args ) == 1 : return _VarsDef ( args [ 0 ]) else : return _VarDef ( * args , ** kvs )","title":"VarDef()"},{"location":"api/#freetensor.libop","text":"","title":"libop"},{"location":"api/#freetensor.libop.assign","text":"","title":"assign"},{"location":"api/#freetensor.libop.assign.add_to","text":"(Broadcasted) add to a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor Source code in freetensor/libop/assign.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"add_to()"},{"location":"api/#freetensor.libop.assign.assign","text":"(Broadcasted) assign to a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor Source code in freetensor/libop/assign.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"assign()"},{"location":"api/#freetensor.libop.assign.floordiv_to","text":"(Broadcasted) rounding-towards-negative-infinity integer division (following Python convention, but not C) from a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor Source code in freetensor/libop/assign.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"floordiv_to()"},{"location":"api/#freetensor.libop.assign.mod_to","text":"(Broadcasted) modulo (results are non-negative, following Python convention, but not C) from a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor Source code in freetensor/libop/assign.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"mod_to()"},{"location":"api/#freetensor.libop.assign.mul_to","text":"(Broadcasted) multiply to a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor Source code in freetensor/libop/assign.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"mul_to()"},{"location":"api/#freetensor.libop.assign.sub_to","text":"(Broadcasted) subtract from a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor Source code in freetensor/libop/assign.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"sub_to()"},{"location":"api/#freetensor.libop.assign.truediv_to","text":"(Broadcasted) floating-point division from a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor Source code in freetensor/libop/assign.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"truediv_to()"},{"location":"api/#freetensor.libop.constant","text":"","title":"constant"},{"location":"api/#freetensor.libop.constant.zeros","text":"Create a zero tensor Parameters: shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used Returns: The zero tensor Source code in freetensor/libop/constant.py @core . inline def zeros ( shape , dtype , mtype = None ): ''' Create a zero tensor Parameters ---------- shape : Sequence[Expr] or Var Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype : str or DataType Data type of the variable mtype : str or MemType (Optional) Memory type of the variable. If omitted, the main memory type of the default Target in config will be used Returns ------- VarRef : The zero tensor ''' y = core . empty ( shape , dtype , mtype ) #! label: recur zeros_ ( y ) return y","title":"zeros()"},{"location":"api/#freetensor.libop.constant.zeros_","text":"Fill zeros to a tensor Parameters: y ( VarRef ) \u2013 The tensor to fill Source code in freetensor/libop/constant.py @core . inline def zeros_ ( y ): ''' Fill zeros to a tensor Parameters ---------- y : VarRef The tensor to fill ''' if core . ndim ( y ) == 0 : y [()] = core . zero_value ( y . dtype ) else : #! label: L_elem for i in range ( core . shape ( y , 0 )): #! label: recur zeros_ ( y [ i ])","title":"zeros_()"},{"location":"api/#freetensor.libop.conv","text":"","title":"conv"},{"location":"api/#freetensor.libop.conv.conv","text":"Convolution. The result is returned Parameters follow ONNX convention. Currently only 2-D convolution is supported Source code in freetensor/libop/conv.py @core . inline def conv ( X , W , B = None , auto_pad : str = 'NOTSET' , dilations : Optional [ Sequence [ int ]] = None , group : int = 1 , kernel_shape : Optional [ Sequence [ int ]] = None , pads : Optional [ Sequence [ int ]] = None , strides : Optional [ Sequence [ int ]] = None ): ''' Convolution. The result is returned Parameters follow ONNX convention. Currently only 2-D convolution is supported ''' n_spatial_dim = 2 # Currently only 2-D convolution is supported (TODO) if dilations is None : dilations = [ 1 ] * n_spatial_dim if strides is None : strides = [ 1 ] * n_spatial_dim if pads is None : if auto_pad == 'VALID' : pads = list ( zip ( * ([[ 0 , 0 ]] * n_spatial_dim ))) pads = pads [ 0 ] + pads [ 1 ] elif auto_pad == 'SAME_UPPER' : assert kernel_shape is not None , \"SAME_UPPER pad with dynamic kernel_shape is currently not supported\" # TODO pads = list ( zip ( * [ calc_same_upper_pad ( dil , kern , stride ) for dil , kern , stride in zip ( dilations , kernel_shape , strides ) ])) pads = pads [ 0 ] + pads [ 1 ] elif auto_pad == 'SAME_LOWER' : assert kernel_shape is not None , \"SAME_UPPER pad with dynamic kernel_shape is currently not supported\" # TODO pads = list ( zip ( * [ calc_same_lower_pad ( dil , kern , stride ) for dil , kern , stride in zip ( dilations , kernel_shape , strides ) ])) pads = pads [ 0 ] + pads [ 1 ] else : assert False , \"auto_pad should be set if pads is not specified\" dtype = core . up_cast ( X . dtype , W . dtype ) mtype = core . same_mtype ( X . mtype , W . mtype ) if B is not None : dtype = core . up_cast ( dtype , B . dtype ) mtype = core . same_mtype ( mtype , B . mtype ) #! label: V_Y Y = core . empty ([ X . shape ( 0 ), W . shape ( 0 ), calc_out_size ( X . shape ( 2 ), dilations [ 0 ], W . shape ( 2 ), pads [ 0 ], pads [ 2 ], strides [ 0 ]), calc_out_size ( X . shape ( 3 ), dilations [ 1 ], W . shape ( 3 ), pads [ 1 ], pads [ 3 ], strides [ 1 ]) ], dtype , mtype ) #! label: recur conv_ ( X , W , B , Y , auto_pad , dilations , group , kernel_shape , pads , strides ) return Y","title":"conv()"},{"location":"api/#freetensor.libop.conv.conv_","text":"Convolution. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D convolution is supported Source code in freetensor/libop/conv.py @core . inline def conv_ ( X , W , B , Y , auto_pad : str = 'NOTSET' , dilations : Optional [ Sequence [ int ]] = None , group : int = 1 , kernel_shape : Optional [ Sequence [ int ]] = None , pads : Optional [ Sequence [ int ]] = None , strides : Optional [ Sequence [ int ]] = None ): ''' Convolution. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D convolution is supported ''' n_spatial_dim = 2 # Currently only 2-D convolution is supported (TODO) if dilations is None : dilations = [ 1 ] * n_spatial_dim if strides is None : strides = [ 1 ] * n_spatial_dim if pads is None : if auto_pad == 'VALID' : pads = list ( zip ( * ([[ 0 , 0 ]] * n_spatial_dim ))) pads = pads [ 0 ] + pads [ 1 ] elif auto_pad == 'SAME_UPPER' : assert kernel_shape is not None , \"SAME_UPPER pad with dynamic kernel_shape is currently not supported\" # TODO pads = list ( zip ( * [ calc_same_upper_pad ( dil , kern , stride ) for dil , kern , stride in zip ( dilations , kernel_shape , strides ) ])) pads = pads [ 0 ] + pads [ 1 ] elif auto_pad == 'SAME_LOWER' : assert kernel_shape is not None , \"SAME_UPPER pad with dynamic kernel_shape is currently not supported\" # TODO pads = list ( zip ( * [ calc_same_lower_pad ( dil , kern , stride ) for dil , kern , stride in zip ( dilations , kernel_shape , strides ) ])) pads = pads [ 0 ] + pads [ 1 ] else : assert False , \"auto_pad should be set if pads is not specified\" if B is None : # yapf: disable #! label: L_n for n in range ( X . shape ( 0 )): #! label: L_g for g in range ( group ): #! label: L_c_out for c_out in range ( W . shape ( 0 ) // group ): #! label: L_h for h in range ( Y . shape ( 2 )): #! label: L_w for w in range ( Y . shape ( 3 )): #! label: init Y [ n , g * ( W . shape ( 0 ) // group ) + c_out , h , w ] = 0 #! label: L_c_in for c_in in range ( W . shape ( 1 )): #! label: L_kh for kh in range ( W . shape ( 2 )): #! label: L_kw for kw in range ( W . shape ( 3 )): # h_in = h * stride + kh * dilation - pad # w_in = w * stride + kw * dilation - pad if ( h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ] >= 0 and h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ] < X . shape ( 2 ) and w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ] >= 0 and w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ] < X . shape ( 3 )): #! label: compute Y [ n , g * ( W . shape ( 0 ) // group ) + c_out , h , w ] += X [ n , g * W . shape ( 1 ) + c_in , h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ], w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ] ] * W [ g * ( W . shape ( 0 ) // group ) + c_out , c_in , kh , kw ] # yapf: enable else : # yapf: disable #! label: L_n for n in range ( X . shape ( 0 )): #! label: L_g for g in range ( group ): #! label: L_c_out for c_out in range ( W . shape ( 0 ) // group ): #! label: L_h for h in range ( Y . shape ( 2 )): #! label: L_w for w in range ( Y . shape ( 3 )): #! label: init Y [ n , g * ( W . shape ( 0 ) // group ) + c_out , h , w ] = B [ g * ( W . shape ( 0 ) // group ) + c_out ] #! label: L_c_in for c_in in range ( W . shape ( 1 )): #! label: L_kh for kh in range ( W . shape ( 2 )): #! label: L_kw for kw in range ( W . shape ( 3 )): # h_in = h * stride + kh * dilation - pad # w_in = w * stride + kw * dilation - pad if ( h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ] >= 0 and h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ] < X . shape ( 2 ) and w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ] >= 0 and w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ] < X . shape ( 3 )): #! label: compute Y [ n , g * ( W . shape ( 0 ) // group ) + c_out , h , w ] += X [ n , g * W . shape ( 1 ) + c_in , h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ], w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ] ] * W [ g * ( W . shape ( 0 ) // group ) + c_out , c_in , kh , kw ] # yapf: enable","title":"conv_()"},{"location":"api/#freetensor.libop.element_wise","text":"","title":"element_wise"},{"location":"api/#freetensor.libop.element_wise.abs","text":"Element-wise absolute value of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"abs()"},{"location":"api/#freetensor.libop.element_wise.abs_","text":"Element-wise absolute value of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"abs_()"},{"location":"api/#freetensor.libop.element_wise.add","text":"(Broadcasted) element-wise addition of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"add()"},{"location":"api/#freetensor.libop.element_wise.add_","text":"(Broadcasted) element-wise addition of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"add_()"},{"location":"api/#freetensor.libop.element_wise.ceil","text":"Element-wise ceil of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"ceil()"},{"location":"api/#freetensor.libop.element_wise.ceil_","text":"Element-wise ceil of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"ceil_()"},{"location":"api/#freetensor.libop.element_wise.ceildiv","text":"(Broadcasted) element-wise rounding-towards-positive-infinity integer division of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"ceildiv()"},{"location":"api/#freetensor.libop.element_wise.ceildiv_","text":"(Broadcasted) element-wise rounding-towards-positive-infinity integer division of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"ceildiv_()"},{"location":"api/#freetensor.libop.element_wise.eq","text":"(Broadcasted) element-wise equal of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"eq()"},{"location":"api/#freetensor.libop.element_wise.eq_","text":"(Broadcasted) element-wise equal of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"eq_()"},{"location":"api/#freetensor.libop.element_wise.exp","text":"Element-wise natrual exponent of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"exp()"},{"location":"api/#freetensor.libop.element_wise.exp_","text":"Element-wise natrual exponent of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"exp_()"},{"location":"api/#freetensor.libop.element_wise.floor","text":"Element-wise floor of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"floor()"},{"location":"api/#freetensor.libop.element_wise.floor_","text":"Element-wise floor of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"floor_()"},{"location":"api/#freetensor.libop.element_wise.floordiv","text":"(Broadcasted) element-wise rounding-towards-negative-infinity integer division (following Python convention, but not C, recommended for performance) of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"floordiv()"},{"location":"api/#freetensor.libop.element_wise.floordiv_","text":"(Broadcasted) element-wise rounding-towards-negative-infinity integer division (following Python convention, but not C, recommended for performance) of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"floordiv_()"},{"location":"api/#freetensor.libop.element_wise.ge","text":"(Broadcasted) element-wise greater-than-or-equal-to of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"ge()"},{"location":"api/#freetensor.libop.element_wise.ge_","text":"(Broadcasted) element-wise greater-than-or-equal-to of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"ge_()"},{"location":"api/#freetensor.libop.element_wise.gt","text":"(Broadcasted) element-wise greater-than of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"gt()"},{"location":"api/#freetensor.libop.element_wise.gt_","text":"(Broadcasted) element-wise greater-than of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"gt_()"},{"location":"api/#freetensor.libop.element_wise.l_and","text":"(Broadcasted) element-wise logical and of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"l_and()"},{"location":"api/#freetensor.libop.element_wise.l_and_","text":"(Broadcasted) element-wise logical and of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"l_and_()"},{"location":"api/#freetensor.libop.element_wise.l_not","text":"Element-wise logical not of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"l_not()"},{"location":"api/#freetensor.libop.element_wise.l_not_","text":"Element-wise logical not of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"l_not_()"},{"location":"api/#freetensor.libop.element_wise.l_or","text":"(Broadcasted) element-wise logical or of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"l_or()"},{"location":"api/#freetensor.libop.element_wise.l_or_","text":"(Broadcasted) element-wise logical or of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"l_or_()"},{"location":"api/#freetensor.libop.element_wise.le","text":"(Broadcasted) element-wise less-than-or-equal-to of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"le()"},{"location":"api/#freetensor.libop.element_wise.le_","text":"(Broadcasted) element-wise less-than-or-equal-to of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"le_()"},{"location":"api/#freetensor.libop.element_wise.lt","text":"(Broadcasted) element-wise less-than of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"lt()"},{"location":"api/#freetensor.libop.element_wise.lt_","text":"(Broadcasted) element-wise less-than of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"lt_()"},{"location":"api/#freetensor.libop.element_wise.max","text":"(Broadcasted) element-wise maximum of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"max()"},{"location":"api/#freetensor.libop.element_wise.max_","text":"(Broadcasted) element-wise maximum of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"max_()"},{"location":"api/#freetensor.libop.element_wise.min","text":"(Broadcasted) element-wise minimum of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"min()"},{"location":"api/#freetensor.libop.element_wise.min_","text":"(Broadcasted) element-wise minimum of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"min_()"},{"location":"api/#freetensor.libop.element_wise.mod","text":"(Broadcasted) element-wise modulo (results are non-negative, following Python convention, but not C, recommended for performance) of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"mod()"},{"location":"api/#freetensor.libop.element_wise.mod_","text":"(Broadcasted) element-wise modulo (results are non-negative, following Python convention, but not C, recommended for performance) of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"mod_()"},{"location":"api/#freetensor.libop.element_wise.mul","text":"(Broadcasted) element-wise multiplication of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"mul()"},{"location":"api/#freetensor.libop.element_wise.mul_","text":"(Broadcasted) element-wise multiplication of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"mul_()"},{"location":"api/#freetensor.libop.element_wise.ne","text":"(Broadcasted) element-wise non-equal of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"ne()"},{"location":"api/#freetensor.libop.element_wise.ne_","text":"(Broadcasted) element-wise non-equal of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"ne_()"},{"location":"api/#freetensor.libop.element_wise.neg","text":"Element-wise negation of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"neg()"},{"location":"api/#freetensor.libop.element_wise.neg_","text":"Element-wise negation of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"neg_()"},{"location":"api/#freetensor.libop.element_wise.relu","text":"Element-wise ReLU of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"relu()"},{"location":"api/#freetensor.libop.element_wise.relu_","text":"Element-wise ReLU of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"relu_()"},{"location":"api/#freetensor.libop.element_wise.remainder","text":"(Broadcasted) element-wise remainder (results can be positive or negative, following C convention, but not Python, NOT recommended for performance) of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"remainder()"},{"location":"api/#freetensor.libop.element_wise.remainder_","text":"(Broadcasted) element-wise remainder (results can be positive or negative, following C convention, but not Python, NOT recommended for performance) of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"remainder_()"},{"location":"api/#freetensor.libop.element_wise.round_towards_0_div","text":"(Broadcasted) element-wise rounding-towards-0 integer division (following C convention, but not Python, NOT recommended for performance) of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"round_towards_0_div()"},{"location":"api/#freetensor.libop.element_wise.round_towards_0_div_","text":"(Broadcasted) element-wise rounding-towards-0 integer division (following C convention, but not Python, NOT recommended for performance) of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"round_towards_0_div_()"},{"location":"api/#freetensor.libop.element_wise.sigmoid","text":"Element-wise sigmoid of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"sigmoid()"},{"location":"api/#freetensor.libop.element_wise.sigmoid_","text":"Element-wise sigmoid of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"sigmoid_()"},{"location":"api/#freetensor.libop.element_wise.sqrt","text":"Element-wise square root of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"sqrt()"},{"location":"api/#freetensor.libop.element_wise.sqrt_","text":"Element-wise square root of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"sqrt_()"},{"location":"api/#freetensor.libop.element_wise.square","text":"Element-wise square of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"square()"},{"location":"api/#freetensor.libop.element_wise.square_","text":"Element-wise square of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"square_()"},{"location":"api/#freetensor.libop.element_wise.sub","text":"(Broadcasted) element-wise subtraction of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"sub()"},{"location":"api/#freetensor.libop.element_wise.sub_","text":"(Broadcasted) element-wise subtraction of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"sub_()"},{"location":"api/#freetensor.libop.element_wise.tanh","text":"Element-wise tanh of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"tanh()"},{"location":"api/#freetensor.libop.element_wise.tanh_","text":"Element-wise tanh of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"tanh_()"},{"location":"api/#freetensor.libop.element_wise.truediv","text":"(Broadcasted) element-wise floating-point division of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"truediv()"},{"location":"api/#freetensor.libop.element_wise.truediv_","text":"(Broadcasted) element-wise floating-point division of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/element_wise.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"truediv_()"},{"location":"api/#freetensor.libop.matmul","text":"","title":"matmul"},{"location":"api/#freetensor.libop.matmul.einsum","text":"Einstein summation. The result is returned Parameters: fmt ( str ) \u2013 The format string. E.g. \"ik,kj->ij\" represents a matrix multiplcation args ( Sequence[VarRef] ) \u2013 All inputs arguments. E.g. if fmt is \"ik,kj->ij\" , it iterates axis i and k of args[0] , axis k and j of args[1] , axis i and j of the returned value Returns: The result tensor Source code in freetensor/libop/matmul.py @core . inline def einsum ( fmt : str , * args ): ''' Einstein summation. The result is returned Parameters ---------- fmt : str The format string. E.g. `\"ik,kj->ij\"` represents a matrix multiplcation args : Sequence[VarRef] All inputs arguments. E.g. if `fmt` is `\"ik,kj->ij\"`, it iterates axis `i` and `k` of `args[0]`, axis `k` and `j` of `args[1]`, axis `i` and `j` of the returned value Returns ------- VarRef : The result tensor ''' lefts , right = fmt . split ( '->' ) lefts = lefts . split ( ',' ) shapes = [] for v in right : offsets = [ left . find ( v ) for left in lefts ] iter_args , iter_offsets = zip ( * filter ( lambda x : x [ 1 ] != - 1 , zip ( args , offsets ))) assert len ( iter_args ) > 0 shapes . append ( iter_args [ 0 ] . shape ( iter_offsets [ 0 ])) # FIXME: compute dtype and mtype from every inputs Y = core . empty ( shapes , args [ 0 ] . dtype , args [ 0 ] . mtype ) einsum_ ( fmt , * args , Y ) return Y","title":"einsum()"},{"location":"api/#freetensor.libop.matmul.einsum_","text":"Einstein summation. The result is written to the last argument Parameters: fmt ( str ) \u2013 The format string. E.g. \"ik,kj->ij\" represents a matrix multiplcation args ( Sequence[VarRef] ) \u2013 All arguments including inputs and the output. E.g. if fmt is \"ik,kj->ij\" , it iterates axis i and k of args[0] , axis k and j of args[1] , axis i and j of args[2] Source code in freetensor/libop/matmul.py @core . inline def einsum_ ( fmt : str , * args ): ''' Einstein summation. The result is written to the last argument Parameters ---------- fmt : str The format string. E.g. `\"ik,kj->ij\"` represents a matrix multiplcation args : Sequence[VarRef] All arguments including inputs and the output. E.g. if `fmt` is `\"ik,kj->ij\"`, it iterates axis `i` and `k` of `args[0]`, axis `k` and `j` of `args[1]`, axis `i` and `j` of `args[2]` ''' lefts , right = fmt . split ( '->' ) lefts = lefts . split ( ',' ) order = right for left in lefts : for idx in left : if idx not in order : order += idx _einsum_ ( lefts , right , order , True , * args )","title":"einsum_()"},{"location":"api/#freetensor.libop.matmul.gemm","text":"General matrix multiplcation following BLAS convention and return the result It performs Y = alpha tr?(A) @ tr?(B) + C , where @ represents matrix multiplication, tr? represents an optional transposition Parameters: A ( VarRef ) \u2013 The left-hand-side operand of matrix multiplication B ( VarRef ) \u2013 The right-hand-side operand of matrix multiplication C ( VarRef (Optional) ) \u2013 The bias tensor trans_A ( bool ) \u2013 If true, transpose A . Defaults to False trans_B ( bool ) \u2013 If true, transpose B . Defaults to False alpha ( float ) \u2013 Coefficient of tr?(A) @ tr?(B) . Defaults to 1.0 beta ( float ) \u2013 Coefficient of C . Defaults to 1.0 Returns: The resulting tensor Source code in freetensor/libop/matmul.py @core . inline def gemm ( A , B , C = None , has_bias : bool = False , trans_A : bool = False , trans_B : bool = False , alpha : float = 1.0 , beta : float = 1.0 ): ''' General matrix multiplcation following BLAS convention and return the result It performs `Y = alpha tr?(A) @ tr?(B) + C`, where `@` represents matrix multiplication, `tr?` represents an optional transposition Parameters ---------- A : VarRef The left-hand-side operand of matrix multiplication B : VarRef The right-hand-side operand of matrix multiplication C : VarRef (Optional) The bias tensor trans_A : bool (Optional) If true, transpose `A`. Defaults to False trans_B : bool (Optional) If true, transpose `B`. Defaults to False alpha : Number (Optional) Coefficient of `tr?(A) @ tr?(B)`. Defaults to 1.0 beta : Number (Optional) Coefficient of `C`. Defaults to 1.0 Returns ------- VarRef : The resulting tensor ''' dtype = core . up_cast ( A . dtype , B . dtype ) mtype = core . same_mtype ( A . mtype , B . mtype ) if C is not None : dtype = core . up_cast ( dtype , C . dtype ) mtype = core . same_mtype ( mtype , C . mtype ) Y = core . empty ( _comp_shape ( A , B , trans_A , trans_B ), dtype , mtype ) #! label: recur gemm_ ( A , B , C , Y , trans_A , trans_B , alpha , beta ) return Y","title":"gemm()"},{"location":"api/#freetensor.libop.matmul.gemm_","text":"General matrix multiplcation following BLAS convention. The result is written to an existing tensor It performs Y = alpha tr?(A) @ tr?(B) + C , where @ represents matrix multiplication, tr? represents an optional transposition Parameters: A ( VarRef ) \u2013 The left-hand-side operand of matrix multiplication B ( VarRef ) \u2013 The right-hand-side operand of matrix multiplication C ( VarRef (Optional) ) \u2013 The bias tensor Y ( VarRef ) \u2013 The resulting tensor trans_A ( bool ) \u2013 If true, transpose A . Defaults to False trans_B ( bool ) \u2013 If true, transpose B . Defaults to False alpha ( float ) \u2013 Coefficient of tr?(A) @ tr?(B) . Defaults to 1.0 beta ( float ) \u2013 Coefficient of C . Defaults to 1.0 Source code in freetensor/libop/matmul.py @core . inline def gemm_ ( A , B , C , Y , trans_A : bool = False , trans_B : bool = False , alpha : float = 1.0 , beta : float = 1.0 ): ''' General matrix multiplcation following BLAS convention. The result is written to an existing tensor It performs `Y = alpha tr?(A) @ tr?(B) + C`, where `@` represents matrix multiplication, `tr?` represents an optional transposition Parameters ---------- A : VarRef The left-hand-side operand of matrix multiplication B : VarRef The right-hand-side operand of matrix multiplication C : VarRef (Optional) The bias tensor Y : VarRef The resulting tensor trans_A : bool (Optional) If true, transpose `A`. Defaults to False trans_B : bool (Optional) If true, transpose `B`. Defaults to False alpha : Number (Optional) Coefficient of `tr?(A) @ tr?(B)`. Defaults to 1.0 beta : Number (Optional) Coefficient of `C`. Defaults to 1.0 ''' a_fmt = 'ki' if trans_A else 'ik' b_fmt = 'jk' if trans_B else 'kj' fmt = f \" { a_fmt } , { b_fmt } ->ij\" if C is None : #! label: einsum einsum_ ( fmt , A , B , Y ) #! label: mul_to mul_to ( Y , alpha ) else : #! label: einsum einsum_ ( fmt , A , B , Y ) #! label: mul_to mul_to ( Y , alpha ) #! label: add_to add_to ( Y , mul ( beta , C ))","title":"gemm_()"},{"location":"api/#freetensor.libop.matmul.matmul","text":"Matrix multiplcation. The result is returned Parameters: A ( VarRef ) \u2013 The left-hand-side operand B ( VarRef ) \u2013 The right-hand-side operand Returns: The resulting tensor Source code in freetensor/libop/matmul.py @core . inline def matmul ( A , B ): ''' Matrix multiplcation. The result is returned Parameters ---------- A : VarRef The left-hand-side operand B : VarRef The right-hand-side operand Returns ------- VarRef : The resulting tensor ''' #! label: einsum Y = einsum ( _make_matmul_fmt ( A . ndim , B . ndim ), A , B ) return Y","title":"matmul()"},{"location":"api/#freetensor.libop.matmul.matmul_","text":"Matrix multiplcation. The result is written to an existing tensor Parameters: A ( VarRef ) \u2013 The left-hand-side operand B ( VarRef ) \u2013 The right-hand-side operand C ( VarRef ) \u2013 The resulting tensor Source code in freetensor/libop/matmul.py @core . inline def matmul_ ( A , B , Y ): ''' Matrix multiplcation. The result is written to an existing tensor Parameters ---------- A : VarRef The left-hand-side operand B : VarRef The right-hand-side operand C : VarRef The resulting tensor ''' #! label: einsum einsum_ ( _make_matmul_fmt ( A . ndim , B . ndim ), A , B , Y )","title":"matmul_()"},{"location":"api/#freetensor.libop.pooling","text":"","title":"pooling"},{"location":"api/#freetensor.libop.pooling.global_avg_pool","text":"Global averaging pooling. The result is returned Parameters follow ONNX convention. Currently only 2-D pooling is supported Source code in freetensor/libop/pooling.py @core . inline def global_avg_pool ( X ): ''' Global averaging pooling. The result is returned Parameters follow ONNX convention. Currently only 2-D pooling is supported ''' n_spatial_dim = 2 # Currently only 2-D pooling is supported (TODO) Y = core . empty ([ X . shape ( 0 ), X . shape ( 1 )], X . dtype , X . mtype ) #! label: recur global_avg_pool_ ( X , Y ) return Y","title":"global_avg_pool()"},{"location":"api/#freetensor.libop.pooling.global_avg_pool_","text":"Global averaging pooling. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D pooling is supported Source code in freetensor/libop/pooling.py @core . inline def global_avg_pool_ ( X , Y ): ''' Global averaging pooling. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D pooling is supported ''' n_spatial_dim = 2 # Currently only 2-D pooling is supported (TODO) #! label: L_n for n in range ( X . shape ( 0 )): #! label: L_c for c in range ( X . shape ( 1 )): #! label: init Y [ n , c ] = 0 #! label: L_h for h in range ( X . shape ( 2 )): #! label: L_w for w in range ( X . shape ( 3 )): #! label: compute Y [ n , c ] += X [ n , c , h , w ] #! label: flush Y [ n , c ] /= X . shape ( 2 ) * X . shape ( 3 )","title":"global_avg_pool_()"},{"location":"api/#freetensor.libop.pooling.max_pool","text":"Maximum pooling. The result is returned Parameters follow ONNX convention. Currently only 2-D pooling is supported Source code in freetensor/libop/pooling.py @core . inline def max_pool ( X , auto_pad : str = 'NOTSET' , dilations : Optional [ Sequence [ int ]] = None , kernel_shape : Sequence [ int ] = None , pads : Optional [ Sequence [ int ]] = None , strides : Optional [ Sequence [ int ]] = None ): ''' Maximum pooling. The result is returned Parameters follow ONNX convention. Currently only 2-D pooling is supported ''' n_spatial_dim = 2 # Currently only 2-D pooling is supported (TODO) # TODO: ceil_mode # TODO: return_indices if dilations is None : dilations = [ 1 ] * n_spatial_dim if strides is None : # NOTE: strides default to 1 in ONNX, while default to kernel_shape in PyTorch strides = [ 1 ] * n_spatial_dim if pads is None : if auto_pad == 'VALID' : pads = list ( zip ( * ([[ 0 , 0 ]] * n_spatial_dim ))) pads = pads [ 0 ] + pads [ 1 ] elif auto_pad == 'SAME_UPPER' : pads = list ( zip ( * [ calc_same_upper_pad ( dil , kern , stride ) for dil , kern , stride in zip ( dilations , kernel_shape , strides ) ])) pads = pads [ 0 ] + pads [ 1 ] elif auto_pad == 'SAME_LOWER' : pads = list ( zip ( * [ calc_same_lower_pad ( dil , kern , stride ) for dil , kern , stride in zip ( dilations , kernel_shape , strides ) ])) pads = pads [ 0 ] + pads [ 1 ] else : assert False , \"auto_pad should be set if pads is not specified\" Y = core . empty ([ X . shape ( 0 ), X . shape ( 1 ), calc_out_size ( X . shape ( 2 ), dilations [ 0 ], kernel_shape [ 0 ], pads [ 0 ], pads [ 2 ], strides [ 0 ]), calc_out_size ( X . shape ( 3 ), dilations [ 1 ], kernel_shape [ 1 ], pads [ 1 ], pads [ 3 ], strides [ 1 ]) ], X . dtype , X . mtype ) #! label: recur max_pool_ ( X , Y , auto_pad , dilations , kernel_shape , pads , strides ) return Y","title":"max_pool()"},{"location":"api/#freetensor.libop.pooling.max_pool_","text":"Maximum pooling. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D pooling is supported Source code in freetensor/libop/pooling.py @core . inline def max_pool_ ( X , Y , auto_pad : str = 'NOTSET' , dilations : Optional [ Sequence [ int ]] = None , kernel_shape : Sequence [ int ] = None , pads : Optional [ Sequence [ int ]] = None , strides : Optional [ Sequence [ int ]] = None ): ''' Maximum pooling. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D pooling is supported ''' n_spatial_dim = 2 # Currently only 2-D pooling is supported (TODO) # TODO: ceil_mode # TODO: return_indices if dilations is None : dilations = [ 1 ] * n_spatial_dim if strides is None : # NOTE: strides default to 1 in ONNX, while default to kernel_shape in PyTorch strides = [ 1 ] * n_spatial_dim if pads is None : if auto_pad == 'VALID' : pads = list ( zip ( * ([[ 0 , 0 ]] * n_spatial_dim ))) pads = pads [ 0 ] + pads [ 1 ] elif auto_pad == 'SAME_UPPER' : pads = list ( zip ( * [ calc_same_upper_pad ( dil , kern , stride ) for dil , kern , stride in zip ( dilations , kernel_shape , strides ) ])) pads = pads [ 0 ] + pads [ 1 ] elif auto_pad == 'SAME_LOWER' : pads = list ( zip ( * [ calc_same_lower_pad ( dil , kern , stride ) for dil , kern , stride in zip ( dilations , kernel_shape , strides ) ])) pads = pads [ 0 ] + pads [ 1 ] else : assert False , \"auto_pad should be set if pads is not specified\" # yapf: disable #! label: L_n for n in range ( X . shape ( 0 )): #! label: L_c for c in range ( X . shape ( 1 )): #! label: L_h for h in range ( Y . shape ( 2 )): #! label: L_w for w in range ( Y . shape ( 3 )): #! label: init Y [ n , c , h , w ] = core . min_value ( X . dtype ) #! label: L_kh for kh in range ( kernel_shape [ 0 ]): #! label: L_kw for kw in range ( kernel_shape [ 1 ]): # h_in = h * stride + kh * dilation - pad # w_in = w * stride + kw * dilation - pad if ( h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ] >= 0 and h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ] < X . shape ( 2 ) and w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ] >= 0 and w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ] < X . shape ( 3 )): #! label: compute Y [ n , c , h , w ] = core . max ( Y [ n , c , h , w ], X [ n , c , h * strides [ 0 ] + kh * dilations [ 0 ] - pads [ 0 ], w * strides [ 1 ] + kw * dilations [ 1 ] - pads [ 1 ]]) # yapf: enable","title":"max_pool_()"},{"location":"api/#freetensor.libop.reduction","text":"","title":"reduction"},{"location":"api/#freetensor.libop.reduction.all","text":"Reduction of logical and of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/reduction.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"all()"},{"location":"api/#freetensor.libop.reduction.all_","text":"Reduction of logical and of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Source code in freetensor/libop/reduction.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"all_()"},{"location":"api/#freetensor.libop.reduction.any","text":"Reduction of logical or of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/reduction.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"any()"},{"location":"api/#freetensor.libop.reduction.any_","text":"Reduction of logical or of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Source code in freetensor/libop/reduction.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"any_()"},{"location":"api/#freetensor.libop.reduction.reduce_max","text":"Maximum of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/reduction.py @core . inline def reduce_max ( x , axes : Sequence [ int ], keepdims : bool = True ): ''' Maximum of a tensor through one or more dimensions and return the result Parameters ---------- x : VarRef The input tensor axes : Sequence[int] (Optional) Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims : bool (Optional) Keep the reduced dimensions as singleton dimensions. Defaults to True Returns ------- VarRef The result tensor ''' #! label: impl y = _general_reduce ( core . max , core . min_value ( core . dtype ( x )), x , axes , keepdims ) return y","title":"reduce_max()"},{"location":"api/#freetensor.libop.reduction.reduce_max_","text":"Maximum of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Source code in freetensor/libop/reduction.py @core . inline def reduce_max_ ( x , y , axes : Sequence [ int ], keepdims : bool = True ): ''' Maximum of a tensor through one or more dimensions. The result is written to another tensor Parameters ---------- x : VarRef The input tensor y : VarRef The result tensor axes : Sequence[int] (Optional) Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims : bool (Optional) Keep the reduced dimensions as singleton dimensions. Defaults to True ''' #! label: impl _general_reduce_ ( core . max , core . min_value ( core . dtype ( x )), x , y , axes , keepdims )","title":"reduce_max_()"},{"location":"api/#freetensor.libop.reduction.reduce_min","text":"Minimum of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/reduction.py @core . inline def reduce_min ( x , axes : Sequence [ int ], keepdims : bool = True ): ''' Minimum of a tensor through one or more dimensions and return the result Parameters ---------- x : VarRef The input tensor axes : Sequence[int] (Optional) Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims : bool (Optional) Keep the reduced dimensions as singleton dimensions. Defaults to True Returns ------- VarRef The result tensor ''' #! label: impl y = _general_reduce ( core . min , core . max_value ( core . dtype ( x )), x , axes , keepdims ) return y","title":"reduce_min()"},{"location":"api/#freetensor.libop.reduction.reduce_min_","text":"Minimum of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Source code in freetensor/libop/reduction.py @core . inline def reduce_min_ ( x , y , axes : Sequence [ int ], keepdims : bool = True ): ''' Minimum of a tensor through one or more dimensions. The result is written to another tensor Parameters ---------- x : VarRef The input tensor y : VarRef The result tensor axes : Sequence[int] (Optional) Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims : bool (Optional) Keep the reduced dimensions as singleton dimensions. Defaults to True ''' #! label: impl _general_reduce_ ( core . min , core . max_value ( core . dtype ( x )), x , y , axes , keepdims )","title":"reduce_min_()"},{"location":"api/#freetensor.libop.reduction.reduce_prod","text":"Product of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/reduction.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"reduce_prod()"},{"location":"api/#freetensor.libop.reduction.reduce_prod_","text":"Product of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Source code in freetensor/libop/reduction.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"reduce_prod_()"},{"location":"api/#freetensor.libop.reduction.reduce_sum","text":"Sum of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/reduction.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"reduce_sum()"},{"location":"api/#freetensor.libop.reduction.reduce_sum_","text":"Sum of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Source code in freetensor/libop/reduction.py def g ( * _args , ** _kvs ): return f ( * args , * _args , ** kvs , ** _kvs )","title":"reduce_sum_()"},{"location":"api/#freetensor.libop.reshape","text":"","title":"reshape"},{"location":"api/#freetensor.libop.reshape.expand","text":"Broadcast a tensor to a given shape, following the broadcasting rules Parameters: a ( VarRef ) \u2013 The input tensor b ( Sequence of expressions ) \u2013 The broadcasted shape Returns: The broadcasted tensor Source code in freetensor/libop/reshape.py @core . inline def expand ( a , expand_shape ): ''' Broadcast a tensor to a given shape, following the broadcasting rules Parameters ---------- a : VarRef The input tensor b : Sequence of expressions The broadcasted shape Returns ------- VarRef : The broadcasted tensor ''' # FIXME: out_shape = broadcast(a.shape, expand_shape) out = core . empty ( expand_shape , core . dtype ( a ), core . mtype ( a )) #! label: recur expand_ ( a , out ) return out","title":"expand()"},{"location":"api/#freetensor.libop.reshape.expand_","text":"Broadcast a tensor to an existing tensor, following the broadcasting rules Parameters: a ( VarRef ) \u2013 The input tensor b ( VarRef ) \u2013 The broadcasted tensor Source code in freetensor/libop/reshape.py @core . inline def expand_ ( a , out ): ''' Broadcast a tensor to an existing tensor, following the broadcasting rules Parameters ---------- a : VarRef The input tensor b : VarRef The broadcasted tensor ''' if out . ndim == 0 : out [()] = a else : #! label: L_elem for i in range ( out . shape ( 0 )): if core . ndim ( a ) < out . ndim : #! label: recur expand_ ( a , out [ i ]) else : #! label: recur expand_ ( a [ i % a . shape ( 0 )], out [ i ])","title":"expand_()"},{"location":"api/#freetensor.libop.reshape.flatten","text":"Flatten a tensor to have fewer dimensions, and return the result Parameters: x ( VarRef ) \u2013 The input tensor axis ( int (Optional) ) \u2013 The result tensor will have up to axis dimensions. All dimensions after axis will be flatten to 1-D. Negative axis means counting form the last dimension Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/reshape.py @core . inline def flatten ( x , axis = 1 ): ''' Flatten a tensor to have fewer dimensions, and return the result Parameters ---------- x : VarRef The input tensor axis : int (Optional) The result tensor will have up to `axis` dimensions. All dimensions after `axis` will be flatten to 1-D. Negative axis means counting form the last dimension Returns ------- VarRef The result tensor ''' y = core . empty ( _flatten_comp_shape ( x , axis ), core . dtype ( x ), core . mtype ( x )) #! label: recur flatten_ ( x , y , axis ) return y","title":"flatten()"},{"location":"api/#freetensor.libop.reshape.flatten_","text":"Flatten a tensor to have fewer dimensions, and write to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axis ( int ) \u2013 The result tensor will have up to axis dimensions. All dimensions after axis will be flatten to 1-D. Negative axis means counting form the last dimension Source code in freetensor/libop/reshape.py @core . inline def flatten_ ( x , y , axis : int = 1 ): ''' Flatten a tensor to have fewer dimensions, and write to another tensor Parameters ---------- x : VarRef The input tensor y : VarRef The result tensor axis : int (Optional) The result tensor will have up to `axis` dimensions. All dimensions after `axis` will be flatten to 1-D. Negative axis means counting form the last dimension ''' if axis == 0 : #! label: recur _flatten_inner_ ( x , y [ 0 ]) else : #! label: L_outer for i in range ( x . shape ( 0 )): #! label: recur flatten_ ( x [ i ], y [ i * ( y . shape ( 0 ) // x . shape ( 0 )):( i + 1 ) * ( y . shape ( 0 ) // x . shape ( 0 ))], axis - 1 )","title":"flatten_()"},{"location":"api/#freetensor.libop.reshape.reshape","text":"Reshape a tensor into a different shape with the same size This operator will try to generate nested loops instead of looping over all elements in a plain loop, so schedules can be better applied. It guarantees to generates loops in the following cases: Splitting a dimension. E.g. 4 to 2x2, and there will be a 2x2 loop nest. Merging dimensions. E.g. 2x2 to 4, and there will be a 2x2 loop nest. Each non-affecting dimension will be iterated by a unique loop. E.g. 3x5x7 to 5x3x7, and there will be a 15x7 loop nest, where the \"7\" dimension will be iterated by a unique loop. Parameters: x ( VarRef ) \u2013 The input tensor shape ( list of expression ) \u2013 The target shape Returns: VarRef \u2013 The result tensor Source code in freetensor/libop/reshape.py @core . inline def reshape ( x , shape ): ''' Reshape a tensor into a different shape with the same size This operator will try to generate nested loops instead of looping over all elements in a plain loop, so schedules can be better applied. It guarantees to generates loops in the following cases: 1. Splitting a dimension. E.g. 4 to 2x2, and there will be a 2x2 loop nest. 2. Merging dimensions. E.g. 2x2 to 4, and there will be a 2x2 loop nest. 3. Each non-affecting dimension will be iterated by a unique loop. E.g. 3x5x7 to 5x3x7, and there will be a 15x7 loop nest, where the \"7\" dimension will be iterated by a unique loop. Parameters ---------- x : VarRef The input tensor shape : list of expression The target shape Returns ------- VarRef The result tensor ''' y = core . empty ( shape , core . dtype ( x ), core . mtype ( x )) reshape_ ( x , y ) return y","title":"reshape()"},{"location":"api/#freetensor.libop.reshape.reshape_","text":"Fill a tensor into another tensor with the same size but maybe different shape This operator will try to generate nested loops instead of looping over all elements in a plain loop, so schedules can be better applied. It guarantees to generates loops in the following cases: Splitting a dimension. E.g. 4 to 2x2, and there will be a 2x2 loop nest. Merging dimensions. E.g. 2x2 to 4, and there will be a 2x2 loop nest. Each non-affecting dimension will be iterated by a unique loop. E.g. 3x5x7 to 5x3x7, and there will be a 15x7 loop nest, where the \"7\" dimension will be iterated by a unique loop. Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor Source code in freetensor/libop/reshape.py @core . inline def reshape_ ( x , y ): ''' Fill a tensor into another tensor with the same size but maybe different shape This operator will try to generate nested loops instead of looping over all elements in a plain loop, so schedules can be better applied. It guarantees to generates loops in the following cases: 1. Splitting a dimension. E.g. 4 to 2x2, and there will be a 2x2 loop nest. 2. Merging dimensions. E.g. 2x2 to 4, and there will be a 2x2 loop nest. 3. Each non-affecting dimension will be iterated by a unique loop. E.g. 3x5x7 to 5x3x7, and there will be a 15x7 loop nest, where the \"7\" dimension will be iterated by a unique loop. Parameters ---------- x : VarRef The input tensor y : VarRef The result tensor ''' if core . ndim ( x ) == 0 and core . ndim ( y ) == 0 : y [ ... ] = x [ ... ] elif core . ndim ( x ) > 0 and core . ndim ( y ) == 0 : assert x . shape ( 0 ) == 1 reshape_ ( x [ 0 ], y ) elif core . ndim ( y ) > 0 and core . ndim ( x ) == 0 : assert y . shape ( 0 ) == 1 reshape_ ( x , y [ 0 ]) else : factor_x0 = _factorize ( x . shape ( 0 )) factor_y0 = _factorize ( y . shape ( 0 )) x0_divisible_y0 = _factor_pairs_divisible ( factor_x0 , factor_y0 ) y0_divisible_x0 = _factor_pairs_divisible ( factor_y0 , factor_x0 ) if x0_divisible_y0 and y0_divisible_x0 : # Identical dimension assert x . shape ( 0 ) == y . shape ( 0 ) for i in range ( x . shape ( 0 )): reshape_ ( x [ i ], y [ i ]) elif x0_divisible_y0 : # Splitting a dimension. Iterating y assert x . shape ( 0 ) % y . shape ( 0 ) == 0 x_chunk_len = x . shape ( 0 ) // y . shape ( 0 ) for i in range ( y . shape ( 0 )): # Construct the slice with `length` here to make the next dimension simple reshape_ ( x [ core . ffi . FrontendVarIdx ( i * x_chunk_len , None , x_chunk_len )], y [ i ]) elif y0_divisible_x0 : # Merging dimensions. Iterating x assert y . shape ( 0 ) % x . shape ( 0 ) == 0 y_chunk_len = y . shape ( 0 ) // x . shape ( 0 ) for i in range ( x . shape ( 0 )): # Construct the slice with `length` here to make the next dimension simple reshape_ ( x [ i ], y [ core . ffi . FrontendVarIdx ( i * y_chunk_len , None , y_chunk_len )]) else : # Find next non-affecting dimension, and use one loop to reshape all # affecting dimensions before it factor_x = ( 1 , {}) factor_y = ( 1 , {}) l = 0 r = 0 while l < core . ndim ( x ): factor_x = _factor_pairs_mul ( factor_x , _factorize ( x . shape ( l ))) l += 1 while r < core . ndim ( y ): factor_y_new = _factor_pairs_mul ( factor_y , _factorize ( y . shape ( r ))) if _factor_pairs_divisible ( factor_x , factor_y_new ): factor_y = factor_y_new r += 1 else : break if _factor_pairs_divisible ( factor_y , factor_x ): break if not _factor_pairs_divisible ( factor_y , factor_x ): r = core . ndim ( y ) x_lengths = [ 1 ] * ( l + 1 ) y_lengths = [ 1 ] * ( r + 1 ) for k in core . static_range ( l - 1 , - 1 , - 1 ): x_lengths [ k ] = x . shape ( k ) * x_lengths [ k + 1 ] for k in core . static_range ( r - 1 , - 1 , - 1 ): y_lengths [ k ] = y . shape ( k ) * y_lengths [ k + 1 ] assert x_lengths [ 0 ] == y_lengths [ 0 ] for i in range ( x_lengths [ 0 ]): x_next , y_next = x , y for k in core . static_range ( l ): x_next = x_next [ i // x_lengths [ k + 1 ] % x . shape ( k )] for k in core . static_range ( r ): y_next = y_next [ i // y_lengths [ k + 1 ] % y . shape ( k )] reshape_ ( x_next , y_next )","title":"reshape_()"},{"location":"api/#freetensor.libop.reshape.unsqueeze","text":"Insert singleton dimensions to a tensor, and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] ) \u2013 Dimension numbers of the new singleton dimensions. Negative axis means counting from the last dimension Returns: VarRef \u2013 The resulting tensor Source code in freetensor/libop/reshape.py @core . inline def unsqueeze ( x , axes : Sequence [ int ]): ''' Insert singleton dimensions to a tensor, and return the result Parameters ---------- x : VarRef The input tensor axes : Dimension numbers of the new singleton dimensions. Negative axis means counting from the last dimension Returns ------- VarRef The resulting tensor ''' y = core . empty ( _unsqueeze_comp_shape ( _circular_axes ( axes , core . ndim ( x )), x ), core . dtype ( x ), core . mtype ( x )) #! label: recur unsqueeze_ ( x , y , axes ) return y","title":"unsqueeze()"},{"location":"api/#freetensor.libop.reshape.unsqueeze_","text":"Insert singleton dimensions to a tensor, and write the result to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The resulting tensor axes ( Sequence[int] ) \u2013 Dimension numbers of the new singleton dimensions. Negative axis means counting from the last dimension Source code in freetensor/libop/reshape.py @core . inline def unsqueeze_ ( x , y , axes : Sequence [ int ]): ''' Insert singleton dimensions to a tensor, and write the result to another tensor Parameters ---------- x : VarRef The input tensor y : VarRef The resulting tensor axes : Dimension numbers of the new singleton dimensions. Negative axis means counting from the last dimension ''' axes = _circular_axes ( axes , core . ndim ( x )) if y . ndim == 0 : y [()] = x elif begin_with_0 ( axes ): #! label: recur unsqueeze_ ( x , y [ 0 ], all_minus_one ( axes [ 1 :])) else : #! label: L for i in range ( x . shape ( 0 )): #! label: recur unsqueeze_ ( x [ i ], y [ i ], all_minus_one ( axes ))","title":"unsqueeze_()"},{"location":"api/#freetensor.libop.softmax","text":"","title":"softmax"},{"location":"api/#freetensor.libop.softmax.softmax","text":"Softmax of tensor x along an axis and return the result Parameters: x ( VarRef ) \u2013 The input tensor axis ( int (Optional) ) \u2013 Axis that the softmax is performed along. Negative axis means count from the last dimension Returns: The result tensor Source code in freetensor/libop/softmax.py @core . inline def softmax ( x , axis =- 1 ): ''' Softmax of tensor `x` along an axis and return the result Parameters ---------- x : VarRef The input tensor axis : int (Optional) Axis that the softmax is performed along. Negative axis means count from the last dimension Returns ------- VarRef : The result tensor ''' #! label: max maxval = reduce_max ( x , axes = [ axis ], keepdims = True ) #! label: sub corrected = sub ( x , maxval ) #! label: exp exponent = exp ( corrected ) #! label: sum summation = reduce_sum ( exponent , axes = [ axis ], keepdims = True ) #! label: div out = truediv ( exponent , summation ) return out","title":"softmax()"},{"location":"api/#freetensor.libop.softmax.softmax_","text":"Softmax of tensor x along an axis, and write to tensor y Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axis ( int ) \u2013 Axis that the softmax is performed along. Negative axis means count from the last dimension Source code in freetensor/libop/softmax.py @core . inline def softmax_ ( x , y , axis : int = - 1 ): ''' Softmax of tensor `x` along an axis, and write to tensor `y` Parameters ---------- x : VarRef The input tensor y : VarRef The result tensor axis : int (Optional) Axis that the softmax is performed along. Negative axis means count from the last dimension ''' #! label: max maxval = reduce_max ( x , axes = [ axis ], keepdims = True ) #! label: sub corrected = sub ( x , maxval ) #! label: exp exponent = exp ( corrected ) #! label: sum summation = reduce_sum ( exponent , axes = [ axis ], keepdims = True ) #! label: div truediv_ ( exponent , summation , y )","title":"softmax_()"},{"location":"about/contrib/","text":"Contributing \u00b6 Pull Requests are welcome! Please configure (or install some plugins for) your editor, to support clang-format , yapf and editorconfig , for code formating. And please note that we use different naming styles in Python and C++ parts.","title":"Contributing"},{"location":"about/contrib/#contributing","text":"Pull Requests are welcome! Please configure (or install some plugins for) your editor, to support clang-format , yapf and editorconfig , for code formating. And please note that we use different naming styles in Python and C++ parts.","title":"Contributing"},{"location":"about/pub/","text":"Publication \u00b6 Shizhi Tang, Jidong Zhai, Haojie Wang, Lin Jiang, Liyan Zheng, Zhenhao Yuan, and Chen Zhang. 2022. FreeTensor: A Free-Form DSL with Holistic Optimizations for Irregular Tensor Programs. In Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation (PLDI \u201922), June 13-17, 2022, San Diego, CA, USA . ACM, New York, NY, USA, 16 pages. https://doi.org/10.1145/3519939.3523448. ( Download ) @inproceedings{10.1145/3519939.3523448, author = {Tang, Shizhi and Zhai, Jidong and Wang, Haojie and Jiang, Lin and Zheng, Liyan and Yuan, Zhenhao and Zhang, Chen}, title = {FreeTensor: A Free-Form DSL with Holistic Optimizations for Irregular Tensor Programs}, year = {2022}, isbn = {9781450392655}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3519939.3523448}, doi = {10.1145/3519939.3523448}, booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation}, pages = {872\u2013887}, numpages = {16}, keywords = {tensor computing, optimizing compilers, DSL}, location = {San Diego, CA, USA}, series = {PLDI 2022} } Evaluation code can be found in this repository . NOTE: API of FreeTensor has been changed since submission. To reproduce the exact result in the paper, please consider the Artifact Evaluation version of FreeTensor, published here .","title":"Publication"},{"location":"about/pub/#publication","text":"Shizhi Tang, Jidong Zhai, Haojie Wang, Lin Jiang, Liyan Zheng, Zhenhao Yuan, and Chen Zhang. 2022. FreeTensor: A Free-Form DSL with Holistic Optimizations for Irregular Tensor Programs. In Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation (PLDI \u201922), June 13-17, 2022, San Diego, CA, USA . ACM, New York, NY, USA, 16 pages. https://doi.org/10.1145/3519939.3523448. ( Download ) @inproceedings{10.1145/3519939.3523448, author = {Tang, Shizhi and Zhai, Jidong and Wang, Haojie and Jiang, Lin and Zheng, Liyan and Yuan, Zhenhao and Zhang, Chen}, title = {FreeTensor: A Free-Form DSL with Holistic Optimizations for Irregular Tensor Programs}, year = {2022}, isbn = {9781450392655}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3519939.3523448}, doi = {10.1145/3519939.3523448}, booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation}, pages = {872\u2013887}, numpages = {16}, keywords = {tensor computing, optimizing compilers, DSL}, location = {San Diego, CA, USA}, series = {PLDI 2022} } Evaluation code can be found in this repository . NOTE: API of FreeTensor has been changed since submission. To reproduce the exact result in the paper, please consider the Artifact Evaluation version of FreeTensor, published here .","title":"Publication"},{"location":"guide/","text":"Get Started \u00b6 Build and Run Your First Program with FreeTenor Optimize a Program with Schedules Running on a GPU Automatic Differentiation","title":"Get Started"},{"location":"guide/#get-started","text":"Build and Run Your First Program with FreeTenor Optimize a Program with Schedules Running on a GPU Automatic Differentiation","title":"Get Started"},{"location":"guide/ad/","text":"Automatic Differentiation \u00b6 Automatic Differentiation (AD) transforms a program to another program that computes the original one's derivative or gradient. FreeTensor supports Reverse-Mode AD, and there is a plan to support Forward-Mode AD in the future. Reverse-Mode AD \u00b6 Suppose there is a program x -> y -> z -> w that computes an output w from intermediate variables z and y , and an input variable x . Reverse-Mode AD generates a gradient program dw/dw=1 -> dw/dz -> dw/dy -> dw/dx that computes dw/dx by Chain Rule. y , z and w may be saved in a \"tape\" when evaluation the original program, to be reused in the gradient one. If FreeTensor is built with WITH_PYTORCH=ON , you can skip this section and turn to the @optimize_to_pytorch integration , which integrates seamlessly with PyTorch's autograd mechanism, but will incur some runtime overhead. Here is an example of Reverse-Mode AD in FreeTensor: import freetensor as ft import numpy as np n = 4 def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.zeros((), \"float32\") for i in range(n): y[()] += a[i] * b[i] return y fwd, bwd, input_grads, output_grads = ft.grad(test, ['a', 'b'], [ft.Return()]) fwd = ft.optimize(fwd) bwd = ft.optimize(bwd) a = np.array([0, 1, 2, 3], dtype=\"float32\") b = np.array([3, 2, 1, 0], dtype=\"float32\") y = fwd(a, b) print(y.numpy()) dzdy = np.array(1, dtype='float32') dzda, dzdb = bwd(**{output_grads[ft.Return()]: dzdy})[input_grads['a'], input_grads['b']] print(dzda.numpy()) print(dzdb.numpy()) You need to call ft.grad (or the inplace version ft.grad_ ) to generate a forward function and a backward function. Please note that the forward function fwd is not the same as the original function test , because fwd may save some intermediate tensors to a global tape , and fwd must be executed before the backward one bwd . After that, you call ft.optimize to optimize and compile the program just as in previous examples, but for both fwd and bwd this time. Finally, you execute fwd and bwd . The parameters and return values of bwd are the gradients of a , b and y , which have their own names. To set and get these parameters and return values, you look up for them in two dictionaries input_grads and output_grads returned from ft.grad (in type ft.ArgRetDict . input_grads and output_grads accept either a name of a parameter, or a special ft.Return to specify a return value. When invoking bwd , parameters can be set via keyword arguments, and return values can be collect via a bracket (from a special type ft.ReturnValuesPack ). Intermediate variables are not always have to be saved to the \"tape\" from the forward function. If a variable is need in the backward function but not saved, it will be re-computed, which is sometimes even faster than saving it due to better locality. By default, FreeTensor uses heuristics to determine which variable to save. To get better performance, you may want to control which intermediate variables should be saved by setting an optional tapes parameter in ft.grad . tapes can either be a different mode, or a explicit list of AST node IDs of all VarDef nodes of the variables you want to save.","title":"Automatic Differentiation"},{"location":"guide/ad/#automatic-differentiation","text":"Automatic Differentiation (AD) transforms a program to another program that computes the original one's derivative or gradient. FreeTensor supports Reverse-Mode AD, and there is a plan to support Forward-Mode AD in the future.","title":"Automatic Differentiation"},{"location":"guide/ad/#reverse-mode-ad","text":"Suppose there is a program x -> y -> z -> w that computes an output w from intermediate variables z and y , and an input variable x . Reverse-Mode AD generates a gradient program dw/dw=1 -> dw/dz -> dw/dy -> dw/dx that computes dw/dx by Chain Rule. y , z and w may be saved in a \"tape\" when evaluation the original program, to be reused in the gradient one. If FreeTensor is built with WITH_PYTORCH=ON , you can skip this section and turn to the @optimize_to_pytorch integration , which integrates seamlessly with PyTorch's autograd mechanism, but will incur some runtime overhead. Here is an example of Reverse-Mode AD in FreeTensor: import freetensor as ft import numpy as np n = 4 def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.zeros((), \"float32\") for i in range(n): y[()] += a[i] * b[i] return y fwd, bwd, input_grads, output_grads = ft.grad(test, ['a', 'b'], [ft.Return()]) fwd = ft.optimize(fwd) bwd = ft.optimize(bwd) a = np.array([0, 1, 2, 3], dtype=\"float32\") b = np.array([3, 2, 1, 0], dtype=\"float32\") y = fwd(a, b) print(y.numpy()) dzdy = np.array(1, dtype='float32') dzda, dzdb = bwd(**{output_grads[ft.Return()]: dzdy})[input_grads['a'], input_grads['b']] print(dzda.numpy()) print(dzdb.numpy()) You need to call ft.grad (or the inplace version ft.grad_ ) to generate a forward function and a backward function. Please note that the forward function fwd is not the same as the original function test , because fwd may save some intermediate tensors to a global tape , and fwd must be executed before the backward one bwd . After that, you call ft.optimize to optimize and compile the program just as in previous examples, but for both fwd and bwd this time. Finally, you execute fwd and bwd . The parameters and return values of bwd are the gradients of a , b and y , which have their own names. To set and get these parameters and return values, you look up for them in two dictionaries input_grads and output_grads returned from ft.grad (in type ft.ArgRetDict . input_grads and output_grads accept either a name of a parameter, or a special ft.Return to specify a return value. When invoking bwd , parameters can be set via keyword arguments, and return values can be collect via a bracket (from a special type ft.ReturnValuesPack ). Intermediate variables are not always have to be saved to the \"tape\" from the forward function. If a variable is need in the backward function but not saved, it will be re-computed, which is sometimes even faster than saving it due to better locality. By default, FreeTensor uses heuristics to determine which variable to save. To get better performance, you may want to control which intermediate variables should be saved by setting an optional tapes parameter in ft.grad . tapes can either be a different mode, or a explicit list of AST node IDs of all VarDef nodes of the variables you want to save.","title":"Reverse-Mode AD"},{"location":"guide/build-and-run/","text":"Build and Run \u00b6 Dependencies \u00b6 Linux Python (>= 3.8, for the Python frontend) GCC (>= 10, to support C++20 and the \"unroll\" pragma) CUDA (>= 11.4.1, to support GCC 10, Optional) MKL (Optional) PyTorch (Optional, see below) Java (= 11, Build-time dependency only) Other Python dependencies: pip3 install --user numpy sourceinspect astor Pygments xgboost Note on Python version Because we are analyzing Python AST, which is sensitive to Python version, there may be potential bugs for Python strictly later than 3.8. Please file an issue if something goes wrong PyTorch support FreeTensor can optionally link PyTorch to support a copy-free interface between FreeTensor and PyTorch. Please note that, if you are using CUDA, FreeTensor and PyTorch should link CUDA of the same version . PyTorch can be installed in any way you like, see PyTorch's guide . Build \u00b6 First, clone this repo. Don't forget there are some submodules. git clone --recursive <path/to/this/repo> Then, build. mkdir build cd build cmake .. make -j # Or use Ninja There are some options to cmake : -DFT_WITH_CUDA=ON/OFF : build with/without CUDA (defaults to ON ). -DFT_WITH_MKL=<path/to/mkl/root> : build with MKL (path to MKL is required, defaults to building without it). The path accepts by CMake should be a raw unescaped path; i.e. -DFT_WITH_MKL=\"/some path\" is good since the quotes are resolved by the shell but -DFT_WITH_MKL=\\\"/some\\ path\\\" is not. -DFT_WITH_PYTORCH=ON/OFF : build with/without copy-free interface from/to PyTorch, requring PyTorch installed on the system (defaults to OFF ). -DFT_DEBUG_LOG_NODE=ON (for developers): enables tracing to tell by which pass a specific AST node is modified. -DFT_DEBUG_PROFILE=ON (for developers): profiles some heavy functions in the compiler. -DFT_DEBUG_SANITIZE=<sanitizer_name> (for developers): build with GCC sanitizer (set it to a sanitizer name to use, e.g. address). It will build a shared library with a name like freetensor_ffi.cpython-37m-x86_64-linux-gnu.so , which can be used in Python via import freetensor . Run a Program with FreeTensor \u00b6 To run any program with FreeTensor, one should add the python/ and build/ directory to PYTHONPATH first. E.g. to run a python program a.py with FreeTensor in the build/ directory, PYTHONPATH=../python:../build:$PYTHONPATH python3 a.py Global Configurations \u00b6 There are serveral global configurations can be set via environment variables: FT_PRETTY_PRINT=ON/OFF . Enable/disable colored printing. FT_PRINT_ALL_ID=ON/OFF . Print (or not) IDs of all statements in an AST. FT_WERROR=ON/OFF . Treat warnings as errors (or not). FT_BACKEND_COMPILER_CXX=<path/to/compiler> . The C++ compiler used to compiler the optimized program. Default to the same compiler found when building FreeTensor itself, and compilers found in the PATH enviroment variable. This environment variable should be set to a colon-separated list of paths, in which the paths are searched from left to right. FT_BACKEND_COMPILER_NVCC=<path/to/compiler> . The CUDA compiler used to compiler the optimized program (if built with CUDA). Default to the same compiler found when building FreeTensor itself, and compilers found in the PATH enviroment variable. This environment variable should be set to a colon-separated list of paths, in which the paths are searched from left to right. FT_DEBUG_RUNTIME_CHECK . Check out-of-bound access and integer overflow at the generated code at runtime. This option is only for debugging, and will introduce significant runtime overhead. Currently the checker cannot print the error site, please also enable FT_DEBUG_BINARY and then use GDB to locate the error site. FT_DEBUG_BINARY=ON (for developers). Compile with -g at backend. Do not delete the binary file after loaded. This configurations can also set at runtime in ft.config . Run the Tests \u00b6 To run the test, first change into the test/ directory, then PYTHONPATH=../python:../build:$PYTHONPATH pytest To run a single test case, specify the test case name, and optionally use pytest -s to display the standard output. E.g, PYTHONPATH=../python:../build:$PYTHONPATH pytest -s 00.hello_world/test_basic.py::test_hello_world Debugging (for developers) If using GDB, one should invoke PyTest with python3 -m : PYTHONPATH=../python:../build:$PYTHONPATH gdb --args python3 -m pytest If using Valgrind, one should set Python to use the system malloc: PYTHONPATH=../python:../build:$PYTHONPATH PYTHONMALLOC=malloc valgrind python3 -m pytest Sometimes Valgrind is not enough to detect some errors. An alternative is to use the sanitizer from GCC. For example, if you are using the \"address\" sanitizer, first set -DFT_DEBUG_SANITIZE=address to cmake , and then: PYTHONPATH=../python:../build:$PYTHONPATH LD_PRELOAD=`gcc -print-file-name=libasan.so` pytest -s If you are using another sanitizer, change the string set to FT_DEBUG_SANITIZE and the library's name. For example, -DFT_DEBUG_SANITIZE=undefined and libubsan.so . Build this Document \u00b6 First install some dependencies: pip3 install --user mkdocs mkdocstrings==0.18.1 \"pytkdocs[numpy-style]\" From the root directory of FreeTensor, run a HTTP server to serve the document (recommended, but without document on C++ interface due to a limitation ): PYTHONPATH=./python:./build:$PYTHONPATH mkdocs serve Or build and save the pages (with document on C++ interface, requiring Doxygen and Graphviz): doxygen Doxyfile && PYTHONPATH=./python:./build:$PYTHONPATH mkdocs build Publish the documents to GitHub Pages (for developers) doxygen Doxyfile && PYTHONPATH=./python:./build:$PYTHONPATH mkdocs gh-deploy","title":"Build and Run"},{"location":"guide/build-and-run/#build-and-run","text":"","title":"Build and Run"},{"location":"guide/build-and-run/#dependencies","text":"Linux Python (>= 3.8, for the Python frontend) GCC (>= 10, to support C++20 and the \"unroll\" pragma) CUDA (>= 11.4.1, to support GCC 10, Optional) MKL (Optional) PyTorch (Optional, see below) Java (= 11, Build-time dependency only) Other Python dependencies: pip3 install --user numpy sourceinspect astor Pygments xgboost Note on Python version Because we are analyzing Python AST, which is sensitive to Python version, there may be potential bugs for Python strictly later than 3.8. Please file an issue if something goes wrong PyTorch support FreeTensor can optionally link PyTorch to support a copy-free interface between FreeTensor and PyTorch. Please note that, if you are using CUDA, FreeTensor and PyTorch should link CUDA of the same version . PyTorch can be installed in any way you like, see PyTorch's guide .","title":"Dependencies"},{"location":"guide/build-and-run/#build","text":"First, clone this repo. Don't forget there are some submodules. git clone --recursive <path/to/this/repo> Then, build. mkdir build cd build cmake .. make -j # Or use Ninja There are some options to cmake : -DFT_WITH_CUDA=ON/OFF : build with/without CUDA (defaults to ON ). -DFT_WITH_MKL=<path/to/mkl/root> : build with MKL (path to MKL is required, defaults to building without it). The path accepts by CMake should be a raw unescaped path; i.e. -DFT_WITH_MKL=\"/some path\" is good since the quotes are resolved by the shell but -DFT_WITH_MKL=\\\"/some\\ path\\\" is not. -DFT_WITH_PYTORCH=ON/OFF : build with/without copy-free interface from/to PyTorch, requring PyTorch installed on the system (defaults to OFF ). -DFT_DEBUG_LOG_NODE=ON (for developers): enables tracing to tell by which pass a specific AST node is modified. -DFT_DEBUG_PROFILE=ON (for developers): profiles some heavy functions in the compiler. -DFT_DEBUG_SANITIZE=<sanitizer_name> (for developers): build with GCC sanitizer (set it to a sanitizer name to use, e.g. address). It will build a shared library with a name like freetensor_ffi.cpython-37m-x86_64-linux-gnu.so , which can be used in Python via import freetensor .","title":"Build"},{"location":"guide/build-and-run/#run-a-program-with-freetensor","text":"To run any program with FreeTensor, one should add the python/ and build/ directory to PYTHONPATH first. E.g. to run a python program a.py with FreeTensor in the build/ directory, PYTHONPATH=../python:../build:$PYTHONPATH python3 a.py","title":"Run a Program with FreeTensor"},{"location":"guide/build-and-run/#global-configurations","text":"There are serveral global configurations can be set via environment variables: FT_PRETTY_PRINT=ON/OFF . Enable/disable colored printing. FT_PRINT_ALL_ID=ON/OFF . Print (or not) IDs of all statements in an AST. FT_WERROR=ON/OFF . Treat warnings as errors (or not). FT_BACKEND_COMPILER_CXX=<path/to/compiler> . The C++ compiler used to compiler the optimized program. Default to the same compiler found when building FreeTensor itself, and compilers found in the PATH enviroment variable. This environment variable should be set to a colon-separated list of paths, in which the paths are searched from left to right. FT_BACKEND_COMPILER_NVCC=<path/to/compiler> . The CUDA compiler used to compiler the optimized program (if built with CUDA). Default to the same compiler found when building FreeTensor itself, and compilers found in the PATH enviroment variable. This environment variable should be set to a colon-separated list of paths, in which the paths are searched from left to right. FT_DEBUG_RUNTIME_CHECK . Check out-of-bound access and integer overflow at the generated code at runtime. This option is only for debugging, and will introduce significant runtime overhead. Currently the checker cannot print the error site, please also enable FT_DEBUG_BINARY and then use GDB to locate the error site. FT_DEBUG_BINARY=ON (for developers). Compile with -g at backend. Do not delete the binary file after loaded. This configurations can also set at runtime in ft.config .","title":"Global Configurations"},{"location":"guide/build-and-run/#run-the-tests","text":"To run the test, first change into the test/ directory, then PYTHONPATH=../python:../build:$PYTHONPATH pytest To run a single test case, specify the test case name, and optionally use pytest -s to display the standard output. E.g, PYTHONPATH=../python:../build:$PYTHONPATH pytest -s 00.hello_world/test_basic.py::test_hello_world Debugging (for developers) If using GDB, one should invoke PyTest with python3 -m : PYTHONPATH=../python:../build:$PYTHONPATH gdb --args python3 -m pytest If using Valgrind, one should set Python to use the system malloc: PYTHONPATH=../python:../build:$PYTHONPATH PYTHONMALLOC=malloc valgrind python3 -m pytest Sometimes Valgrind is not enough to detect some errors. An alternative is to use the sanitizer from GCC. For example, if you are using the \"address\" sanitizer, first set -DFT_DEBUG_SANITIZE=address to cmake , and then: PYTHONPATH=../python:../build:$PYTHONPATH LD_PRELOAD=`gcc -print-file-name=libasan.so` pytest -s If you are using another sanitizer, change the string set to FT_DEBUG_SANITIZE and the library's name. For example, -DFT_DEBUG_SANITIZE=undefined and libubsan.so .","title":"Run the Tests"},{"location":"guide/build-and-run/#build-this-document","text":"First install some dependencies: pip3 install --user mkdocs mkdocstrings==0.18.1 \"pytkdocs[numpy-style]\" From the root directory of FreeTensor, run a HTTP server to serve the document (recommended, but without document on C++ interface due to a limitation ): PYTHONPATH=./python:./build:$PYTHONPATH mkdocs serve Or build and save the pages (with document on C++ interface, requiring Doxygen and Graphviz): doxygen Doxyfile && PYTHONPATH=./python:./build:$PYTHONPATH mkdocs build Publish the documents to GitHub Pages (for developers) doxygen Doxyfile && PYTHONPATH=./python:./build:$PYTHONPATH mkdocs gh-deploy","title":"Build this Document"},{"location":"guide/first-program/","text":"Your First Program with FreeTenor \u00b6 In this page, we introduce some basic concepts of FreeTensor. Example: Vector addition \u00b6 import freetensor as ft import numpy as np n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Here is a basic example program in FreeTensor. You write a Python function that manipulates FreeTensor's tensor type ft.Var , decorate the function with ft.optimize , and finally invoke the decorated function. FreeTensor will generate C++ code for this vector addition, compile it using a native compiler, and finally load it back to Python. Set verbose = 1 to optimize if you are interested in the generated native code. To write such a function, you need to follow some basic concept described in this page. Declare and Define Tensors \u00b6 All tensors, including function parameters, intermediate tensors and return values should be properly declared or defined. Scalars are 0-D tensors in FreeTensor. Declare or define a tensor with an empty shape, and you will get a scalar. Function parameters should be declared like x : ft.Var[shape, data_type] . Declaring a parameter either in the function signature or as a stand-alone statment is acceptable. If your parameter uses another parameter as shape, you will need the latter manner. An optional parameter atype can be set to \"output\" or \"inout\" if you want to mutate a function argument. Intermediate and returning tensors can be created by ft.empty , ft.var or ft.zeros . If you are using FreeTensor for GPU computing, an optional parameter mtype can be set to specify where to store the tensor. It defaults to the main memory of your currently chosen computing device. All tensors and their slices are implemented by an internal ft.VarRef type. If you are looking for a tensor's API, ft.VarRef is the right place. Manipulating Tensors \u00b6 To read or write tensors in a function, just write for ... in range(...) loops that iterate through elements in the tensors, and do arithmetic operations on them. We also provide some functions that operates on a whole tensor or a tensor slice in libop . Special note on tensor assignments We follow Python convention for tensor assignments, but sometimes it is a little counterintuitive. Suppose you have two list s in Python: a and b . a = b replaces the object a with the object b , while a[...] = b assigns data in b to a . FreeTensor does not support replacing a tensor object with another one. It supports assignments only. Therefore, we need to write a[...] = b to assign tensor. a[:] = b (for non-scalars), a[None] = b and a[()] = b is also supported. Dynamic or Static \u00b6 Another concept is that statements and expressions in your program are divided into two categories: dynamic and static . Dynamic statements or expressions are restricted to a small subset of Python, and are compiled to native code. Static statements or expressions can be any Python statements or expressions, and are executed before compilation. In other words, static statements or expressions are like macros or templates in C++, while dynamic ones are actually quotations in Multi-Stage Programming . The following statements and expressions are considered dynamic: Declarations, definitions and operations of FreeTensor's tensor type ft.Var (or its internal implementation ft.VarRef ). if statements, for ... in range(...) and assert statements that have a ft.Var condition or range. All other statements and expressions are considered static. With the help of dynamic and static categories, you can utilize complex Python functions as the static part, while still generate high-performance native code using dynamic loops. For example, the following code combines static and dynamic code to sum multiple vectors together: import freetensor as ft import numpy as np n = 4 @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"], c: ft.Var[(n,), \"int32\"]): inputs = [a, b, c] # Static y = ft.empty((n,), \"int32\") # Dynamic for i in range(n): # Dyanmic y[i] = 0 # Dynamic for item in inputs: # Static y[i] += item[i] # Dynamic return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\"), np.array([3, 4, 5, 6], dtype=\"int32\")).numpy() print(y) However, there might be some counterintuitive behaviours when using static statments or expressions. Please remember that static static statements or expressions are executed before compilation, so the following piece of code will result in a list containing only one item: the expression i , instead of 10 numbers: lst = [] for i in range(10): # Dynamic lst.append(i) # Static. Appends only once Dynamic Tensor Shapes \u00b6 In the example of vector addition above, we support any vector length, but only in a static way. This means each time you change the vector length n , you need to recompile (run optimize again) the function. FreeTensor supports defining tensors with dynamic shapes, just by setting their shapes to a dynamic values. The following code shows an example: import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) assert np.array_equal(y, [3, 5, 7, 9]) In this way, in only have to compile your program once. But you will expect a longer compiling time, and some optimizations are not possible with dynamic shapes. Copy-free interface from/to PyTorch \u00b6 If FreeTensor is built with WITH_PYTORCH=ON , you can directly pass PyTorch tensors to or get them from FreeTensor. For example, import freetensor as ft import torch n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(torch.tensor([1, 2, 3, 4], dtype=torch.int32), torch.tensor([2, 3, 4, 5], dtype=torch.int32)).torch() print(y) FreeTensor also supports integration with PyTorch's \"function\" interface. You can use @ft.optimize_to_pytorch to directly generate a PyTorch \"function\" (specifically, a function wrapper around PyTorch's Function.invoke , just like usual PyTorch functions). This approach seamlessly integrates with PyTorch's autograd mechanism, but incurs some more runtime overhead. Please also note that, because we do not know whether we need to do autograd and which input tensors need gradients until we first run a function, compiling of the FreeTensor code will be delayed to run time. The compiled binary code will be cached and reused if following runs requires the same set of inputs to be derived. The following code shows an example of this approach: import freetensor as ft import torch n = 4 # Change this line to ft.optimize_to_pytorch(verbose=1) to see the resulting # native code @ft.optimize_to_pytorch def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.empty((n,), \"float32\") for i in range(n): y[i] = a[i] * b[i] return y # Forward a = torch.tensor([1, 2, 3, 4], requires_grad=True, dtype=torch.float32) b = torch.tensor([2, 3, 4, 5], requires_grad=True, dtype=torch.float32) y = test(a, b) print(\"y = \", y) # Backward y.grad = torch.tensor([1, 1, 1, 1], dtype=torch.float32) y.backward(y.grad) print(\"a.grad = \", a.grad) print(\"b.grad = \", b.grad)","title":"Your First Program with FreeTenor"},{"location":"guide/first-program/#your-first-program-with-freetenor","text":"In this page, we introduce some basic concepts of FreeTensor.","title":"Your First Program with FreeTenor"},{"location":"guide/first-program/#example-vector-addition","text":"import freetensor as ft import numpy as np n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Here is a basic example program in FreeTensor. You write a Python function that manipulates FreeTensor's tensor type ft.Var , decorate the function with ft.optimize , and finally invoke the decorated function. FreeTensor will generate C++ code for this vector addition, compile it using a native compiler, and finally load it back to Python. Set verbose = 1 to optimize if you are interested in the generated native code. To write such a function, you need to follow some basic concept described in this page.","title":"Example: Vector addition"},{"location":"guide/first-program/#declare-and-define-tensors","text":"All tensors, including function parameters, intermediate tensors and return values should be properly declared or defined. Scalars are 0-D tensors in FreeTensor. Declare or define a tensor with an empty shape, and you will get a scalar. Function parameters should be declared like x : ft.Var[shape, data_type] . Declaring a parameter either in the function signature or as a stand-alone statment is acceptable. If your parameter uses another parameter as shape, you will need the latter manner. An optional parameter atype can be set to \"output\" or \"inout\" if you want to mutate a function argument. Intermediate and returning tensors can be created by ft.empty , ft.var or ft.zeros . If you are using FreeTensor for GPU computing, an optional parameter mtype can be set to specify where to store the tensor. It defaults to the main memory of your currently chosen computing device. All tensors and their slices are implemented by an internal ft.VarRef type. If you are looking for a tensor's API, ft.VarRef is the right place.","title":"Declare and Define Tensors"},{"location":"guide/first-program/#manipulating-tensors","text":"To read or write tensors in a function, just write for ... in range(...) loops that iterate through elements in the tensors, and do arithmetic operations on them. We also provide some functions that operates on a whole tensor or a tensor slice in libop . Special note on tensor assignments We follow Python convention for tensor assignments, but sometimes it is a little counterintuitive. Suppose you have two list s in Python: a and b . a = b replaces the object a with the object b , while a[...] = b assigns data in b to a . FreeTensor does not support replacing a tensor object with another one. It supports assignments only. Therefore, we need to write a[...] = b to assign tensor. a[:] = b (for non-scalars), a[None] = b and a[()] = b is also supported.","title":"Manipulating Tensors"},{"location":"guide/first-program/#dynamic-or-static","text":"Another concept is that statements and expressions in your program are divided into two categories: dynamic and static . Dynamic statements or expressions are restricted to a small subset of Python, and are compiled to native code. Static statements or expressions can be any Python statements or expressions, and are executed before compilation. In other words, static statements or expressions are like macros or templates in C++, while dynamic ones are actually quotations in Multi-Stage Programming . The following statements and expressions are considered dynamic: Declarations, definitions and operations of FreeTensor's tensor type ft.Var (or its internal implementation ft.VarRef ). if statements, for ... in range(...) and assert statements that have a ft.Var condition or range. All other statements and expressions are considered static. With the help of dynamic and static categories, you can utilize complex Python functions as the static part, while still generate high-performance native code using dynamic loops. For example, the following code combines static and dynamic code to sum multiple vectors together: import freetensor as ft import numpy as np n = 4 @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"], c: ft.Var[(n,), \"int32\"]): inputs = [a, b, c] # Static y = ft.empty((n,), \"int32\") # Dynamic for i in range(n): # Dyanmic y[i] = 0 # Dynamic for item in inputs: # Static y[i] += item[i] # Dynamic return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\"), np.array([3, 4, 5, 6], dtype=\"int32\")).numpy() print(y) However, there might be some counterintuitive behaviours when using static statments or expressions. Please remember that static static statements or expressions are executed before compilation, so the following piece of code will result in a list containing only one item: the expression i , instead of 10 numbers: lst = [] for i in range(10): # Dynamic lst.append(i) # Static. Appends only once","title":"Dynamic or Static"},{"location":"guide/first-program/#dynamic-tensor-shapes","text":"In the example of vector addition above, we support any vector length, but only in a static way. This means each time you change the vector length n , you need to recompile (run optimize again) the function. FreeTensor supports defining tensors with dynamic shapes, just by setting their shapes to a dynamic values. The following code shows an example: import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) assert np.array_equal(y, [3, 5, 7, 9]) In this way, in only have to compile your program once. But you will expect a longer compiling time, and some optimizations are not possible with dynamic shapes.","title":"Dynamic Tensor Shapes"},{"location":"guide/first-program/#copy-free-interface-fromto-pytorch","text":"If FreeTensor is built with WITH_PYTORCH=ON , you can directly pass PyTorch tensors to or get them from FreeTensor. For example, import freetensor as ft import torch n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(torch.tensor([1, 2, 3, 4], dtype=torch.int32), torch.tensor([2, 3, 4, 5], dtype=torch.int32)).torch() print(y) FreeTensor also supports integration with PyTorch's \"function\" interface. You can use @ft.optimize_to_pytorch to directly generate a PyTorch \"function\" (specifically, a function wrapper around PyTorch's Function.invoke , just like usual PyTorch functions). This approach seamlessly integrates with PyTorch's autograd mechanism, but incurs some more runtime overhead. Please also note that, because we do not know whether we need to do autograd and which input tensors need gradients until we first run a function, compiling of the FreeTensor code will be delayed to run time. The compiled binary code will be cached and reused if following runs requires the same set of inputs to be derived. The following code shows an example of this approach: import freetensor as ft import torch n = 4 # Change this line to ft.optimize_to_pytorch(verbose=1) to see the resulting # native code @ft.optimize_to_pytorch def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.empty((n,), \"float32\") for i in range(n): y[i] = a[i] * b[i] return y # Forward a = torch.tensor([1, 2, 3, 4], requires_grad=True, dtype=torch.float32) b = torch.tensor([2, 3, 4, 5], requires_grad=True, dtype=torch.float32) y = test(a, b) print(\"y = \", y) # Backward y.grad = torch.tensor([1, 1, 1, 1], dtype=torch.float32) y.backward(y.grad) print(\"a.grad = \", a.grad) print(\"b.grad = \", b.grad)","title":"Copy-free interface from/to PyTorch"},{"location":"guide/gpu/","text":"Running on a GPU \u00b6 Example: Vector addition on a GPU \u00b6 If FreeTensor is built with a CUDA backend, you can compile your program to a GPU. We still take a vector addition as an example: import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): n = 4 # Add verbose=1 to see the resulting native code @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize('Li', 'threadIdx.x')) def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li # Label the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Similar to parallelizing to OpenMP threads , in this example, we parallelize Loop Li to the threadIdx.x dimension of CUDA. There are two major differences: You are now calling parallelize schedule with a threadIdx.x parameter, instead of openmp . All the code are enclosed by a with ft.GPU(0) scope. Usually, you not only parallelize your loops to threadIdx.x , but also other CUDA dimensions like blockIdx.x . To achieve this, you either parallelize different loops in a loop nests to different CUDA dimensions, or split your loops before parallelizing them. As for the with ft.GPU(0) scope, ft.GPU(0) specifies a Device (a specific hardware device of GPU). By calling with on a device, default values of several classes and functions are set, but currently you only need to be aware of two things: It sets the Device of optimize . It sets the default mtype of all tensors in the program, which is an optional parameter of ft.Var , ft.empty , etc. mtype refers to memory type. It controls where a tensor is stored. It defaults to \"cpu\" for a CPU program, and \"gpu/global\" for a GPU program. You probably GPU requires putting each variable to a right place (global memory, shared memory, registers, etc.), and this can be done by setting mtype s of each tensor. There are several ways to set mtype s: (Recommended) Leave them to the default \"gpu/global\" first, and modify them with the set_mem_type schedule. In this way, you write some architecture-dependent schedules, but keep your function architecture-independent. (Experimental) Leave them to the default \"gpu/global\" first, and modify them automatically using auto_schedule , or the auto_set_mem_type schedule (which is a part of auto_schedule ). Set them explicitly in the program by setting an optional mtype parameter of ft.Var , ft.empty , etc. mtype=\"byvalue\" for Dynamic Tensor Shapes \u00b6 Tensors with normal mtypes ( \"cpu\" , \"gpu/global\" , etc.) are passed by references, which means a \"cpu\" tensor can only be accessed from a CPU, and a \"gpu/global\" tensor can only be accessed from a GPU. However, sometimes, and especially for dynamic tensor shapes, we want the shapes to be passed by values, and accessible from both CPUs and GPUs (remember we need tensor's shape both when launching a kernel from the CPU side, and during actual computatoin on the GPU side). In this case, we can set the shape-related tensors a \"byvalue\" mtype , and here is an example: import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize(\"Li\", \"threadIdx.x\")) # Use \"byvalue\" for `n` so it can be used both during kernel launching # and inside a kernel def test(n: ft.Var[(), \"int32\", \"input\", \"byvalue\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li # Label the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y)","title":"Running on a GPU"},{"location":"guide/gpu/#running-on-a-gpu","text":"","title":"Running on a GPU"},{"location":"guide/gpu/#example-vector-addition-on-a-gpu","text":"If FreeTensor is built with a CUDA backend, you can compile your program to a GPU. We still take a vector addition as an example: import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): n = 4 # Add verbose=1 to see the resulting native code @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize('Li', 'threadIdx.x')) def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li # Label the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Similar to parallelizing to OpenMP threads , in this example, we parallelize Loop Li to the threadIdx.x dimension of CUDA. There are two major differences: You are now calling parallelize schedule with a threadIdx.x parameter, instead of openmp . All the code are enclosed by a with ft.GPU(0) scope. Usually, you not only parallelize your loops to threadIdx.x , but also other CUDA dimensions like blockIdx.x . To achieve this, you either parallelize different loops in a loop nests to different CUDA dimensions, or split your loops before parallelizing them. As for the with ft.GPU(0) scope, ft.GPU(0) specifies a Device (a specific hardware device of GPU). By calling with on a device, default values of several classes and functions are set, but currently you only need to be aware of two things: It sets the Device of optimize . It sets the default mtype of all tensors in the program, which is an optional parameter of ft.Var , ft.empty , etc. mtype refers to memory type. It controls where a tensor is stored. It defaults to \"cpu\" for a CPU program, and \"gpu/global\" for a GPU program. You probably GPU requires putting each variable to a right place (global memory, shared memory, registers, etc.), and this can be done by setting mtype s of each tensor. There are several ways to set mtype s: (Recommended) Leave them to the default \"gpu/global\" first, and modify them with the set_mem_type schedule. In this way, you write some architecture-dependent schedules, but keep your function architecture-independent. (Experimental) Leave them to the default \"gpu/global\" first, and modify them automatically using auto_schedule , or the auto_set_mem_type schedule (which is a part of auto_schedule ). Set them explicitly in the program by setting an optional mtype parameter of ft.Var , ft.empty , etc.","title":"Example: Vector addition on a GPU"},{"location":"guide/gpu/#mtypebyvalue-for-dynamic-tensor-shapes","text":"Tensors with normal mtypes ( \"cpu\" , \"gpu/global\" , etc.) are passed by references, which means a \"cpu\" tensor can only be accessed from a CPU, and a \"gpu/global\" tensor can only be accessed from a GPU. However, sometimes, and especially for dynamic tensor shapes, we want the shapes to be passed by values, and accessible from both CPUs and GPUs (remember we need tensor's shape both when launching a kernel from the CPU side, and during actual computatoin on the GPU side). In this case, we can set the shape-related tensors a \"byvalue\" mtype , and here is an example: import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize(\"Li\", \"threadIdx.x\")) # Use \"byvalue\" for `n` so it can be used both during kernel launching # and inside a kernel def test(n: ft.Var[(), \"int32\", \"input\", \"byvalue\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li # Label the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y)","title":"mtype=\"byvalue\" for Dynamic Tensor Shapes"},{"location":"guide/schedules/","text":"Optimize a Program with Schedules \u00b6 Oftentimes, only compiling your programs to native code is not enough, and you need further optimizations. This can be done by applying \"schedules\" (explicit program transformations) to you program. Example: Parallel Vector addition \u00b6 import freetensor as ft import numpy as np n = 4 # Add verbose=1 to see the resulting native code @ft.optimize(schedule_callback=lambda s: s.parallelize('Li', 'openmp') ) # <-- 2. Apply the schedule def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li # <-- 1. Label the loop as Li for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Here is an example of a parallel vector addition executed with OpenMP multithreading. Each element is computed by one thread. To achieve this, there are two steps: Label the loop to be parallelized with a #! label: comment. Here label refers to label of an AST node, which is not required to be unique. Apply a parallelize schedule to Li in the schedule_callback argument to optimize ; since the Li label is unambiguous here, the only Li loop is selectd and parallelized. And you are done. You can have a look at the generated OpenMP multithreaded code by setting verbose=1 . Parameter s in schedule_callback is a Schedule object. Besides parallelize , there are more supported scheduling primitives. If you are using the @optimize_to_pytorch integration , you need to set schedules for the forward pass and the backward pass separately. Combining Multiple Schdules \u00b6 Some optimizations can be done by applying multiple schedules. For example, a tiled matrix-multiplication can be done by first split the loops, then reorder them, and finally apply cache s to create tile tensors. In order to demonstrate the idea, we show a simplier example here: still a vector addtion, but with the loop split and only the outer one parallelize d. Please note that this is an example only for demonstration. Usually you do not need it because OpenMP has its own \"schedule(static)\" for parallelized loops. import freetensor as ft import numpy as np n = 1024 def sch(s): outer, inner = s.split('Li', 32) s.parallelize(outer, 'openmp') # Set verbose=1 to see the resulting native code # Set verbose=2 to see the code after EVERY schedule @ft.optimize(schedule_callback=sch) def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(np.arange(1024), dtype=\"int32\"), np.array(np.arange(1024), dtype=\"int32\")).numpy() print(y) One important thing is to track labels of the loops, because the labels will change after schedules. You get labels (to be precise, IDs, which is can be looked-up by labels) of new loops generated from one schedule from its return values ( outer and inner in this case), and pass them to a next schedule. Specify What to Schedule by Selectors \u00b6 In the example above, we label a loop Li and apply schedules on it. It is straight-forward in a tiny example, but as programs grow, it often gets hard to track each statement by a unique label, especially there are inlined function calls. To make things easy, FreeTensor supports specifying a statement by a selector, written in the following rules: A label is a selector. E.g., Li matches a statement with a label Li . (For debugging only) A numerical ID is also a selector. E.g., #31 . A node type surrounded in angle brackets ( <> ) is also a selector. E.g., <For> matches for-loop statements. A selector can be extended to match a new statement produced by a previous schedule. E.g., $split.0{Li} matches the outer loop split from the loop Li . This is useful when return values from schedules are hard to track. Please refer the API document for detailed grammar. Selectors can be combined to match a statement by nesting order. A<-B matches a statement A DIRECTLY NESTED IN another statement B . A<<-B matches a statement DIRECTLY or INDIRECTLY nested in another statement B . A<-(B<-)*C matches a statement A DIRECTLY or INDIRECTLY nested in another statement C with intermedaite nesting statements satisfying the condition in B . B->A matches a statement B directly OUT OF another statement A . B->>A and C->(B->)*A are alike. ( A , B , C can be nested selectors.) Use <-| for the root node, and ->| for a leaf node. Selectors can be combined to match a statement by DFS order. A<:B matches a statement A DIRECTLY BEFORE another statement B . A<<:B matches a statement A DIRECTLY or INDIRECTLY before another statement B . B:>A matches a statment B directly AFTER another statement A . B:>>A matches a statement B directly or indirectly after another statement A . Selectors can be combined to match a statement in a function call. A<~B matches a statement A DIRECTLY called by a call site B . A<<~B matches a statement DIRECTLY or INDIRECTLY called by a call site B . A<~(B<~)*C matches a statement A DIRECTLY or INDIRECTLY called by a call site C with intermediate call sites satisfying the condition in B . ( A , B , C can be nested selectors.) Use <~| for the root function. All the arrow-like selectors ( <- , <~ , <: , etc.) are right-associated. For example, A<-B<-C matches A nested in B , where B is nested in C . All the arrow-like selectors can be used with the first argument omitted. For example, <-B matches ALL statements nested in B . Selectors can be combined with logical \"and\" ( & ), \"or\" ( | ), \"not\" ( ! ) and parentheses. E.g., Li|Lj matches a statement labeled Li OR Lj . Li&Lj matches a statement labeled Li&Lj . All schedules support passing selectors. Auto Scheduling (Experimental) \u00b6 Manually scheduling a program requires a lot of efforts. We provide an experimental automatic scheduling functions in Schedule . You can call s.auto_schedule to pick schedules fully automatically. s.auto_schedule calls other s.auto_xxxxxx functions internally, you can also call one or some of them instead. Please note that these auto-scheduling functions are experimental, and their API is subject to changes.","title":"Optimize a Program with Schedules"},{"location":"guide/schedules/#optimize-a-program-with-schedules","text":"Oftentimes, only compiling your programs to native code is not enough, and you need further optimizations. This can be done by applying \"schedules\" (explicit program transformations) to you program.","title":"Optimize a Program with Schedules"},{"location":"guide/schedules/#example-parallel-vector-addition","text":"import freetensor as ft import numpy as np n = 4 # Add verbose=1 to see the resulting native code @ft.optimize(schedule_callback=lambda s: s.parallelize('Li', 'openmp') ) # <-- 2. Apply the schedule def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li # <-- 1. Label the loop as Li for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Here is an example of a parallel vector addition executed with OpenMP multithreading. Each element is computed by one thread. To achieve this, there are two steps: Label the loop to be parallelized with a #! label: comment. Here label refers to label of an AST node, which is not required to be unique. Apply a parallelize schedule to Li in the schedule_callback argument to optimize ; since the Li label is unambiguous here, the only Li loop is selectd and parallelized. And you are done. You can have a look at the generated OpenMP multithreaded code by setting verbose=1 . Parameter s in schedule_callback is a Schedule object. Besides parallelize , there are more supported scheduling primitives. If you are using the @optimize_to_pytorch integration , you need to set schedules for the forward pass and the backward pass separately.","title":"Example: Parallel Vector addition"},{"location":"guide/schedules/#combining-multiple-schdules","text":"Some optimizations can be done by applying multiple schedules. For example, a tiled matrix-multiplication can be done by first split the loops, then reorder them, and finally apply cache s to create tile tensors. In order to demonstrate the idea, we show a simplier example here: still a vector addtion, but with the loop split and only the outer one parallelize d. Please note that this is an example only for demonstration. Usually you do not need it because OpenMP has its own \"schedule(static)\" for parallelized loops. import freetensor as ft import numpy as np n = 1024 def sch(s): outer, inner = s.split('Li', 32) s.parallelize(outer, 'openmp') # Set verbose=1 to see the resulting native code # Set verbose=2 to see the code after EVERY schedule @ft.optimize(schedule_callback=sch) def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(np.arange(1024), dtype=\"int32\"), np.array(np.arange(1024), dtype=\"int32\")).numpy() print(y) One important thing is to track labels of the loops, because the labels will change after schedules. You get labels (to be precise, IDs, which is can be looked-up by labels) of new loops generated from one schedule from its return values ( outer and inner in this case), and pass them to a next schedule.","title":"Combining Multiple Schdules"},{"location":"guide/schedules/#specify-what-to-schedule-by-selectors","text":"In the example above, we label a loop Li and apply schedules on it. It is straight-forward in a tiny example, but as programs grow, it often gets hard to track each statement by a unique label, especially there are inlined function calls. To make things easy, FreeTensor supports specifying a statement by a selector, written in the following rules: A label is a selector. E.g., Li matches a statement with a label Li . (For debugging only) A numerical ID is also a selector. E.g., #31 . A node type surrounded in angle brackets ( <> ) is also a selector. E.g., <For> matches for-loop statements. A selector can be extended to match a new statement produced by a previous schedule. E.g., $split.0{Li} matches the outer loop split from the loop Li . This is useful when return values from schedules are hard to track. Please refer the API document for detailed grammar. Selectors can be combined to match a statement by nesting order. A<-B matches a statement A DIRECTLY NESTED IN another statement B . A<<-B matches a statement DIRECTLY or INDIRECTLY nested in another statement B . A<-(B<-)*C matches a statement A DIRECTLY or INDIRECTLY nested in another statement C with intermedaite nesting statements satisfying the condition in B . B->A matches a statement B directly OUT OF another statement A . B->>A and C->(B->)*A are alike. ( A , B , C can be nested selectors.) Use <-| for the root node, and ->| for a leaf node. Selectors can be combined to match a statement by DFS order. A<:B matches a statement A DIRECTLY BEFORE another statement B . A<<:B matches a statement A DIRECTLY or INDIRECTLY before another statement B . B:>A matches a statment B directly AFTER another statement A . B:>>A matches a statement B directly or indirectly after another statement A . Selectors can be combined to match a statement in a function call. A<~B matches a statement A DIRECTLY called by a call site B . A<<~B matches a statement DIRECTLY or INDIRECTLY called by a call site B . A<~(B<~)*C matches a statement A DIRECTLY or INDIRECTLY called by a call site C with intermediate call sites satisfying the condition in B . ( A , B , C can be nested selectors.) Use <~| for the root function. All the arrow-like selectors ( <- , <~ , <: , etc.) are right-associated. For example, A<-B<-C matches A nested in B , where B is nested in C . All the arrow-like selectors can be used with the first argument omitted. For example, <-B matches ALL statements nested in B . Selectors can be combined with logical \"and\" ( & ), \"or\" ( | ), \"not\" ( ! ) and parentheses. E.g., Li|Lj matches a statement labeled Li OR Lj . Li&Lj matches a statement labeled Li&Lj . All schedules support passing selectors.","title":"Specify What to Schedule by Selectors"},{"location":"guide/schedules/#auto-scheduling-experimental","text":"Manually scheduling a program requires a lot of efforts. We provide an experimental automatic scheduling functions in Schedule . You can call s.auto_schedule to pick schedules fully automatically. s.auto_schedule calls other s.auto_xxxxxx functions internally, you can also call one or some of them instead. Please note that these auto-scheduling functions are experimental, and their API is subject to changes.","title":"Auto Scheduling (Experimental)"}]}