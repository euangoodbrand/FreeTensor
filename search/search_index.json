{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FreeTensor \u00b6 A language and compiler for irregular tensor programs. GitHub User Guide API Reference Publication License Features by Example \u00b6 Write a simple vector addition with loops that compiles to native code: import freetensor as ft import numpy as np n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) If you are not willing to compile the program once for each different n , you can set n as another function argument (but you may lose some performance). In FreeTensor, all variables are tensors, where scalars are 0-D tensors. import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) assert np.array_equal(y, [3, 5, 7, 9]) If building with CUDA, you can also run the program on a GPU. This time, a \" schedule \" (an explicit program transformation) is needed, and memory types of variables should be properly set. import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize(\"Li\", \"threadIdx.x\")) # Use \"byvalue\" for `n` so it can be used both during kernel launching # and inside a kernel def test(n: ft.Var[(), \"int32\", \"input\", \"byvalue\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li # Name the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Some common tensor operations, including tensor addition (broadcasting is supported), are pre-defined functions in FreeTensor. They are defiend in freetensor.libop , and they can also be invoked using operator overloading. These functions are pure Python functions, which will be inlined into your code, and will enjoy a joint optimization. import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = a + b # Or y = ft.add(a, b) return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) FreeTensor also supports reverse-mode Automatic Differentiation: import freetensor as ft import numpy as np n = 4 def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.zeros((), \"float32\") for i in range(n): y[()] += a[i] * b[i] return y fwd, bwd, input_grads, output_grads = ft.grad(test, ['a', 'b'], [ft.Return()]) fwd = ft.optimize(fwd) bwd = ft.optimize(bwd) a = np.array([0, 1, 2, 3], dtype=\"float32\") b = np.array([3, 2, 1, 0], dtype=\"float32\") y = fwd(a, b) print(y.numpy()) dzdy = np.array(1, dtype='float32') dzda, dzdb = bwd(**{output_grads[ft.Return()]: dzdy})[input_grads['a'], input_grads['b']] print(dzda.numpy()) print(dzdb.numpy())","title":"Home"},{"location":"#features-by-example","text":"Write a simple vector addition with loops that compiles to native code: import freetensor as ft import numpy as np n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) If you are not willing to compile the program once for each different n , you can set n as another function argument (but you may lose some performance). In FreeTensor, all variables are tensors, where scalars are 0-D tensors. import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) assert np.array_equal(y, [3, 5, 7, 9]) If building with CUDA, you can also run the program on a GPU. This time, a \" schedule \" (an explicit program transformation) is needed, and memory types of variables should be properly set. import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize(\"Li\", \"threadIdx.x\")) # Use \"byvalue\" for `n` so it can be used both during kernel launching # and inside a kernel def test(n: ft.Var[(), \"int32\", \"input\", \"byvalue\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li # Name the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Some common tensor operations, including tensor addition (broadcasting is supported), are pre-defined functions in FreeTensor. They are defiend in freetensor.libop , and they can also be invoked using operator overloading. These functions are pure Python functions, which will be inlined into your code, and will enjoy a joint optimization. import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = a + b # Or y = ft.add(a, b) return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) FreeTensor also supports reverse-mode Automatic Differentiation: import freetensor as ft import numpy as np n = 4 def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.zeros((), \"float32\") for i in range(n): y[()] += a[i] * b[i] return y fwd, bwd, input_grads, output_grads = ft.grad(test, ['a', 'b'], [ft.Return()]) fwd = ft.optimize(fwd) bwd = ft.optimize(bwd) a = np.array([0, 1, 2, 3], dtype=\"float32\") b = np.array([3, 2, 1, 0], dtype=\"float32\") y = fwd(a, b) print(y.numpy()) dzdy = np.array(1, dtype='float32') dzda, dzdb = bwd(**{output_grads[ft.Return()]: dzdy})[input_grads['a'], input_grads['b']] print(dzda.numpy()) print(dzdb.numpy())","title":"Features by Example"},{"location":"api/","text":"Python API \u00b6 freetensor.core special \u00b6 freetensor.core.autograd \u00b6 freetensor . core . autograd . grad ( func = None , requires = None , provides = None , tapes =< GradTapeMode . NoReuseOnly : 2 > , tape_in_closure = True , reset_provided_grad = True , invert = True , user_grads = None , attach_backward = False , jit_cache =< function cache at 0x14a9900c19d0 > , verbose = 0 , _override_func_params = None ) \u00b6 Reverse mode automatic differentiation (out-of-place version) It returns a forward function, a backward function, and two maps on names. The forward function computes the original results. The backward function computes the gradients. The maps map from the names of the original arguments and return values, to the names of their gradients. If tape_in_closure is True (by default), the forward function has the same interface of the original function, but it will store some intermediate tensors (the tape) in some hidden global states. The backward functions receives the same inputs as the original function plus the graidents of the outputs, and also reads from the hidden states. The outputs of the original function are no longer exist in the backward function, and the input-outputs of the original function are converted to pure inputs. As grad is an out-of-place AD interface, the backward function returns the resulting gradients as additional return values. Names of the additional arguments and return values can be looked up in the maps returned by grad . If tape_in_closure is False, global states described above will be passed by explicit arguments and return values, so you can store or manipluate these states between the forward run and the backward run. Parameters: func ( AST ) \u2013 The original function requires ( Sequence[Union[str, freetensor.core.param_ret_dict.Parameter]] ) \u2013 Name of input variables that need gradients. A parameter of a function can also be specified with a Parameter object by position provides ( Sequence[Union[str, freetensor.core.param_ret_dict.Parameter, freetensor.core.param_ret_dict.Return]] ) \u2013 Name of output variables whose gradients are known. A mutable parameter of a function can also be specified with a Parameter object by position. A return value of a function can also be specified with a Return object by position tapes ( Union[Sequence, freetensor_ffi.GradTapeMode, freetensor_ffi.TapeStrategy] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure ( bool ) \u2013 True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True reset_provided_grad ( bool ) \u2013 If true, reset gradients for all variables in provides to 0 after use. This ensures the final result is correct when computing gradients of a program part by part with multiple calls to this function. If false, do not touch the provided gradient, which makes it convenient to run for multiple rounds for timing. invert ( bool ) \u2013 If set to true, it can reduce the amount of recomputation or taping required. However, this may result in a loss of precision for floating-point numbers. Defaults user_grads ( Optional[Sequence[freetensor_ffi.StmtSetToUserGrad]] ) \u2013 For custom gradient. You do not have to explicitly set this parameter unless you are manipulating func by yourself (not getting it from the Python frontend). See UserGrad for details attach_backward ( bool ) \u2013 If True, the forward function will be the only return value, with backward function and other metadata attached to it jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances verbose ( int ) \u2013 Verbosity level Returns: Func if attach_backward else Tuple \u2013 If attach_backward is False, return a tuple. Return[0]: forward AST. Return[1]: backward AST. Return[2]: Mapping from names in requries to its gradient name. Return[3]: Mapping from names in provides to its gradient name. If attach_backward is True, only the forward AST is returned, and others can be get by .backward , .input_name_to_gradient_name and .output_name_to_gradient_name on the return value, respectively. freetensor . core . autograd . grad_ ( func = None , requires = None , provides = None , tapes =< GradTapeMode . NoReuseOnly : 2 > , tape_in_closure = True , reset_provided_grad = True , invert = True , user_grads = None , attach_backward = False , jit_cache =< function cache at 0x14a9900c19d0 > , verbose = 0 , _override_func_params = None ) \u00b6 Reverse mode automatic differentiation (in-place version) It returns a forward function, a backward function, and two maps on names. The forward function computes the original results. The backward function computes the gradients. The maps map from the names of the original arguments and return values, to the names of their gradients. If tape_in_closure is True (by default), the forward function has the same interface of the original function, but it will store some intermediate tensors (the tape) in some hidden global states. The backward functions receives the same inputs as the original function plus the graidents of the outputs, and also reads from the hidden states. The outputs of the original function are no longer exist in the backward function, and the input-outputs of the original function are converted to pure inputs. As grad_ is an in-place AD interface, the backward function passes the resulting gradients by additional mutable arguments. Names of the additional arguments can be looked up in the maps returned by grad_ . If tape_in_closure is False, global states described above will be passed by explicit arguments and return values, so you can store or manipluate these states between the forward run and the backward run. Parameters: func ( AST ) \u2013 The original function requires ( Sequence[Union[str, freetensor.core.param_ret_dict.Parameter]] ) \u2013 Name of input variables that need gradients. A parameter of a function can also be specified with a Parameter object by position provides ( Sequence[Union[str, freetensor.core.param_ret_dict.Parameter, freetensor.core.param_ret_dict.Return]] ) \u2013 Name of output variables whose gradients are known. A mutable parameter of a function can also be specified with a Parameter object by position. A return value of a function can also be specified with a Return object by position tapes ( Union[Sequence, freetensor_ffi.GradTapeMode, freetensor_ffi.TapeStrategy] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure ( bool ) \u2013 True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True reset_provided_grad ( bool ) \u2013 If true, reset gradients for all variables in provides to 0 after use. This ensures the final result is correct when computing gradients of a program part by part with multiple calls to this function. If false, do not touch the provided gradient, which makes it convenient to run for multiple rounds for timing. invert ( bool ) \u2013 If set to true, it can reduce the amount of recomputation or taping required. However, this may result in a loss of precision for floating-point numbers. Defaults to true. user_grads ( Optional[Sequence[freetensor_ffi.StmtSetToUserGrad]] ) \u2013 For custom gradient. You do not have to explicitly set this parameter unless you are manipulating func by yourself (not getting it from the Python frontend). See UserGrad for details attach_backward ( bool ) \u2013 If True, the forward function will be the only return value, with backward function and other metadata attached to it jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances verbose ( int ) \u2013 Verbosity level Returns: Func if attach_backward else Tuple \u2013 If attach_backward is False, return a tuple. Return[0]: forward AST. Return[1]: backward AST. Return[2]: Mapping from names in requries to its gradient name. Return[3]: Mapping from names in provides to its gradient name. If attach_backward is True, only the forward AST is returned, and others can be get by .backward , .input_name_to_gradient_name and .output_name_to_gradient_name on the return value, respectively. freetensor . core . autograd . grad_body ( stmt , requires , provides , tapes =< GradTapeMode . NoReuseOnly : 2 > , reset_provided_grad = True , invert = True , user_grads = []) \u00b6 grad or grad_ on a function body (for internal tests only) freetensor . core . autograd . jacrev ( func = None , inputs = None , output = None , flatten = False , tapes =< GradTapeMode . NoReuseOnly : 2 > , tape_in_closure = True , invert = True , user_grads = None , attach_backward = False , jit_cache =< function cache at 0x14a9900c19d0 > , verbose = 0 , _override_func_params = None ) \u00b6 Compute Jacobian tensors using Reverse mode automatic differentiation (out-of-place) jacrev computes one Jacobian tensor for one output and one or more inputs. Each Jacobian tensor consists of derivatives of all elements in the output tensor w.r.t. all elements in each inputs tensor. It returns a forward function, a backward function, and a map on names. The forward function computes the original results. The backward function computes the Jacobian tensors. The map maps from the names of the original arguments to the names of their Jacobian tensors. By default, the forward function has the same interface of the original function, but it will store some intermediate tensors (the tape) in some hidden global states. The backward functions receives the same inputs as the original function, and also reads from the hidden states. The outputs of the original function are no longer exist in the backward function, and the input-outputs of the original function are converted to pure inputs. As jacrev is an out-of-place interface, the backward function returns the resulting Jacobian as additional return values. Names of the additional return values can be looked up in the map returned by jacrev . Suppose the output's shape is (d1, d2, ...) , and there are two inputs, whose shapes are (e1, e2, ...) and (f1, f2, ...) , respectively. If flatten is False (by default), the Jacobian tensors' shape will be (d1, d2, ..., e1, e2, ...) and (d1, d2, ..., f1, f2, ...) , respectively. If flatten is True, there will be only one Jacbian tensor, whose shape will be (d1 * d2 * ..., e1 * e2 * ... + f1 * f2 * ...) . If tape_in_closure is False, global states described above will be passed by explicit arguments and return values, so you can store or manipluate these states between the forward run and the backward run. Parameters: func ( AST ) \u2013 The original function inputs ( Sequence[Union[str, freetensor.core.param_ret_dict.Parameter]] ) \u2013 Name of input variables that the Jacobian tensors are for. output ( Union[str, freetensor.core.param_ret_dict.Parameter, freetensor.core.param_ret_dict.Return] ) \u2013 Name of one output variables that the Jacobian tensors are for. A return value of a function can be specified with a Return object flatten ( bool ) \u2013 If True, concatenate all Jacobian tensors together, to form an (n, m) -shaped output, where n is the total number of elements in the specified output, and m is the total number of elements in the specified inputs. This requires all involved inputs having the same data type and memory type. In this case, the name of the Jacobian tensor will be \"jacrev.flatten\" , and the returned name map will be empty tapes ( Union[Sequence, freetensor_ffi.GradTapeMode, freetensor_ffi.TapeStrategy] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure ( bool ) \u2013 True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True invert ( bool ) \u2013 If set to true, it can reduce the amount of recomputation or taping required. However, this may result in a loss of precision for floating-point numbers. Defaults user_grads ( Optional[Sequence[freetensor_ffi.StmtSetToUserGrad]] ) \u2013 For custom gradient. You do not have to explicitly set this parameter unless you are manipulating func by yourself (not getting it from the Python frontend). See UserGrad for details attach_backward ( bool ) \u2013 If True, the forward function will be the only return value, with backward function and other metadata attached to it jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances verbose ( int ) \u2013 Verbosity level Returns: Func if attach_backward else Tuple \u2013 If attach_backward is False, return a tuple. Return[0]: forward AST. Return[1]: backward AST. Return[2]: Mapping from names in inputs to its Jacobian tensor name. If attach_backward is True, only the forward AST is returned, and others can be get by .backward and .input_name_to_gradient_name on the return value, respectively. freetensor . core . autograd . jacrev_ ( func = None , inputs = None , output = None , flatten = False , tapes =< GradTapeMode . NoReuseOnly : 2 > , tape_in_closure = True , invert = True , user_grads = None , attach_backward = False , jit_cache =< function cache at 0x14a9900c19d0 > , verbose = 0 , _override_func_params = None ) \u00b6 Compute Jacobian tensors using Reverse mode automatic differentiation (in-place) jacrev computes one Jacobian tensor for one output and one or more inputs. Each Jacobian tensor consists of derivatives of all elements in the output tensor w.r.t. all elements in each inputs tensor. It returns a forward function, a backward function, and a map on names. The forward function computes the original results. The backward function computes the Jacobian tensors. The map maps from the names of the original arguments to the names of their Jacobian tensors. By default, the forward function has the same interface of the original function, but it will store some intermediate tensors (the tape) in some hidden global states. The backward functions receives the same inputs as the original function, and also reads from the hidden states. The outputs of the original function are no longer exist in the backward function, and the input-outputs of the original function are converted to pure inputs. As jacrev_ is an in-place interface, the backward function passes the resulting gradients by additional mutable arguments. Names of the additional arguments can be looked up in the map returned by jacrev_ . Suppose the output's shape is (d1, d2, ...) , and there are two inputs, whose shapes are (e1, e2, ...) and (f1, f2, ...) , respectively. If flatten is False (by default), the Jacobian tensors' shape will be (d1, d2, ..., e1, e2, ...) and (d1, d2, ..., f1, f2, ...) , respectively. If flatten is True, there will be only one Jacbian tensor, whose shape will be (d1 * d2 * ..., e1 * e2 * ... + f1 * f2 * ...) . If tape_in_closure is False, global states described above will be passed by explicit arguments and return values, so you can store or manipluate these states between the forward run and the backward run. Parameters: func ( AST ) \u2013 The original function inputs ( Sequence[Union[str, freetensor.core.param_ret_dict.Parameter]] ) \u2013 Name of input variables that the Jacobian tensors are for. A parameter of a function can also be specified with a Parameter object by position output ( Union[str, freetensor.core.param_ret_dict.Parameter, freetensor.core.param_ret_dict.Return] ) \u2013 Name of one output variables that the Jacobian tensors are for. A mutable parameter of a function can also be specified with a Parameter object by position. A return value of a function can also be specified with a Return object by position flatten ( bool ) \u2013 If True, concatenate all Jacobian tensors together, to form an (n, m) -shaped output, where n is the total number of elements in the specified output, and m is the total number of elements in the specified inputs. This requires all involved inputs having the same data type and memory type. In this case, the name of the Jacobian tensor will be \"jacrev.flatten\" , and the returned name map will be empty tapes ( Union[Sequence, freetensor_ffi.GradTapeMode] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure ( bool ) \u2013 True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True invert ( bool ) \u2013 If set to true, it can reduce the amount of recomputation or taping required. However, this may result in a loss of precision for floating-point numbers. Defaults user_grads ( Optional[Sequence[freetensor_ffi.StmtSetToUserGrad]] ) \u2013 For custom gradient. You do not have to explicitly set this parameter unless you are manipulating func by yourself (not getting it from the Python frontend). See UserGrad for details attach_backward ( bool ) \u2013 If True, the forward function will be the only return value, with backward function and other metadata attached to it jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances verbose ( int ) \u2013 Verbosity level Returns: Func if attach_backward else Tuple \u2013 If attach_backward is False, return a tuple. Return[0]: forward AST. Return[1]: backward AST. Return[2]: Mapping from names in inputs to its Jacobian tensor name. If attach_backward is True, only the forward AST is returned, and others can be get by .backward and .input_name_to_gradient_name on the return value, respectively. freetensor.core.codegen \u00b6 freetensor.core.codegen.NativeCode ( EnableAttachBackward , NativeCode ) \u00b6 Generated native code with metadata NOTE: This class does not support serialization yet. If you need serialization, serialize the Func, and re-run codegen. freetensor . core . codegen . NativeCode . __contains__ ( self , item ) special \u00b6 Legacy interface for testing if a string is in the code freetensor . core . codegen . codegen ( ast = None , target = None , jit_cache =< function cache at 0x14a9900c19d0 > , verbose = False ) \u00b6 Generate native code Parameters: ast ( Func ) \u2013 The AST to be lowered. It must includes function signature to determine parameters and return values. If not specified, a partial function is returned, which can be used as a decorator jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances target ( Optional[freetensor_ffi.Target] ) \u2013 The target architecture. If omitted, use the default one in config Returns: NativeCode \u2013 Return a NativeCode for the generated code if there is no JIT parameters. Return a JITTemplate that generates a NativeCode if there is at least one freetensor.core.config \u00b6 Global configurations freetensor . core . config . backend_compiler_cxx ( * args , ** kvs ) \u00b6 backend_compiler_cxx() -> List[str] Backend compiler used to compile generated C++ code freetensor . core . config . backend_compiler_nvcc ( * args , ** kvs ) \u00b6 backend_compiler_nvcc() -> List[str] Backend compiler used to compile generated CUDA code freetensor . core . config . backend_openmp ( * args , ** kvs ) \u00b6 backend_openmp() -> List[str] OpenMP library linked to the compiled executable freetensor . core . config . debug_binary ( * args , ** kvs ) \u00b6 debug_binary() -> bool Check if compiling binary in debug mode freetensor . core . config . debug_cuda_with_um ( * args , ** kvs ) \u00b6 debug_cuda_with_um() -> bool Check if debugging with Unified Memory enabled freetensor . core . config . default_device ( * args , ** kvs ) \u00b6 default_device() -> freetensor_ffi.Device Check current default device freetensor . core . config . default_target ( * args , ** kvs ) \u00b6 default_target() -> freetensor_ffi.Target Check current default target freetensor . core . config . fast_math ( * args , ** kvs ) \u00b6 fast_math() -> bool Run pass/float_simplify optimization pass, and enable fast math on backend compilers freetensor . core . config . pretty_print ( * args , ** kvs ) \u00b6 pretty_print() -> bool Check if colored printing enabled freetensor . core . config . print_all_id ( * args , ** kvs ) \u00b6 print_all_id() -> bool Check if printing IDs of all statements in an AST freetensor . core . config . print_source_location ( * args , ** kvs ) \u00b6 print_source_location() -> bool Check if printing Python source location of all statements in an AST freetensor . core . config . set_backend_compiler_cxx ( * args , ** kvs ) \u00b6 set_backend_compiler_cxx(path: List[str]) -> None Set backend compiler used to compile generated C++ code, unescaped raw path expected freetensor . core . config . set_backend_compiler_nvcc ( * args , ** kvs ) \u00b6 set_backend_compiler_nvcc(path: List[str]) -> None Set backend compiler used to compile generated CUDA code, unescaped raw path expected freetensor . core . config . set_backend_openmp ( * args , ** kvs ) \u00b6 set_backend_openmp(arg0: List[str]) -> None Set the OpenMP library linked to the compiled executable freetensor . core . config . set_debug_binary ( * args , ** kvs ) \u00b6 set_debug_binary(flag: bool = True) -> None Compile with -g at backend. FreeTensor will not delete the binary file after loading it freetensor . core . config . set_debug_cuda_with_um ( * args , ** kvs ) \u00b6 set_debug_cuda_with_um(arg0: bool) -> None Allocate CUDA buffers on Unified Memory, for faster (debugging) access of GPU Array from CPU, but with slower Array allocations and more synchronizations. No performance effect on normal in-kernel computations freetensor . core . config . set_default_device ( * args , ** kvs ) \u00b6 set_default_device(device: freetensor_ffi.Device) -> None Set default device (internal implementation of with Device ) freetensor . core . config . set_default_target ( * args , ** kvs ) \u00b6 set_default_target(target: freetensor_ffi.Target) -> None Set default target (internal implementation of with Target ) freetensor . core . config . set_fast_math ( * args , ** kvs ) \u00b6 set_fast_math(flag: bool = True) -> None Set to run pass/float_simplify optimization pass, and enable fast math on backend compilers (or not) freetensor . core . config . set_pretty_print ( * args , ** kvs ) \u00b6 set_pretty_print(flag: bool = True) -> None Set colored printing freetensor . core . config . set_print_all_id ( * args , ** kvs ) \u00b6 set_print_all_id(flag: bool = True) -> None Print IDs of all statements in an AST freetensor . core . config . set_print_source_location ( * args , ** kvs ) \u00b6 set_print_source_location(flag: bool = True) -> None Print Python source location of all statements in an AST freetensor . core . config . set_werror ( * args , ** kvs ) \u00b6 set_werror(flag: bool = True) -> None Error on warning freetensor . core . config . werror ( * args , ** kvs ) \u00b6 werror() -> bool Check if error-on-warning enabled freetensor . core . config . with_cuda ( * args , ** kvs ) \u00b6 with_cuda() -> bool Check if FreeTensor is built with CUDA freetensor . core . config . with_mkl ( * args , ** kvs ) \u00b6 with_mkl() -> bool Check if FreeTensor is built with MKL freetensor . core . config . with_pytorch ( * args , ** kvs ) \u00b6 with_pytorch() -> bool Check if FreeTensor is built with PyTorch interface freetensor.core.context \u00b6 Facility to pick statements to build an AST Classes and functions in this module are internally used by transformer to construct ASTs. They are also used by some internal tests. API of these classes and functions are subject to changes. End users are encouraged to use transformer , instead of this module. freetensor.core.context.ContextStack \u00b6 freetensor . core . context . ContextStack . get_last_stmt_id ( self ) \u00b6 Can be used inside the staged code, to get the ID of the immediately preceding statement freetensor . core . context . ContextStack . push_append_stmt_callback ( self , callback ) \u00b6 Add a callback to be called with all next statements to be appended. For If statement, it can be called twice, one without \"else\" branch, and then maybe one more with \"else\" branch freetensor.core.context.StmtRange \u00b6 Record a set of statement in a program, can be used for custom gradient Usage: with StmtRange() as rng: # Some statements StmtRange can be used interleaved with AST scopes. In these cases, you can directly call __enter__ and __exit__ . E.g., rng = StmtRange() rng.__enter__() # Some statements with VarDef(...) # Some scopes # Some other statements rng.__exit__(None, None, None) freetensor . core . context . pop_ast ( verbose = False ) \u00b6 Get AST and reset context Internally used by transformer and tests freetensor . core . context . pop_ast_and_user_grads ( verbose = False ) \u00b6 Get AST and reset context. Return an extra list for custom gradients Set UserGrad for details freetensor.core.driver \u00b6 freetensor.core.driver.Driver ( EnableAttachBackward , Driver ) \u00b6 freetensor . core . driver . Driver . __call__ ( self , * args , ** kws ) special \u00b6 Set argument, execute the binary code, and collect the returns If there is only one return value, it is returned directly. Otherwise, the return values are packed in a ReturnValuesPack This function will introduce some overhaed handling arguments and return values. For an accurate execution time measurement, plase call self.set_args first, then self.time , and finally self.collect_returns freetensor . core . driver . Driver . __init__ ( self , native_code , device = None , host_device = None , cxx_flags = [], verbose = False ) special \u00b6 Compile a program using a backend compiler and load it into memory This class is for internal use. Please consider using build_binary Parameters: native_code ( NativeCode ) \u2013 Native code generated from codegen device ( Optional[freetensor_ffi.Device] ) \u2013 The device to run the program. If omitted, use the default device in config cxx_flags ( Sequence[str] ) \u2013 Additional C++ flags passed to the backend compiler verbose ( bool ) \u2013 True to print extra infomation freetensor . core . driver . Driver . collect_returns ( self , always_return_pack = False ) \u00b6 Collect return values from an invocation Return values must be collect. Otherwise there will be memory leaks If there is only one return value, it is returned directly. Otherwise, or if always_return_pack is set, the return values are packed in a ReturnValuesPack freetensor . core . driver . Driver . native_code ( self ) \u00b6 Get native code compiled by backend compiler freetensor . core . driver . Driver . set_args ( self , * args , ** kws ) \u00b6 Set argument for an invocation freetensor . core . driver . array ( data , dtype = None , dont_drop_borrow = False , moved = False ) \u00b6 Factory function for Array This function is preferred over directly calling Array 's constructor, because it accepts more data format. If data is another FreeTensor Array , the original object will be returned, with dont_drop_borrow and moved set to new values. If dtype is set and different from the original data type, the Array will be copied first to convert the data type. If data is Numpy Array or PyTorch Tensor , it will be converted to FreeTensor Array . Memory copy will be avoided in most cases, but it is inevitable if the data is strided. If dtype is set and different from the original data type, the Array or Tensor will be copied first to convert the data type. Otherwise, the data will be treated as an n-dimensional array-like object, and will be parsed according the rules in NumPy. The data type is also set accordingly, unless dtype is set. Parameters: data ( FreeTensor Array, Numpy Array, PyTorch Tensor, or other array-like objects ) \u2013 Data to be copied to or borrowed by the new Array object dtype ( ft.DataType or str ) \u2013 If data is not in dtype , convert it to dtype first before constructing the Array dont_drop_borrow ( bool ) \u2013 If true, report an error if we have to drop a borrwed data. This flag is set to true when the Array is cunstructed IMPLICITLY (not by this function) from a user object by borrowing from it, where users may expect they are acutually manipulating the their user object, instead of this Array moved ( bool ) \u2013 If true, it means we do not care about data in this Array any more after the program runs. Variables with \"input-mutable\" access type may modify the Array freetensor . core . driver . build_binary ( code = None , device = None , host_device = None , jit_cache =< function cache at 0x14a9900c19d0 > , cxx_flags = [], verbose = False ) \u00b6 Compile a program using a backend compiler and load it into memory Parameters: code ( Optional[freetensor.core.codegen.NativeCode] ) \u2013 Native code generated by codegen . If not specified, a partial function is returned, which can be used as a decorator device ( Optional[freetensor_ffi.Device] ) \u2013 The device to run the program. If omitted, use the default device in config jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances cxx_flags ( Sequence[str] ) \u2013 Additional C++ flags passed to the backend compiler verbose ( bool ) \u2013 Verbosity level Returns: Driver or JITTemplate \u2013 Return a Driver for the executable if there is no JIT parameters. Return a JITTemplate that generates a Driver if there is at least one freetensor . core . driver . move ( data ) \u00b6 Alias for array(data, dont_drop_borrow=False, moved=True) freetensor.core.enable_attach_backward \u00b6 freetensor.core.enable_attach_backward.EnableAttachBackward \u00b6 Get backward object (Func, Driver, etc) and other meta data from a forward object This class is a Mixin Class. It should be inherited BEFORE other base classes in multiple inheritance. freetensor . core . enable_attach_backward . EnableAttachBackward . __init__ ( self , * args , ** kvs ) special \u00b6 Forward all arguments to other base classes In Python, super(). init calls the next base class in the full inheritance graph of the final class, not only base classes of BackwardAttachedMixin. See https://docs.python.org/3/tutorial/classes.html#multiple-inheritance freetensor.core.expr \u00b6 Facility to build AST expressions Classes and functions in this module are not only used internally for constructing AST nodes, and also exposed to users via multi-stage programming freetensor.core.expr.AlreadyMadeReduceTo \u00b6 A single-value type that marks a ReduceTo node is already made, and there is no need to make another Store node In standard Python data model, functions like iadd returns the modified self, and setitem does a self-assignment. We do the augmenting assignment directly in iadd and return AlreadyMadeReduceTo, so we do not have to Store it again freetensor.core.expr.UndeclaredParam dataclass \u00b6 Error type. For error reporting only. freetensor.core.expr.VarRef ( FrontendVar ) \u00b6 Variable of FreeTensor All variables in FreeTensor DSL (declared via Var , created by empty or var , returned by libop , etc.), and their slices, are VarRef objects. Operations on VarRef objects generates AST nodes freetensor . core . expr . VarRef . as_reduce_to ( self , reduce_op , metadata , value , atomic = False ) \u00b6 as_reduce_to(self: freetensor_ffi.FrontendVar, op: freetensor_ffi.ReduceOp, metadata: freetensor_ffi.Metadata, value: freetensor_ffi.Expr, atomic: bool = False) -> freetensor_ffi.Stmt freetensor . core . expr . VarRef . as_store ( self , metadata , value ) \u00b6 as_store(self: freetensor_ffi.FrontendVar, metadata: freetensor_ffi.Metadata, value: freetensor_ffi.Expr) -> freetensor_ffi.Stmt freetensor . core . expr . VarRef . select ( self , idx , dim ) \u00b6 Alias for self[..., idx, ...] , where idx is at the dim -th dimension freetensor . core . expr . VarRef . select_slice ( self , begin , end , * , dim ) \u00b6 Alias for self[..., begin:end, ...] , where begin:end is at the dim -th dimension freetensor . core . expr . VarRef . shape ( self , dim = None ) \u00b6 Return lengths of all dimensions or the length of one dimension .shape() -> list of lengths of all dimensions .shape(dim) -> length of dimension dim , where dim can be int or Expr All lengths can be Expr (if the length is dynamically decided) or int (if statically decided) freetensor.core.expr.VarRefFromVarDef ( VarRef ) \u00b6 VarRef with extra checks freetensor . core . expr . VarRefFromVarDef . as_reduce_to ( self , reduce_op , metadata , value , atomic = False ) inherited \u00b6 as_reduce_to(self: freetensor_ffi.FrontendVar, op: freetensor_ffi.ReduceOp, metadata: freetensor_ffi.Metadata, value: freetensor_ffi.Expr, atomic: bool = False) -> freetensor_ffi.Stmt freetensor . core . expr . VarRefFromVarDef . as_store ( self , metadata , value ) inherited \u00b6 as_store(self: freetensor_ffi.FrontendVar, metadata: freetensor_ffi.Metadata, value: freetensor_ffi.Expr) -> freetensor_ffi.Stmt freetensor . core . expr . VarRefFromVarDef . select ( self , idx , dim ) inherited \u00b6 Alias for self[..., idx, ...] , where idx is at the dim -th dimension freetensor . core . expr . VarRefFromVarDef . select_slice ( self , begin , end , * , dim ) inherited \u00b6 Alias for self[..., begin:end, ...] , where begin:end is at the dim -th dimension freetensor . core . expr . VarRefFromVarDef . shape ( self , dim = None ) inherited \u00b6 Return lengths of all dimensions or the length of one dimension .shape() -> list of lengths of all dimensions .shape(dim) -> length of dimension dim , where dim can be int or Expr All lengths can be Expr (if the length is dynamically decided) or int (if statically decided) freetensor.core.expr.VarVersionRef ( VarRef ) \u00b6 Special VarRef used for custom gradient, generated from mark_version freetensor . core . expr . VarVersionRef . as_reduce_to ( self , reduce_op , metadata , value , atomic = False ) inherited \u00b6 as_reduce_to(self: freetensor_ffi.FrontendVar, op: freetensor_ffi.ReduceOp, metadata: freetensor_ffi.Metadata, value: freetensor_ffi.Expr, atomic: bool = False) -> freetensor_ffi.Stmt freetensor . core . expr . VarVersionRef . as_store ( self , metadata , value ) inherited \u00b6 as_store(self: freetensor_ffi.FrontendVar, metadata: freetensor_ffi.Metadata, value: freetensor_ffi.Expr) -> freetensor_ffi.Stmt freetensor . core . expr . VarVersionRef . select ( self , idx , dim ) inherited \u00b6 Alias for self[..., idx, ...] , where idx is at the dim -th dimension freetensor . core . expr . VarVersionRef . select_slice ( self , begin , end , * , dim ) inherited \u00b6 Alias for self[..., begin:end, ...] , where begin:end is at the dim -th dimension freetensor . core . expr . VarVersionRef . shape ( self , dim = None ) inherited \u00b6 Return lengths of all dimensions or the length of one dimension .shape() -> list of lengths of all dimensions .shape(dim) -> length of dimension dim , where dim can be int or Expr All lengths can be Expr (if the length is dynamically decided) or int (if statically decided) freetensor . core . expr . abs ( expr ) \u00b6 Absolute value For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.abs Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The absolute value freetensor . core . expr . add ( lhs , rhs ) \u00b6 lhs + rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.add Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The sum freetensor . core . expr . any () \u00b6 Create an AnyExpr node (only for testing and type inference) Any nodes matches any expression nodes in ast.match freetensor . core . expr . cast ( expr , dtype ) \u00b6 Cast to another type Parameters: expr ( VarRef or Number ) \u2013 The operand dtype ( DataTypr or str ) \u2013 The target data type Returns: VarRef or Number \u2013 The result freetensor . core . expr . ceil ( expr ) \u00b6 Round a float up to an interger (towards +inf) For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ceil Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result freetensor . core . expr . ceildiv ( lhs , rhs ) \u00b6 Ceiling integer division of lhs dividing by rhs The result rounds towards positive infinity For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ceildiv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient freetensor . core . expr . cos ( expr ) \u00b6 Cosine For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.tanh Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result freetensor . core . expr . dtype ( var ) \u00b6 Get element data type of a variable freetensor . core . expr . eq ( lhs , rhs ) \u00b6 lhs == rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.eq Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison freetensor . core . expr . exp ( expr ) \u00b6 Natural exponent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.exp Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The exponent freetensor . core . expr . floor ( expr ) \u00b6 Round a float down to an interger (towards -inf) For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.floor Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result freetensor . core . expr . floordiv ( lhs , rhs ) \u00b6 Floored integer division of lhs dividing by rhs The result rounds towards negative infinity (following Python convention, instead of C) This function is recommended over round_towards_0_div , as it enjoys more optimizations For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.floordiv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient freetensor . core . expr . ge ( lhs , rhs ) \u00b6 lhs >= rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ge Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison freetensor . core . expr . gt ( lhs , rhs ) \u00b6 lhs > rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.gt Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison freetensor . core . expr . if_then_else ( cond , then_case , else_case ) \u00b6 Similar to then_case if cond else else_case NOTE: there is NO guarantee that only one branch will be executed. In some cases, both branches will be executed and the result of one of them will be picked. Therefore, please do NOT use if_then_else to guard an out-of-bound array indexing Parameters: cond ( VarRef of Number ) \u2013 Condition then_case ( VarRef or Number ) \u2013 Then-case experssion else_case ( VarRef or Number ) \u2013 Else-case expression Returns: VarRef or Number \u2013 The result freetensor . core . expr . intrinsic ( fmt , * params , * , ret_type = 'void' , has_side_effect = False ) \u00b6 Invoke whatever target code Parameters: fmt ( str ) \u2013 What to run. \"%\" is filled by parameters one by one. E.g. sinf(%). Use \"%%\" to escape for \"%\". If you need two adjacent parameters, type \"(%)(%)\" or \"% %\". *params ( Sequence[Expr] ) \u2013 (Positional variadic) Parameters to fmt ret_type ( DataType or str ) \u2013 (Keyword argument only) The return type. Void for no return type. Defaults to Void has_side_effect ( bool ) \u2013 (Keyword argument only) True to indicate the intrinsic modifes something other than the return value. Defaults to false freetensor . core . expr . l_and ( lhs , rhs ) \u00b6 Logical and of lhs and rhs NOTE: Short-circuit evaluation is NOT supported For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_and Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The logical and freetensor . core . expr . l_not ( expr ) \u00b6 Logical not For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_not Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The logical not freetensor . core . expr . l_or ( lhs , rhs ) \u00b6 Logical or of lhs and rhs NOTE: Short-circuit evaluation is NOT supported For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_or Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The logical or freetensor . core . expr . le ( lhs , rhs ) \u00b6 lhs <= rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.le Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison freetensor . core . expr . ln ( expr ) \u00b6 Natural logarithm For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ln Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The exponent freetensor . core . expr . load_at_version ( tape_name , dtype , * indices ) \u00b6 Create an LoadAtVersion node (only for custom gradient) This node is only used for custom gradient. See UserGradForPrevStmt . freetensor . core . expr . lt ( lhs , rhs ) \u00b6 lhs < rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.lt Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison freetensor . core . expr . max ( lhs , rhs ) \u00b6 Maximum of lhs and rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.max Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The maximum freetensor . core . expr . min ( lhs , rhs ) \u00b6 Minimum of lhs and rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.min Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The minimum freetensor . core . expr . mod ( lhs , rhs ) \u00b6 lhs modulus rhs The result is always non-negative (following Python convention, instead of C). This function is recommended over remainder , as it enjoys more optimizations For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.mod Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The modulo freetensor . core . expr . mtype ( var ) \u00b6 Get memory type of a variable freetensor . core . expr . mul ( lhs , rhs ) \u00b6 lhs * rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.mul Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The product freetensor . core . expr . ndim ( var ) \u00b6 Get the number of dimensions of a variable freetensor . core . expr . ne ( lhs , rhs ) \u00b6 lhs != rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ne Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison freetensor . core . expr . remainder ( lhs , rhs ) \u00b6 Remainder of lhs dividing rhs The result can be positive or negative (following C convention, instead of Python). End users are encouraged to use lhs % rhs instead, which follows Python convetion, and enjoys better optimization in FreeTensor For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.remainder Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The remainder freetensor . core . expr . round_towards_0_div ( lhs , rhs ) \u00b6 C-style integer division of lhs dividing by rhs The result rounds towards 0 (following C convention, instead of Python) End users are encouraged to use lhs // rhs instead, which follows Python convetion, and enjoys better optimization in FreeTensor For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.round_towards_0_div Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient freetensor . core . expr . shape ( var , i = None ) \u00b6 shape(var, i): Get size of specified dimension of a variable shape(var): Get sizes of all dimensions of a variable freetensor . core . expr . sigmoid ( expr ) \u00b6 Sigmoid For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sigmoid Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result freetensor . core . expr . sin ( expr ) \u00b6 Sine For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.tanh Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result freetensor . core . expr . sqrt ( expr ) \u00b6 Square root For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sqrt Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The square root freetensor . core . expr . square ( expr ) \u00b6 Square For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.square Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The square freetensor . core . expr . sub ( lhs , rhs ) \u00b6 lhs - rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sub Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The difference freetensor . core . expr . tan ( expr ) \u00b6 Tangent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.tanh Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result freetensor . core . expr . tanh ( expr ) \u00b6 Hyperbolic tangent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.tanh Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result freetensor . core . expr . truediv ( lhs , rhs ) \u00b6 Floating point division of lhs dividing by rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.truediv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient freetensor.core.frontend \u00b6 A frontend transforming user Python functions to ASTs via staging. freetensor.core.frontend.FreeTensorOverload ( StagingOverload ) \u00b6 Helper class managing context in IR staging. freetensor . core . frontend . FreeTensorOverload . allow_shortcut_scope ( self , allow ) inherited \u00b6 Opens a scope that allows shortcut control flows in a statically deterministic context. Need to be closed by with statement. freetensor . core . frontend . FreeTensorOverload . assert_stmt ( self , test ) inherited \u00b6 Assert staging tool. freetensor . core . frontend . FreeTensorOverload . assign_stmt ( self , name , value ) inherited \u00b6 Customized assign wrapper. If value is instance of StagedAssignable , it's regarded as a customized assign behavior and gets executed with the assigned target variable name. This wrapper is used for initializing a variable. freetensor . core . frontend . FreeTensorOverload . break_stmt ( self ) inherited \u00b6 Break staging tool. Only allow break in static control flow. freetensor . core . frontend . FreeTensorOverload . continue_stmt ( self ) inherited \u00b6 Continue staging tool. Only allow continue in static control flow. freetensor . core . frontend . FreeTensorOverload . custom_attr ( self , obj , attr ) \u00b6 Customized attribute accessor. The framework first looks for a Python native attribute. If not found, it looks for this overloaded custom attribute resolver. The default implementation provides no custom attribute. Can be overridden by subclasses. Parameters: obj ( Any ) \u2013 Object to access attribute. attr ( str ) \u2013 Attribute name. Returns: Any \u2013 The attribute value. freetensor . core . frontend . FreeTensorOverload . foreach ( self , names , iter , body ) inherited \u00b6 Customized foreach wrapper. If value is instance of StagedIterable , its regarded as a customized foreach behavior and used to generate code for the python for loop. Otherwise, we try to execute the loop as usual. freetensor . core . frontend . FreeTensorOverload . fullname ( self , name ) \u00b6 Get distinct name. freetensor . core . frontend . FreeTensorOverload . functiondef_wrapper ( self , filename , func ) \u00b6 Function definition wrapper. This wrapper performs extra initialization and cleanup for function definition. freetensor . core . frontend . FreeTensorOverload . if_then_else_expr ( self , predicate , then_expr , else_expr ) inherited \u00b6 If-then-else expression staging tool. freetensor . core . frontend . FreeTensorOverload . if_then_else_stmt ( self , predicate , then_body , else_body = None ) inherited \u00b6 If-then-else statement staging tool. When predicate is deterministic in staging, only one branch is generated. Otherwise, a If node in IR is generated. freetensor . core . frontend . FreeTensorOverload . load_attr ( self , obj , attr ) inherited \u00b6 Load attribute staging tool. Allows customization of reading attributes. freetensor . core . frontend . FreeTensorOverload . mark_position ( self , lineno ) \u00b6 Code position handler. Defaults to a no-op. Can be overridden by subclasses. Parameters: lineno ( int ) \u2013 Line number of the next statement. freetensor . core . frontend . FreeTensorOverload . metadata ( self , entry ) \u00b6 Metadata handler. A metadata line is a comment starting with #! and followed by a metadata, represented as a string parameter. Defaults to a no-op. Can be overridden by subclasses. Parameters: content ( str ) \u2013 The metadata content. freetensor . core . frontend . FreeTensorOverload . return_stmt ( self , value , funcname ) inherited \u00b6 Return staging tool. Only allow return in static control flow. freetensor . core . frontend . FreeTensorOverload . unpack_assign_stmt ( self , names , values ) inherited \u00b6 Customized assign wrapper for one or more targets. If values is instance of StagedUnpackAssignable , it's regarded as a customized assign behavior and gets executed with all the assigned targets' names. Otherwise, it calls assign_stmt with each sub-assignments. Please note that names can be nested tuples like (\"a\", (\"b\", \"c\")) . Please also note that names can also be a single string like \"a\" even if values is a tuple. There is no unpacking in this case freetensor . core . frontend . FreeTensorOverload . while_stmt ( self , fpred , body ) inherited \u00b6 While statement staging tool. freetensor.core.frontend.FreeTensorStagingError ( StagingError ) \u00b6 Error occurred during staging function execution (i.e. IR tree generation). freetensor.core.frontend.LifetimeScope \u00b6 This scope is used to register multiple scopes inside a single lifetime scope. The inner scopes might be used to register variables, etc. They will be exited in reverse order of their registration. freetensor.core.frontend.PredefinedVarCreator ( VarCreator ) dataclass \u00b6 freetensor.core.frontend.PredefinedVarCreator.__class__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). freetensor . core . frontend . PredefinedVarCreator . __class__ . __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). freetensor . core . frontend . PredefinedVarCreator . __class__ . __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. freetensor . core . frontend . PredefinedVarCreator . __class__ . __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). freetensor . core . frontend . PredefinedVarCreator . __class__ . register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. freetensor . core . frontend . PredefinedVarCreator . __init__ ( self , initializer , dtype , mtype ) special \u00b6 Initialize self. See help(type(self)) for accurate signature. freetensor . core . frontend . PredefinedVarCreator . assign ( self , name ) \u00b6 Customized assign behavior. Creates a VarDef with its full name. freetensor.core.frontend.UserGrad ( UserGradStaged ) \u00b6 Define a custom gradient Follow the following steps to define custom gradient: Add some mark_version statements in the program. mark_version('y0', y) marks the specific versions of variable y at the program position of the statement and at all iterations as 'y0' . Add a UserGrad scope. 2.1. UserGrad optionally receives parameter stmt_range , recorded by the StmtRange helper class, which means the gradient is for the code specified in the range. Ignoring the parameter means setting gradient for the previous statement of the scope. 2.2. Other parameters of UserGrad sets the mapping from original variables to gradient variables. with UserGradForPrevStmt(x, y) as (dx, dy) provides VarRef dx and dy as gradient variables to be used inside the scope. In order to use the value from the forward pass in the backward pass, do not access the forward variables directly in the scope. Instead, use load_at_version expressions. load_at_version(y0, i, j) loads from y[i, j] at the specific version marked by y0 = mark_version(y) , saved from the same iteration in the forward pass . (If directly writing staged code, it is MarkVersion('y0', y) ). In other words, after AD, the position of mark_version and the dynamic loop iterator together makes up the actual version number for the tape. Build the AST with pop_ast_and_user_grads instead of pop_ast . An extra list will be returned together with the AST, which you need to pass as grad 's user_grads argument. This list records the forward-to-backward relation of the nodes. If you are directly writing staged code, use UserGradStaged instead. Parameters: *args ( Sequence[freetensor.core.expr.VarRef] ) \u2013 (Positional variadic) Mapping from original variables to gradient variables stmt_range ( StmtRange ) \u2013 The range in the original program that we are setting custom gradient for freetensor.core.frontend.Var ( StagedTypeAnnotation ) \u00b6 freetensor.core.frontend.Var.__class__ ( ABCMeta ) inherited \u00b6 freetensor.core.frontend.Var.__class__.__base__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). freetensor . core . frontend . Var . __class__ . __base__ . __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). freetensor . core . frontend . Var . __class__ . __base__ . __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. freetensor . core . frontend . Var . __class__ . __base__ . __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). freetensor . core . frontend . Var . __class__ . __base__ . register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. freetensor . core . frontend . Var . __class__ . register ( cls , subclass ) inherited \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. freetensor . core . frontend . Var . __init__ ( self , shape , dtype , atype = 'input' , mtype = None ) special \u00b6 Declare a variable Parameters: shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable atype ( str or AccessType ) \u2013 Access type of the variable. It specifies whether (and how) the variable is an I/O variable of the function it belongs to. Defaults to \"input\" mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used freetensor.core.frontend.VarCreator ( StagedAssignable ) dataclass \u00b6 VarCreator(shape: Union[Sequence, freetensor.core.expr.VarRef], dtype: str, mtype: str, assigned: bool = False) freetensor.core.frontend.VarCreator.__class__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). freetensor . core . frontend . VarCreator . __class__ . __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). freetensor . core . frontend . VarCreator . __class__ . __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. freetensor . core . frontend . VarCreator . __class__ . __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). freetensor . core . frontend . VarCreator . __class__ . register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. freetensor . core . frontend . VarCreator . assign ( self , name ) \u00b6 Customized assign behavior. Creates a VarDef with its full name. freetensor.core.frontend.VersionMarker ( StagedAssignable ) dataclass \u00b6 VersionMarker(var: freetensor.core.expr.VarRef, assigned: bool = False) freetensor.core.frontend.VersionMarker.__class__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). freetensor . core . frontend . VersionMarker . __class__ . __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). freetensor . core . frontend . VersionMarker . __class__ . __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. freetensor . core . frontend . VersionMarker . __class__ . __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). freetensor . core . frontend . VersionMarker . __class__ . register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. freetensor . core . frontend . VersionMarker . assign ( self , tape_name ) \u00b6 Customized assign behavior. Creates a MarkVersion with its full name. freetensor.core.frontend.dynamic_range ( StagedIterable ) \u00b6 Dynamic range that generates For loop in IR tree. freetensor . core . frontend . dynamic_range . __init__ ( self , start , stop = None , step = 1 ) special \u00b6 Initialize a dynamic range. Arguments semantic identical to builtin range . freetensor . core . frontend . dynamic_range . foreach ( self , name , body ) \u00b6 Customized foreach behavior. Creates a For loop. freetensor . core . frontend . capture_var ( * args , ** kwargs ) \u00b6 Capture external array as tensor variable. freetensor . core . frontend . empty ( * args , ** kwargs ) \u00b6 Create an empty variable Parameters: shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used freetensor . core . frontend . push_for_backward ( var ) \u00b6 Push the current value from the forward pass to be used at the backward pass This function is for custom gradients. See UserGrad for details on how to provide custom gradients. You may imagine there is a virtual stack for each variable. Each time you call x_handle = push_for_backward(x) in the forward pass, the value of x at the current iteration will be \"pushed\" to the virtual stack. You can access x_handle at the backward pass. Each time you access x_handle , you will \"pop\" the stack and get the value of x pushed at the same iteration . Since the \"stack\" is virtual, you do NOT need to \"pop\" the same count as \"push\"es: the version numbering is fully automatic. Besides, there may not be a real stack at runtime: it can be compiled to any data structure. This function will be staged to mark_version statement in the IR. freetensor . core . frontend . var ( * args , ** kwargs ) \u00b6 Create an with variable a given initializer Parameters: initializer ( Sequence[Sequence[...Sequence[Expr]...]] ) \u2013 (Multi-level of) sequence of expressions. Will be data of the variable shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used freetensor.core.func \u00b6 freetensor.core.func.Func ( EnableAttachBackward , Func ) \u00b6 freetensor . core . func . Func . __call__ ( self , * args , ** kvs ) special \u00b6 Enable invoking a transformed AST in another function being transformed, via inlined_invoke freetensor.core.jit \u00b6 freetensor.core.jit.JIT \u00b6 Declare a function parameter as a JIT parameter A function with one or more JIT parameters will be compiled to a JIT template. It can be instantiate after the JIT paraemters are provided Usage: x: JIT or x: JIT[AnyPythonType] . The latter form has no syntactic meanings, and is only for documentation. NOTE 1: The JIT type annotation can only be used for parameter of the outer-most function intended for @transform (or @optimize , etc). It can NOT be used for inner functions intended for @inline . NOTE 2: The JIT type annoation can only annotated on parameters inside the function signature. It is NOT supported in annotations for statements. freetensor.core.jit.JITTemplate ( ABC ) \u00b6 A template that can be instantiated given concrete arguments By calling instantiate with actual arguments you are expecting to run a JIT function with, an instantiated object will be returned. Subclasses of JITTemplate should override instantiate_by_only_jit_args , and define what is actually returned. Parameters: params ( OrderedDict ) \u2013 Parameter list from inspect.signature(func).parameters jit_param_names ( Sequence ) \u2013 Sequence of names of JIT parameters in the original order defined in the function freetensor.core.jit.JITTemplate.__class__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). freetensor . core . jit . JITTemplate . __class__ . __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). freetensor . core . jit . JITTemplate . __class__ . __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. freetensor . core . jit . JITTemplate . __class__ . __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). freetensor . core . jit . JITTemplate . __class__ . register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. freetensor . core . jit . JITTemplate . instantiate ( self , * args , ** kvs ) \u00b6 Get an instance with the arguments you are expecting to run a JIT function with freetensor . core . jit . JITTemplate . instantiate_and_call ( self , * args , ** kvs ) \u00b6 Get an instance and call it with the arguments you are expecting to run a JIT function with freetensor . core . jit . JITTemplate . instantiate_by_only_jit_args ( self , * jit_args ) \u00b6 Get an instance with only JIT arguments This function accpets a tuple of arguments. Keyword arguments is NOT supported, so memoization can be easier, with considering the order of the arguments. freetensor . core . jit . JITTemplate . separate_args ( self , * args , ** kvs ) \u00b6 Return a list of non-JIT args, and a list of JIT args freetensor.core.optimize \u00b6 freetensor . core . optimize . optimize ( func = None , schedule_callback = None , backward_schedule_callback = None , target = None , device = None , default_dynamic_range = True , jit_cache =< function cache at 0x14a9900c19d0 > , verbose = 0 ) \u00b6 An one-click optimization from Python function to binary executable Usage: @optimize def f(...): ... It is equivalent to: @build_binary @codegen @lower @transform def f(...): ... Parameters: func ( Python function or AST ) \u2013 The user function to optimize. If not specified, a partial function will be returend, which can be used as a decorator schedule_callback ( Optional[Callable[[freetensor.core.schedule.Schedule], NoneType]] ) \u2013 Schedule(s) to apply backward_schedule_callback ( Callable[[freetensor.core.schedule.Schedule], NoneType] ) \u2013 Specify what schedule(s) to do for the backward function, if ast is returned from AD with attach_backward=True . Defaults to be the same with callback target ( Optional[freetensor_ffi.Target] ) \u2013 The target architecture. You don't have to set target if you set device device ( Optional[freetensor_ffi.Device] ) \u2013 Where to run the program default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( int ) \u2013 Verbosity level. Can be 0, 1 or 2 freetensor . core . optimize . optimize_to_pytorch ( func = None , tapes =< GradTapeMode . NoReuseOnly : 2 > , forward_schedule_callback = None , backward_schedule_callback = None , target = None , device = None , default_dynamic_range = True , verbose = 0 ) \u00b6 Compile a FreeTensor function to a PyTorch call, whose gradient can be recognized by PyTorch The compiled function will be a typical PyTorch's \"function\" (rather than a PyTorch's \"module\"). Technically, this means it is a wrapper function around a PyTorch's Function 's apply method Schedules (if any) must be applied to the forward function and the backward function separated. For this reason, currently only first-order gradient is supported Parameters: func ( Python function or AST ) \u2013 The user function to optimize. If not specified, a partial function will be returend, which can be used as a decorator tapes ( Union[Sequence, freetensor_ffi.GradTapeMode] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history forward_schedule_callback ( Optional[Callable[[freetensor.core.schedule.Schedule], NoneType]] ) \u2013 Schedule(s) to apply to the forward function backward_schedule_callback ( Optional[Callable[[freetensor.core.schedule.Schedule], NoneType]] ) \u2013 Schedule(s) to apply to the backward function target ( Optional[freetensor_ffi.Target] ) \u2013 The target architecture. You don't have to set target if you set device device ( Optional[freetensor_ffi.Device] ) \u2013 Where to run the program default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( int ) \u2013 Verbosity level. Can be 0, 1 or 2 freetensor.core.param_ret_dict \u00b6 freetensor.core.param_ret_dict.ParamRetDict \u00b6 Look an object using either a function parameter or return value's name or position freetensor . core . param_ret_dict . ParamRetDict . __init__ ( self , d , * , func = None , func_name = None , param_names = None , return_names = None ) special \u00b6 Either func or ( func_name and param_names and return_names ) should be provided freetensor.core.param_ret_dict.Parameter \u00b6 Alias of a parameter of a function by position instead of by name Parameter(n) represents the n-th parameter (counted from 0) Parameter() can be used if there is only one parameter freetensor.core.param_ret_dict.Return \u00b6 Alias of a return value of a function by position instead of by name Return(n) represents the n-th return value (counted from 0) Return() can be used if there is only one return value freetensor.core.passes \u00b6 freetensor . core . passes . lower ( ast = None , target = None , skip_passes = [], jit_cache =< function cache at 0x14a9900c19d0 > , verbose = 0 ) \u00b6 Lower an AST using a series of passes Parameters: ast ( AST ) \u2013 The AST to be lowered. Can be a Func or a Stmt . If not specified, a partial function of lower will be returned, which can be used as a decorator target ( Optional[freetensor_ffi.Target] ) \u2013 Lower the AST to a target with target-specific passes, then the AST can be used for codegen. If not set, use the default Target in Config skip_passes ( Sequence[str] ) \u2013 Skip some pass for testing or debugging. Names in skip_passes are in underscore_style, as in Python. Please note that some passes will not be skipped even specified in these parameter, because they are indirectly called in some other passes jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances verbose ( int ) \u2013 0 = print nothing. 1 = print the lowered AST. 2 = print AST after every single passes Returns: Func or JITTemplate \u2013 Return a Func for an AST if there is no JIT parameters. Return a JITTemplate that generates a Func if there is at least one freetensor.core.return_values_pack \u00b6 freetensor.core.return_values_pack.ReturnValuesPack \u00b6 Hold return values from a Driver invocation Return values can be retrieved in an anonymous manner: x, y, z = pack , or in a named manner: pack['x'] Please note that a ReturnValuesPack is different from a OrderedDict, as OrderedDict unpacks to keys rather than values freetensor . core . return_values_pack . ReturnValuesPack . __contains__ ( self , key ) special \u00b6 Test if a return value exists freetensor . core . return_values_pack . ReturnValuesPack . __getitem__ ( self , key ) special \u00b6 Get a return value with a name. Tuple is supported for multiple values freetensor . core . return_values_pack . ReturnValuesPack . __iter__ ( self ) special \u00b6 Get all return values in the order declared in Func freetensor.core.schedule \u00b6 freetensor.core.schedule.IDMap \u00b6 A dict-like container recording an ID-to-ID mapping, representing what IDs become what IDs after a schedule An IDMap can be looked up by numerical ID, or by Stmt instances or Selector strings of the original (before applying schedule) AST freetensor.core.schedule.Schedule ( Schedule ) \u00b6 freetensor . core . schedule . Schedule . as_matmul ( self , loop ) \u00b6 Transform nested loops to be a external call to a matrix multiplication Parameters: loop ( str, ID or Stmt ) \u2013 ID of the loop Exceptions: InvalidSchedule \u2013 if the loop cannot be transformed to be a matrix multiplication freetensor . core . schedule . Schedule . ast ( self ) \u00b6 Get the scheduled AST without function signature This is mainly for debugging and testting purpose freetensor . core . schedule . Schedule . auto_fission_fuse ( self , target ) \u00b6 (Experimental) Automatically fuse consecutive loops or vice versa using some heuristics Parameters: target ( Target ) \u2013 Target architecture freetensor . core . schedule . Schedule . auto_inline ( self , target ) \u00b6 (Experimental) Automatically inline very-small VarDef nodes Parameters: target ( Target ) \u2013 Target architecture freetensor . core . schedule . Schedule . auto_parallelize ( self , target ) \u00b6 (Experimental) Automatically parallelize some loops using some heuristics Parameters: target ( Target ) \u2013 Target architecture freetensor . core . schedule . Schedule . auto_pluto ( self , target ) \u00b6 (Experimental) Automatically apply pluto-based schedules Parameters: target ( Target ) \u2013 Target architecture freetensor . core . schedule . Schedule . auto_schedule ( self , target ) \u00b6 (Experimental) Automatic scheduling using some heuristics Parameters: target ( Target ) \u2013 Target architecture freetensor . core . schedule . Schedule . auto_set_mem_type ( self , target ) \u00b6 (Experimental) Automatically set memory types using some heuristics Parameters: target ( Target ) \u2013 Target architecture freetensor . core . schedule . Schedule . auto_swap ( self , target ) \u00b6 (Experimental) Automatically swap statements to enable more fission or fusion Parameters: target ( Target ) \u2013 Target architecture freetensor . core . schedule . Schedule . auto_unroll ( self , target ) \u00b6 (Experimental) Automatically unroll loops using some heuristics Parameters: target ( Target ) \u2013 Target architecture freetensor . core . schedule . Schedule . auto_use_lib ( self , target ) \u00b6 (Experimental) Automatically use external libs using some heuristics Parameters: target ( Target ) \u2013 Target architecture freetensor . core . schedule . Schedule . blend ( self , loop ) \u00b6 Unroll a loop and interleave statements from each iteration E.g. for i = 0 to 2 { f(i); g(i); } will be transformed to be f(0); f(1); g(0); g(1); Virtual threads in TVM can be implemented via blend Parameters: loop ( str, ID or Stmt ) \u2013 The loop being transformed Exceptions: InvalidSchedule \u2013 if the loop is not found, the loop length is not a constant, or the dependences cannot be solved freetensor . core . schedule . Schedule . cache ( self , stmt , var , mtype ) \u00b6 Cache a variable into a new local variable All needed data will be filled into the cache first, then all reads and writes will be directed to the cache, and finally all needed data will be flushed from the cache Note for reduction: This transformation preserves the computation order. It will transform a += x a += y to a.cache = a + x + y a = a.cache If you need a \"real\" cache for reduction, which reorders the computation, use cache_reduction instead Parameters: stmt ( str, ID or Stmt ) \u2013 The statement or block (e.g. an If or a For) to be modified var ( str ) \u2013 Name of the variable to be cached mtype ( MemType ) \u2013 Where to cache Exceptions: InvalidSchedule \u2013 if the ID or name is not found Returns: (ID, ID, ID, ID) \u2013 (ID of the statement that fills the cache, ID of the statement that flushes from the cache, name of the cache variable, ID of the VarDef node of the cache variable) freetensor . core . schedule . Schedule . cache_reduction ( self , stmt , var , mtype ) \u00b6 Perform local reductions (e.g. sum) in a local variable first, and then reduce the local result to the global variable E.g. a += x a += y will be transformed to be a.cache = x + y a += a.cache Parameters: stmt ( str, ID or Stmt ) \u2013 The statement or block (e.g. an If or a For) to be modified var ( str ) \u2013 Name of the variable to be cached. Only reductions are allowed on var in stmt . Plain reads or writes are not allowed mtype ( MemType ) \u2013 Where to cache Exceptions: InvalidSchedule \u2013 if the ID or name is not found, or there are unsupported reads or writes Returns: (ID, ID, ID, ID) \u2013 (ID of the statement that initialize the cache, ID of the statement that reduces the local result to the global result, name of the cache variable, ID of the VarDef node of the cache variable) freetensor . core . schedule . Schedule . fission ( self , loop , side , splitter , allow_enlarge = True ) \u00b6 Fission a loop into two loops each containing part of the statements, one followed by another To split loop into two nested loops, use split instead Statements inside the original loop will be distributed to one or both (happening if they are scope statements) loops. If a statement is originally labeled \"S\", it can be selected by \" \\(fission.0{S}\" (from the first loop) or \"\\) fission.1{S}\" (from the second loop) after fission. If one of the resulting loop has an empty body, it will be removed Parameters: loop ( str, ID or Stmt ) \u2013 The loop to be fissioned side ( FissionSide ) \u2013 If After , splitter is the last statement of the first loop. If Before , splitter is the first statement of the second loop splitter ( str (Selector string), ID, Stmt, or list of them ) \u2013 Where to fission the loop. If multiple statement are selected, fission the look before or after all of them allow_enlarge ( bool ) \u2013 If True, try to avoid dependence by enlarging some VarDef nodes. If False, raise InvalidSchedule in such cases. Exceptions: InvalidSchedule \u2013 if any dependence cannot be resolved Returns: (IDMap, IDMap) \u2013 ({old ID -> new ID in 1st loop}, {old ID -> new ID in 2nd loop}). If a loop is removed because it has an empty body, it will not be in the returned map freetensor . core . schedule . Schedule . fork ( self ) \u00b6 fork(self: freetensor_ffi.Schedule) -> freetensor_ffi.Schedule freetensor . core . schedule . Schedule . func ( self ) \u00b6 Get the scheduled function freetensor . core . schedule . Schedule . fuse ( self , loop0 , loop1 = None , strict = False ) \u00b6 Fuse two directly following loops with the same length into one To merge nested loops into one, use merge instead parallelize , unroll and vectorize properties will be reset on the fused loop Suppose the original loops are labeled \"L1\" and \"L2\", the fused loop can be selected by \"$fuse{L1, L2}\" Parameters: loop0 ( str, ID or Stmt ) \u2013 The leading loop loop1 ( str, ID or Stmt, Optional ) \u2013 The following loop. If omitted, it will try to find a following loop of loop0 strict ( bool ) \u2013 False by default. If set to True, throw an error if unable to determine whether the two loops are of the same length Exceptions: InvalidSchedule \u2013 if the two loops are not directly following, the two loops are not of the same length, or there is any dependence cannot be resolved Returns: ID \u2013 ID of the result loop freetensor . core . schedule . Schedule . inline ( self , vardef ) \u00b6 Remove a variable. When the variable is used, recompute its value Parameters: vardef ( str, ID or Stmt ) \u2013 The VarDef statement of the specific variable. It can not be an I/O varible Exceptions: InvalidSchedule \u2013 if the variable cannot be completely removed freetensor . core . schedule . Schedule . merge ( self , loop1 , loop2 ) \u00b6 Merge two directly nested loops into one To fuse consecutive loops, use fuse instead parallelize , unroll and vectorize properties will be reset on the merged loop Suppose the original loops are labeled \"L1\" and \"L2\", the merged loop can be selected by \"$merge{L1, L2}\" Parameters: loop1, loop2 ( str, ID or Stmt ) \u2013 loops to be merged, can be in any order Exceptions: InvalidSchedule \u2013 if the loops are not directly nested Returns: ID \u2013 ID of the merged loop freetensor . core . schedule . Schedule . move_to ( self , stmt , side , dst ) \u00b6 Move a statement to a new position This is a composite schedule command, which is implemented with other commands If moving a statement out of some loops, identical loops will be added around the moved statement, which is equivalent to fission these loops Parameters: stmt ( str, ID or Stmt ) \u2013 The statement to be moved side ( MoveToSide ) \u2013 Whether stmt will be BEFORE or AFTER `dst dst ( str (Selector string), ID, Stmt, or list of them ) \u2013 Insert stmt to be directly after this statement. If multiple statements are selected, move to before or after all of them Exceptions: InvalidSchedule \u2013 if there is no feasible path to move Returns: (ID, ID) \u2013 (The new ID of the moved statement, The out-most newly introduced statments including the added loops) freetensor . core . schedule . Schedule . parallelize ( self , loop , parallel ) \u00b6 Mark a loop with a parallel implementation This schedule follows a fork-join model: multiple workers (abstract threads) are created (but physically the threads may be cached in a thread pool) when the loop begins, do their jobs in parallel, and join when the loop ends OpenMP threads follow a typical fork-join model. CUDA threads run in a bulk-synchronous parallel (BSP) model, which can also be mimiked by the fork-join model: All threads start when the kernel get launched, but they only begin to do their jobs when the parallel loop begins. Nevertheless, the fork-join model needs the following extension to fully mimic a BSP model: Taking CUDA as an example, we allow binding a loop to threadIdx.x inside another loop bound to threadIdx.x , which is illegal in a classic fork-join model. For example, we may implement a matmul with collaborative fetch as below: for i : threadIdx.x # Li for j : threadIdx.y # Lj local_sum = 0 # In gpu/local memory, unique to (i, j) for k0 # Lk0 for k : threadIdx.y # Lk1_a A_cache[k] = A[i, k] # In gpu/shared, shared by different j for k : threadIdx.x # Lk1_b B_cache[k] = B[k, j] # In gpu/shared, shared by different i for k # Lk1_c sum += A_cache[k] * B_cache[k] C[i, j] = local_sum A seemingly plausible solution to avoid this extension is to reorder Lk0 to outer-most, and then move Lk1_a and Lk1_b out of Li or Lj . This resolves the nested threadIdx.x and threadIdx.y binding problem by running Li+Lk1_a , Lj+Lk1_b and Li+Lj interleavingly, instead of running Lk1_a and Lk1_b inside Li+Lj . However, this approach is illegal, because the local variable local_sum can no longer be kept inside the body of Li and Lj : It has to be reused across multiple runs of Li and Lj Please also note that we can bind one threadIdx.x to two loops only when the body statement is loop-invariant to one of them. For example, the following binding is still illegal, even in our extended fork-join model, because it violates its serial semantics: for i : threadIdx.x for j : threadIdx.x A[i, j] ++ Parameters: loop ( str, ID or Stmt ) \u2013 The loop parallel ( ParallelScope ) \u2013 Parallel scope allow_reduction ( bool ) \u2013 If false, raise InvalidSchedule if this schedule would introduce a parallel reduction Exceptions: InvalidSchedule \u2013 if the loop is not found or unable to be parallelized freetensor . core . schedule . Schedule . permute ( self , loops , transform_func ) \u00b6 Permute perfectly nested loops (directly nested loops without statements in between) with the given loop space transformation function The transformed loops follow ascending lexical order of the transformed terms returned by transformFunc when called with original iteration Parameters: loops ( array like of str, ID or Stmt ) \u2013 the list of perfectly nested loops to be permuted transform_func ( Callable[[Expr], Expr] ) \u2013 the loop space transformation function, should be bijective Returns: list of ID \u2013 the list of IDs of permuted loops freetensor . core . schedule . Schedule . pluto_fuse ( self , loop0 , loop1 , nest_level_0 = 0 , nest_level_1 = 0 , fusable_overlap_threshold = 1 , fusable_nonoverlap_tolerance = 4 , do_simplify = True ) \u00b6 Use Pluto+ algorithm to permute and fuse two loops, with as most parallelizable loops as possible at outermost levels. The two loops are required to be consequent; all directly nested levels are detected and subject to permutation. Remaining levels that cannot be fused are left inside the fused loops as two statements Parameters: loop0 ( str, ID or Stmt ) \u2013 The first loop to fuse loop1 ( str, ID or Stmt ) \u2013 The second loop to fuse nest_level_0 ( int ) \u2013 The number of nesting levels of loop 0 to be considered, defaults to maximum possible nest_level_1 ( int ) \u2013 The number of nesting levels of loop 1 to be considered, defaults to maximum possible fusable_overlap_threshold ( int ) \u2013 The minimum overlapping size of two loops to be regarded fusable. Defaults to 1 fusable_nonoverlap_tolerance ( int ) \u2013 The maximum non-overlapping size at either side of two loops to be regarded fusable. Defaults to 4 do_simplify ( bool ) \u2013 Whether the result is simplified by the way, defaults to true Exceptions: InvalidSchedule \u2013 if the loops are not consequent Returns: (ID, int) \u2013 The ID of fused loop and level of parallelizable loops freetensor . core . schedule . Schedule . pluto_permute ( self , loop , nest_level = 0 , do_simplify = True ) \u00b6 Use Pluto+ algorithm to permute a single loop, with as most parallelizable loops as possible at outermost levels. Parameters: loop ( str, ID or Stmt ) \u2013 The loop to permute nest_level ( int ) \u2013 The number of nesting levels to be considered, defaults to maximum possible do_simplify ( bool ) \u2013 Whether the result is simplified by the way, defaults to true Returns: (ID, int) \u2013 The ID of permuted loop and level of parallelizable loops freetensor . core . schedule . Schedule . reorder ( self , order , mode =< ReorderMode . PerfectOnly : 0 > ) \u00b6 Reorder directly nested loops To swap consecutive loops, use swap instead Parameters: order ( array like of str, ID or Stmt ) \u2013 Vector of loops. The requested order of the loops mode ( ReorderMode ) \u2013 How to deal with imperfectly nested loops. PerfectOnly => raise an exception. MoveOutImperfect => do fission in advance to move out statements between the loops, which may enlarge intermediate tensors. MoveInImperfect => move statements between the loops inwards after adding gurads them them, which may hurt parallelism Exceptions: InvalidSchedule \u2013 if the input is invalid or there are breaking dependences freetensor . core . schedule . Schedule . separate_tail ( self , noDuplicateVarDefs = False ) \u00b6 Seperate main iterations and tail iterations of a loop E.g. for i = 0 -> 3 { for j = 0 -> 4 { if (i * 4 + j < 10) { ... } } } Each loop will be separated into 2 parts: the body and the tail. After simplification, the program will finally be transformed to for i = 0 -> 2 { for j = 0 -> 4 { ... } } for j = 0 -> 2 { ... } Ideally, all programs can benefit from this schedule. However, this schedule may greatly increase the program size and make the compiling time way too long. Therefore, this transformation is implemented as a schedule, which can be applied optionally. (TODO: Optionally apply this schedule to part of the program) Parameters: noDuplicateVarDefs ( bool ) \u2013 If there is two VarDef nodes in two branches, it may result in doubled memory use, since different thread may go to different branch. Set this parameter to true to stop duplicating VarDef nodes. freetensor . core . schedule . Schedule . set_mem_type ( self , vardef , mtype ) \u00b6 Change where a variable is stored Parameters: vardef ( str, ID or Stmt ) \u2013 ID of the VarDef statement of the specific variable mtype ( MemType ) \u2013 Where the variable should be stored rejectIndirectAccess ( bool ) \u2013 Registers usually do not support indirect access. If a variable is accessed indirectly, setting it to use registers is meaningless even successful. If this parameter is set to true, throw an exception if the variable being set is accessed indirectly. Specifically, two types of access are considered indirect: 1) The index is a load from another variable, or 2) The index is a loop iterator and the loop has a dynamic length (which can not be unrolled by a backend compiler). By default, this parameter is determined automatically by mtype . Exceptions: InvalidSchedule \u2013 if the variable is not found, or if rejecting an indirect access freetensor . core . schedule . Schedule . split ( self , node , factor =- 1 , nparts =- 1 , shift = 0 ) \u00b6 Split a loop into two nested loops To fission a loop into two consecutive loops, use fission instead Two modes are provided: Specify factor and leave nparts to -1. It will result in an outer loop with length ceil(n / factor) , and an inner loop with length factor , where n is the original loop length added by shift . The original iterator i will be transformed to i0 * factor + i1 , where i0 and i1 are the iterators of the new outer and inner loops, respectively Specify nparts and leave factor to -1. It will result in an outer loop with length nparts , and an inner loop with length ceil(n / nparts) , where n is the original loop length added by shift . The original iterator i will be transformed to i0 * ceil(n / nparts) + i1 , where i0 and i1 are the iterators of the new outer and inner loops, respectively Please note that the second mode will introduce an i0 * ceil(n / nparts) factor into the program, which cannot be recognized by polyhedral analysis, which may hinder some following schedules. If possible, plese use the first mode, and then reorder the inner and outer loops Suppose the original loop is labeled \"L\", the split two loops can be selected by \" \\(split.0{L}\" (the outer loop) and \"\\) split.1{L}\" (the inner loop). If one of the resulting loop is proved to have only a single iteration, it will be removed Parameters: node ( str, ID or Stmt ) \u2013 The loop to be split factor ( int ) \u2013 Length of the inner loop. Set to -1 if using nparts nparts ( int ) \u2013 Length of the outer loop. Set to -1 if using factor Exceptions: InvalidSchedule \u2013 if the loop is not found Returns: (Optional[ID], Optional[ID]) \u2013 (outer loop ID, inner loop ID), either ID can be None if the loop is proved to have only a single iteration freetensor . core . schedule . Schedule . swap ( self , order ) \u00b6 Swap statements in the same block To reorder nested loops, use reorder instead Parameters: order ( List[str (Selector string), ID, List[ID], Stmt, or List[Stmt]] ) \u2013 The statements. If one item of the order list contains multiple statements, the order list will be flattened Exceptions: InvalidSchedule \u2013 if the statements are not found or the dependences cannot be solved freetensor . core . schedule . Schedule . unroll ( self , loop , immediate = False ) \u00b6 Unroll a loop Parameters: loop ( str, ID or Stmt ) \u2013 ID of the loop immediate ( bool ) \u2013 If false (by default), postpone the unroll procedure to the backend compiler, which saves scheduling time. If true, unroll the loop immediately, which may help further simplifications based on the unrolled result. If your purpose is just to fill the instruction cache, set it to false. If you are unrolling a loop that computes array indices, set it to true Exceptions: InvalidSchedule \u2013 if the loop is not found or length of the loop is not a constant freetensor . core . schedule . Schedule . var_merge ( self , vardef , dim ) \u00b6 Merge two dimensions of a variable Parameters: vardef ( str, ID or Stmt ) \u2013 ID of the VarDef statement of the specific variable dim ( int ) \u2013 Merge the dim -th and the (dim + 1) -th dimension freetensor . core . schedule . Schedule . var_reorder ( self , vardef , order ) \u00b6 Reorder the dimensions of a variable Parameters: vardef ( str, ID or Stmt ) \u2013 ID of the VarDef statement of the specific variable order ( array like of str, ID or Stmt ) \u2013 Vector of integers. The new order of the dimensions Exceptions: InvalidSchedule \u2013 if the variable or the order is illegal freetensor . core . schedule . Schedule . var_split ( self , vardef , dim , mode , factor =- 1 , nparts =- 1 ) \u00b6 Split a dimension of a variable into two Parameters: vardef ( str, ID or Stmt ) \u2013 ID of the VarDef statement of the specific variable dim ( int ) \u2013 which dimension to be split mode ( VarSplitMode ) \u2013 When the dimension to split is not divisible by factor or nparts , the resulting shape may become larger. In FixedSize mode, the actual buffer size will not be changed, and gurads will be added to prevent out-of-bound accesses. In RelaxedSize mode, the buffer size may increase. The RelaxedSize mode cannot be applied to I/O variables factor ( int ) \u2013 Length of the inner (higher no.) dimension. Set to -1 if using nparts nparts ( int ) \u2013 Length of the outer (lower no.) loop. Set to -1 if using factor Exceptions: InvalidSchedule \u2013 if the variable or the dimension is not found freetensor . core . schedule . Schedule . vectorize ( self , loop ) \u00b6 Vectorize a loop Please note that, as vectorization is different from architecture to achitecture, the scheduler may or may not postpone it to the backend compiler. The vectorization is a best-effort schedule Parameters: loop ( str, ID or Stmt ) \u2013 ID of the loop Exceptions: InvalidSchedule \u2013 if the ID or name is not found, or the dependence requirement is not met freetensor . core . schedule . schedule ( ast = None , callback = None , backward_callback = None , jit_cache =< function cache at 0x14a9900c19d0 > , verbose = 0 ) \u00b6 Apply any schedule on an AST through a user callback Parameters: ast ( Func or Stmt ) \u2013 The AST to schedule. If not specified, a partial function will be returned that cna be used as a decorator callback ( Callable[[freetensor.core.schedule.Schedule], NoneType] ) \u2013 Specify what schedule(s) to do in this callback backward_callback ( Callable[[freetensor.core.schedule.Schedule], NoneType] ) \u2013 Specify what schedule(s) to do for the backward function, if ast is returned from AD with attach_backward=True . Defaults to be the same with callback jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances verbose ( int ) \u2013 0 = print nothing. 1 = print the final AST. 2 = print an AST after each schedule Returns: Func or JITTemplate \u2013 Return a Func for an AST if there is no JIT parameters. Return a JITTemplate that generates a Func if there is at least one freetensor.core.staging \u00b6 A staging framework to support the FreeTensor frontend. freetensor.core.staging.AllowShortcutScope dataclass \u00b6 Allow return scope. This is a context manager that allows return in statically deterministic control flow. freetensor.core.staging.BreakException ( Exception ) \u00b6 Exception to be raised by StagingOverload.break_stmt. Breaks from a for loop. freetensor.core.staging.ContinueException ( Exception ) \u00b6 Exception to be raised by StagingOverload.continue_stmt. Continues a for loop. freetensor.core.staging.ReplaceAnnotations ( NodeTransformer ) \u00b6 freetensor . core . staging . ReplaceAnnotations . generic_visit ( self , node ) inherited \u00b6 Called if no explicit visitor function exists for a node. freetensor . core . staging . ReplaceAnnotations . visit ( self , node ) inherited \u00b6 Visit a node. freetensor.core.staging.ReturnException ( Exception ) \u00b6 Exception to be raised by StagingOverload.return_stmt. Holds a return value that will be passed through to the function wrapper. freetensor.core.staging.StagedAssignable ( ABC ) \u00b6 freetensor.core.staging.StagedAssignable.__class__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). freetensor . core . staging . StagedAssignable . __class__ . __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). freetensor . core . staging . StagedAssignable . __class__ . __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. freetensor . core . staging . StagedAssignable . __class__ . __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). freetensor . core . staging . StagedAssignable . __class__ . register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. freetensor.core.staging.StagedPredicate ( ABC ) \u00b6 freetensor.core.staging.StagedPredicate.__class__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). freetensor . core . staging . StagedPredicate . __class__ . __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). freetensor . core . staging . StagedPredicate . __class__ . __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. freetensor . core . staging . StagedPredicate . __class__ . __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). freetensor . core . staging . StagedPredicate . __class__ . register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. freetensor.core.staging.StagedTypeAnnotation \u00b6 freetensor.core.staging.StagedTypeAnnotation.__class__ ( ABCMeta ) inherited \u00b6 freetensor.core.staging.StagedTypeAnnotation.__class__.__base__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). freetensor . core . staging . StagedTypeAnnotation . __class__ . __base__ . __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). freetensor . core . staging . StagedTypeAnnotation . __class__ . __base__ . __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. freetensor . core . staging . StagedTypeAnnotation . __class__ . __base__ . __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). freetensor . core . staging . StagedTypeAnnotation . __class__ . __base__ . register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. freetensor . core . staging . StagedTypeAnnotation . __class__ . register ( cls , subclass ) inherited \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. freetensor.core.staging.StagedTypeAnnotationMeta ( ABCMeta ) \u00b6 freetensor.core.staging.StagedTypeAnnotationMeta.__base__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). freetensor . core . staging . StagedTypeAnnotationMeta . __base__ . __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). freetensor . core . staging . StagedTypeAnnotationMeta . __base__ . __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. freetensor . core . staging . StagedTypeAnnotationMeta . __base__ . __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). freetensor . core . staging . StagedTypeAnnotationMeta . __base__ . register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. freetensor . core . staging . StagedTypeAnnotationMeta . register ( cls , subclass ) inherited \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. freetensor.core.staging.StagedUnpackAssignable ( ABC ) \u00b6 freetensor.core.staging.StagedUnpackAssignable.__class__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). freetensor . core . staging . StagedUnpackAssignable . __class__ . __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). freetensor . core . staging . StagedUnpackAssignable . __class__ . __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. freetensor . core . staging . StagedUnpackAssignable . __class__ . __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). freetensor . core . staging . StagedUnpackAssignable . __class__ . register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. freetensor.core.staging.StagingError ( Exception ) \u00b6 Error occurred during staging function execution (i.e. IR tree generation). freetensor.core.staging.StagingOverload \u00b6 freetensor . core . staging . StagingOverload . allow_shortcut_scope ( self , allow ) \u00b6 Opens a scope that allows shortcut control flows in a statically deterministic context. Need to be closed by with statement. freetensor . core . staging . StagingOverload . assert_stmt ( self , test ) \u00b6 Assert staging tool. freetensor . core . staging . StagingOverload . assign_stmt ( self , name , value ) \u00b6 Customized assign wrapper. If value is instance of StagedAssignable , it's regarded as a customized assign behavior and gets executed with the assigned target variable name. This wrapper is used for initializing a variable. freetensor . core . staging . StagingOverload . break_stmt ( self ) \u00b6 Break staging tool. Only allow break in static control flow. freetensor . core . staging . StagingOverload . continue_stmt ( self ) \u00b6 Continue staging tool. Only allow continue in static control flow. freetensor . core . staging . StagingOverload . custom_attr ( self , obj , attr ) \u00b6 Customized attribute accessor. The framework first looks for a Python native attribute. If not found, it looks for this overloaded custom attribute resolver. The default implementation provides no custom attribute. Can be overridden by subclasses. Parameters: obj ( Any ) \u2013 Object to access attribute. attr ( str ) \u2013 Attribute name. Returns: Any \u2013 The attribute value. freetensor . core . staging . StagingOverload . foreach ( self , names , iter , body ) \u00b6 Customized foreach wrapper. If value is instance of StagedIterable , its regarded as a customized foreach behavior and used to generate code for the python for loop. Otherwise, we try to execute the loop as usual. freetensor . core . staging . StagingOverload . functiondef_wrapper ( self , filename , func ) \u00b6 Function definition wrapper. This wrapper performs extra initialization and cleanup for function definition. freetensor . core . staging . StagingOverload . if_then_else_expr ( self , predicate , then_expr , else_expr ) \u00b6 If-then-else expression staging tool. freetensor . core . staging . StagingOverload . if_then_else_stmt ( self , predicate , then_body , else_body = None ) \u00b6 If-then-else statement staging tool. When predicate is deterministic in staging, only one branch is generated. Otherwise, a If node in IR is generated. freetensor . core . staging . StagingOverload . load_attr ( self , obj , attr ) \u00b6 Load attribute staging tool. Allows customization of reading attributes. freetensor . core . staging . StagingOverload . mark_position ( self , lineno ) \u00b6 Code position handler. Defaults to a no-op. Can be overridden by subclasses. Parameters: lineno ( int ) \u2013 Line number of the next statement. freetensor . core . staging . StagingOverload . metadata ( self , content ) \u00b6 Metadata handler. A metadata line is a comment starting with #! and followed by a metadata, represented as a string parameter. Defaults to a no-op. Can be overridden by subclasses. Parameters: content ( str ) \u2013 The metadata content. freetensor . core . staging . StagingOverload . return_stmt ( self , value , funcname ) \u00b6 Return staging tool. Only allow return in static control flow. freetensor . core . staging . StagingOverload . unpack_assign_stmt ( self , names , values ) \u00b6 Customized assign wrapper for one or more targets. If values is instance of StagedUnpackAssignable , it's regarded as a customized assign behavior and gets executed with all the assigned targets' names. Otherwise, it calls assign_stmt with each sub-assignments. Please note that names can be nested tuples like (\"a\", (\"b\", \"c\")) . Please also note that names can also be a single string like \"a\" even if values is a tuple. There is no unpacking in this case freetensor . core . staging . StagingOverload . while_stmt ( self , fpred , body ) \u00b6 While statement staging tool. freetensor.core.staging.TransformError ( Exception ) \u00b6 Error occurred during AST transforming from python function to staging function that generates IR tree. freetensor.core.staging.Transformer ( NodeTransformer ) dataclass \u00b6 Transformer(filename: 'str', base_lineno: 'int', curr_func: 'str' = None, nonlocals: 'List[List[str]]' = None) freetensor . core . staging . Transformer . generic_visit ( self , node ) inherited \u00b6 Called if no explicit visitor function exists for a node. freetensor . core . staging . Transformer . visit ( self , node ) \u00b6 Visit a node. freetensor . core . staging . Transformer . visit_AnnAssign ( self , old_node ) \u00b6 Rule: x: Ty -> freetensor__annotate__x = annotate_stmt('x', Ty) if freetensor__annotate__x: x = freetensor__annotate__x : pure annotation freetensor . core . staging . Transformer . visit_Assign ( self , old_node ) \u00b6 Rule: lhs = rhs -> lhs = unpack_assign_stmt('lhs', rhs) x.lhs = rhs -> x.lhs = unpack_assign_stmt('lhs', rhs) a, (b, c) = (x, (y, z)) -> a, (b, c) = unpack_assign_stmt(('a', ('b', 'c')), (x, (y, z))) a = b = c -> a = unpack_assign_stmt('a', c); b = unpack_assign_stmt('b', c) If unpack_assign_stmt is not overloaded, assign_stmt will be called for each item freetensor . core . staging . Transformer . visit_Compare ( self , old_node ) \u00b6 Expand multiple comparison into and expression. freetensor . core . staging . Transformer . visit_For ( self , old_node ) \u00b6 Rule: for x in iter: body -> def for_body(x): body foreach('x', iter, for_body) freetensor . core . staging . Transformer . visit_If ( self , old_node ) \u00b6 Rule: if pred: body else: orelse -> def then_body(): body def else_body(): orelse if_then_else_stmt(pred, then_body, else_body) freetensor . core . staging . Transformer . visit_IfExp ( self , old_node ) \u00b6 Rule: body if test else orelse -> if_then_else_expr(test, body, orelse) freetensor . core . staging . Transformer . visit_While ( self , old_node ) \u00b6 Rule: while pred: body -> def while_body(): body while_stmt(lambda: pred, while_body) freetensor . core . staging . call_helper ( callee , * args , ** kwargs ) \u00b6 Call helper that generates a python AST Call node with given callee (overload member) and arguments AST node. freetensor . core . staging . function_helper ( name , args , body , nonlocals ) \u00b6 Function helper that generates a python AST FunctionDef node with given name, arguments name, and body. freetensor.core.stmt \u00b6 Facility to build AST statements Classes and functions in this module are internally used by transformer to construct ASTs. They are also used by some internal tests. API of these classes and functions are subject to changes. End users are encouraged to use transformer , instead of this module. Classes and functions in this module are all in BigCamel naming style, to distinguish from expressions in expr.py freetensor.core.stmt.Assert \u00b6 Scope used to create an Assert node This scope is internally used by transformer and tests E.g.: with Assert(i > 0): ... # Assertion body freetensor.core.stmt.Else \u00b6 Scope used to create an else branch of an If node This scope is internally used by transformer and tests E.g.: with If(i > 0): ... # True branch with Else(): ... # Else branch freetensor.core.stmt.For \u00b6 Scope used to create a For node This scope is internally used by transformer and tests E.g.: with For('i', 0, n) as i: ... # Loop body freetensor.core.stmt.If \u00b6 Scope used to create an If node This scope is internally used by transformer and tests E.g.: with If(i > 0): ... # Branch body freetensor.core.stmt.Invoke \u00b6 Inlined invocation of another AST Invoke is used as a scope ( with Invoke(...) as returned_vars ), so that variables returned by the callee can be used in the socpe Invoke can be used for invoking a gradient function, which has already been lowered as an AST. Please note that once a user function has been lowered as an AST, the dimensionalities of its tensors get fixed. Therefore, to invoke ordinary user functions, please use inline in transformer instead, which supports generic types freetensor.core.stmt.NamedScope \u00b6 Scope used to create an StmtSeq node with an explicit labels E.g.: with NamedScope(): ... # body This scope is used for testing only. StmtSeq nodes can be deleted in many lowering passes freetensor.core.stmt.UserGradStaged \u00b6 Internal staged implementation of UserGrad freetensor . core . stmt . Any () \u00b6 Create an Any node (only for testing) Any nodes matches any statement nodes in ast.match freetensor . core . stmt . Eval ( expr ) \u00b6 Create an Eval node This scope is internally used by transformer and tests freetensor . core . stmt . MarkLabel ( label ) \u00b6 Mark the ID of the following statement This scope is internally used by transformer and tests freetensor . core . stmt . MarkVersion ( tape_name , var ) \u00b6 Create an MarkVersion node (only for custom gradient) This node is only used for custom gradient. See UserGrad . freetensor . core . stmt . VarDef ( * args , ** kvs ) \u00b6 A factory function that creates a VarDef or a series of nested VarDef s This scope is internally used by transformer and tests freetensor.core.transform \u00b6 freetensor . core . transform . inline ( func = None , src = None , fallback = None , default_dynamic_range = True , verbose = False ) \u00b6 Enable a user function to be called by a transformed function at run time Parameters: func ( Python function ) \u2013 The user function src ( str (Optional) ) \u2013 The source code of func . This parameter is only required if the source code cannot be get automatically, e.g., if func is generated from an exec default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( bool ) \u2013 True to print the generated Python code that is used for transforming freetensor . core . transform . transform ( func = None , default_dynamic_range = True , bind = {}, jit_cache =< function cache at 0x14a9900c19d0 > , target = None , verbose = 0 ) \u00b6 Transform a user function to an AST Parameters: func ( Python function ) \u2013 The user function to transform. If not specified, a partial function will be returend, which can be used as a decorator default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True bind ( Mapping[str, Any] ) \u2013 Bind some parameters to specific values before transformations. Accpeting a parameter-name-to-value dict. jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances target ( Target ) \u2013 If not None, set config.default_target when transforming. This affects the default memory type use to create variables from Var , empty and etc. verbose ( int ) \u2013 0 = print nothing. 1 = print the resulting AST. 2 = 1 + print the generated Python code that is used for transforming Returns: Func or JITTemplate \u2013 Return a Func for an AST if there is no JIT parameters. Return a JITTemplate that generates a Func if there is at least one freetensor.core.utils \u00b6 freetensor . core . utils . as_decorator ( f ) \u00b6 Enable a multi-parameter function f to be used as a decorator Suppose g = as_decorator(f) , enable the following usages: @g def h(...): ... @g(a=a, b=b, c=c) def h(...): ... Formally, g will have the same parameters as f . f 's first parameter should be the function it decorate, say h , and may have other parameters with default values. If h is set when called, g will return the decorated function, just as f does. If h is not set, g will return an f 's partial function with all other parameters set, and the partial function can then be decorate another h again. freetensor.libop special \u00b6 freetensor.libop.assign \u00b6 freetensor . libop . assign . add_to ( * _args , ** _kvs ) \u00b6 (Broadcasted) add to a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor freetensor . libop . assign . assign ( * _args , ** _kvs ) \u00b6 (Broadcasted) assign to a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor freetensor . libop . assign . floordiv_to ( * _args , ** _kvs ) \u00b6 (Broadcasted) rounding-towards-negative-infinity integer division (following Python convention, but not C) from a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor freetensor . libop . assign . mod_to ( * _args , ** _kvs ) \u00b6 (Broadcasted) modulo (results are non-negative, following Python convention, but not C) from a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor freetensor . libop . assign . mul_to ( * _args , ** _kvs ) \u00b6 (Broadcasted) multiply to a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor freetensor . libop . assign . sub_to ( * _args , ** _kvs ) \u00b6 (Broadcasted) subtract from a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor freetensor . libop . assign . truediv_to ( * _args , ** _kvs ) \u00b6 (Broadcasted) floating-point division from a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor freetensor.libop.concat \u00b6 freetensor . libop . concat . concat ( inputs , axis = 0 ) \u00b6 Concatenate a list of tensors into a single tensor on an existing axis (out-of-place) All input tensors must have the same shape, except for the dimension size of the axis to concatenate on. All input tensors must have the same data type and memory type. Parameters: inputs ( Sequence[freetensor.core.expr.VarRef] ) \u2013 Tensors for concatenation axis ( int ) \u2013 Dimension number for concatenation. Negative axis means counting from the last dimension Returns: VarRef \u2013 Concatenation result freetensor . libop . concat . concat_ ( inputs , output , axis = 0 ) \u00b6 Concatenate a list of tensors into a single tensor on an existing axis (in-place) All input tensors must have the same shape, except for the dimension size of the axis to concatenate on. All input tensors must have the same data type and memory type. Parameters: inputs ( Sequence[freetensor.core.expr.VarRef] ) \u2013 Tensors for concatenation output ( VarRef ) \u2013 Concatenation result axis ( int ) \u2013 Dimension number for concatenation. Negative axis means counting from the last dimension freetensor . libop . concat . stack ( inputs , axis = 0 ) \u00b6 Concatenate a list of tensors into a single tensor on a new axis (out-of-place) All input tensors must have the same shape, data type and memory type. Parameters: inputs ( Sequence[freetensor.core.expr.VarRef] ) \u2013 Tensors for concatenation axis ( int ) \u2013 Dimension number for concatenation. Negative axis means counting from the last dimension Returns: VarRef \u2013 Concatenation result freetensor . libop . concat . stack_ ( inputs , output , axis = 0 ) \u00b6 Concatenate a list of tensors into a single tensor on a new axis (in-place) All input tensors must have the same shape, data type and memory type. Parameters: inputs ( Sequence[freetensor.core.expr.VarRef] ) \u2013 Tensors for concatenation output ( VarRef ) \u2013 Concatenation result axis ( int ) \u2013 Dimension number for concatenation. Negative axis means counting from the last dimension freetensor.libop.constant \u00b6 freetensor . libop . constant . ones ( shape , dtype , mtype = None ) \u00b6 Create a one-valued tensor Parameters: shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used Returns: The one-valued tensor freetensor . libop . constant . ones_ ( y ) \u00b6 Fill ones to a tensor Parameters: y ( VarRef ) \u2013 The tensor to fill freetensor . libop . constant . zeros ( shape , dtype , mtype = None ) \u00b6 Create a zero tensor Parameters: shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used Returns: The zero tensor freetensor . libop . constant . zeros_ ( y ) \u00b6 Fill zeros to a tensor Parameters: y ( VarRef ) \u2013 The tensor to fill freetensor.libop.conv \u00b6 freetensor . libop . conv . conv ( X , W , B = None , auto_pad = 'NOTSET' , dilations = None , group = 1 , kernel_shape = None , pads = None , strides = None ) \u00b6 Convolution. The result is returned Parameters follow ONNX convention. Currently only 2-D convolution is supported freetensor . libop . conv . conv_ ( X , W , B , Y , auto_pad = 'NOTSET' , dilations = None , group = 1 , kernel_shape = None , pads = None , strides = None ) \u00b6 Convolution. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D convolution is supported freetensor.libop.element_wise \u00b6 freetensor . libop . element_wise . abs ( * _args , ** _kvs ) \u00b6 Element-wise absolute value of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . abs_ ( * _args , ** _kvs ) \u00b6 Element-wise absolute value of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . add ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise addition of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . add_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise addition of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . binary_op ( op , a , b ) \u00b6 (Broadcasted) any element-wise operation on two tensors and return the result Parameters: op ( Callable ) \u2013 The operation applied to each item a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . binary_op_ ( op , a , b , out ) \u00b6 (Broadcasted) any element-wise operation on two tensors. The result is written to another tensor Parameters: op ( Callable ) \u2013 The operation applied to each item a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . ceil ( * _args , ** _kvs ) \u00b6 Element-wise ceil of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . ceil_ ( * _args , ** _kvs ) \u00b6 Element-wise ceil of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . ceildiv ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise rounding-towards-positive-infinity integer division of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . ceildiv_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise rounding-towards-positive-infinity integer division of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . cos ( * _args , ** _kvs ) \u00b6 Element-wise cos of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . cos_ ( * _args , ** _kvs ) \u00b6 Element-wise cos of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . eq ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise equal of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . eq_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise equal of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . exp ( * _args , ** _kvs ) \u00b6 Element-wise natrual exponent of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . exp_ ( * _args , ** _kvs ) \u00b6 Element-wise natrual exponent of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . floor ( * _args , ** _kvs ) \u00b6 Element-wise floor of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . floor_ ( * _args , ** _kvs ) \u00b6 Element-wise floor of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . floordiv ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise rounding-towards-negative-infinity integer division (following Python convention, but not C, recommended for performance) of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . floordiv_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise rounding-towards-negative-infinity integer division (following Python convention, but not C, recommended for performance) of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . ge ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise greater-than-or-equal-to of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . ge_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise greater-than-or-equal-to of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . gt ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise greater-than of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . gt_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise greater-than of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . l_and ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise logical and of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . l_and_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise logical and of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . l_not ( * _args , ** _kvs ) \u00b6 Element-wise logical not of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . l_not_ ( * _args , ** _kvs ) \u00b6 Element-wise logical not of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . l_or ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise logical or of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . l_or_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise logical or of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . le ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise less-than-or-equal-to of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . le_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise less-than-or-equal-to of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . ln ( * _args , ** _kvs ) \u00b6 Element-wise natrual logarithm of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . ln_ ( * _args , ** _kvs ) \u00b6 Element-wise natrual logarithm of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . lt ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise less-than of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . lt_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise less-than of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . max ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise maximum of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . max_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise maximum of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . min ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise minimum of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . min_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise minimum of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . mod ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise modulo (results are non-negative, following Python convention, but not C, recommended for performance) of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . mod_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise modulo (results are non-negative, following Python convention, but not C, recommended for performance) of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . mul ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise multiplication of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . mul_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise multiplication of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . ne ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise non-equal of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . ne_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise non-equal of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . neg ( * _args , ** _kvs ) \u00b6 Element-wise negation of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . neg_ ( * _args , ** _kvs ) \u00b6 Element-wise negation of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . relu ( * _args , ** _kvs ) \u00b6 Element-wise ReLU of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . relu_ ( * _args , ** _kvs ) \u00b6 Element-wise ReLU of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . remainder ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise remainder (results can be positive or negative, following C convention, but not Python, NOT recommended for performance) of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . remainder_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise remainder (results can be positive or negative, following C convention, but not Python, NOT recommended for performance) of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . round_towards_0_div ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise rounding-towards-0 integer division (following C convention, but not Python, NOT recommended for performance) of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . round_towards_0_div_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise rounding-towards-0 integer division (following C convention, but not Python, NOT recommended for performance) of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . sigmoid ( * _args , ** _kvs ) \u00b6 Element-wise sigmoid of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . sigmoid_ ( * _args , ** _kvs ) \u00b6 Element-wise sigmoid of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . sin ( * _args , ** _kvs ) \u00b6 Element-wise sin of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . sin_ ( * _args , ** _kvs ) \u00b6 Element-wise sin of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . sqrt ( * _args , ** _kvs ) \u00b6 Element-wise square root of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . sqrt_ ( * _args , ** _kvs ) \u00b6 Element-wise square root of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . square ( * _args , ** _kvs ) \u00b6 Element-wise square of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . square_ ( * _args , ** _kvs ) \u00b6 Element-wise square of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . sub ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise subtraction of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . sub_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise subtraction of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . tan ( * _args , ** _kvs ) \u00b6 Element-wise tan of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . tan_ ( * _args , ** _kvs ) \u00b6 Element-wise tan of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . tanh ( * _args , ** _kvs ) \u00b6 Element-wise tanh of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . tanh_ ( * _args , ** _kvs ) \u00b6 Element-wise tanh of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . truediv ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise floating-point division of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . truediv_ ( * _args , ** _kvs ) \u00b6 (Broadcasted) element-wise floating-point division of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor freetensor . libop . element_wise . unary_op ( op , x ) \u00b6 Any element-wise operation on a tensor and return the result Parameters: op ( Callable ) \u2013 The operation applied to each item x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor freetensor . libop . element_wise . unary_op_ ( op , x , y ) \u00b6 Any element-wise operation on a tensor. The result is written to another tensor Parameters: op ( Callable ) \u2013 The operation applied to each item x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor freetensor.libop.logsumexp \u00b6 freetensor . libop . logsumexp . logsumexp ( x , axis =- 1 , keepdims = True ) \u00b6 Compute ln sum_i exp(x_i) , where i is along an axis. Return the result. The computation is numerically stabilized. Parameters: x ( VarRef ) \u2013 The input tensor axis ( int ) \u2013 Axis that the reduction is performed along. Negative axis means counting from the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor freetensor . libop . logsumexp . logsumexp_ ( x , y , axis =- 1 , keepdims = True ) \u00b6 Compute ln sum_i exp(x_i) , where i is along an axis. Write to tensor y . The computation is numerically stabilized. Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axis ( int ) \u2013 Axis that the reduction is performed along. Negative axis means counting from the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True freetensor.libop.matmul \u00b6 freetensor . libop . matmul . einsum ( fmt , * args ) \u00b6 Einstein summation. The result is returned Parameters: fmt ( str ) \u2013 The format string. E.g. \"ik,kj->ij\" represents a matrix multiplcation args ( Sequence[VarRef] ) \u2013 All inputs arguments. E.g. if fmt is \"ik,kj->ij\" , it iterates axis i and k of args[0] , axis k and j of args[1] , axis i and j of the returned value Returns: The result tensor freetensor . libop . matmul . einsum_ ( fmt , * args ) \u00b6 Einstein summation. The result is written to the last argument Parameters: fmt ( str ) \u2013 The format string. E.g. \"ik,kj->ij\" represents a matrix multiplcation args ( Sequence[VarRef] ) \u2013 All arguments including inputs and the output. E.g. if fmt is \"ik,kj->ij\" , it iterates axis i and k of args[0] , axis k and j of args[1] , axis i and j of args[2] freetensor . libop . matmul . gemm ( A , B , C = None , has_bias = False , trans_A = False , trans_B = False , alpha = 1.0 , beta = 1.0 ) \u00b6 General matrix multiplcation following BLAS convention and return the result It performs Y = alpha tr?(A) @ tr?(B) + C , where @ represents matrix multiplication, tr? represents an optional transposition Parameters: A ( VarRef ) \u2013 The left-hand-side operand of matrix multiplication B ( VarRef ) \u2013 The right-hand-side operand of matrix multiplication C ( VarRef (Optional) ) \u2013 The bias tensor trans_A ( bool ) \u2013 If true, transpose A . Defaults to False trans_B ( bool ) \u2013 If true, transpose B . Defaults to False alpha ( float ) \u2013 Coefficient of tr?(A) @ tr?(B) . Defaults to 1.0 beta ( float ) \u2013 Coefficient of C . Defaults to 1.0 Returns: The resulting tensor freetensor . libop . matmul . gemm_ ( A , B , C , Y , trans_A = False , trans_B = False , alpha = 1.0 , beta = 1.0 ) \u00b6 General matrix multiplcation following BLAS convention. The result is written to an existing tensor It performs Y = alpha tr?(A) @ tr?(B) + C , where @ represents matrix multiplication, tr? represents an optional transposition Parameters: A ( VarRef ) \u2013 The left-hand-side operand of matrix multiplication B ( VarRef ) \u2013 The right-hand-side operand of matrix multiplication C ( VarRef (Optional) ) \u2013 The bias tensor Y ( VarRef ) \u2013 The resulting tensor trans_A ( bool ) \u2013 If true, transpose A . Defaults to False trans_B ( bool ) \u2013 If true, transpose B . Defaults to False alpha ( float ) \u2013 Coefficient of tr?(A) @ tr?(B) . Defaults to 1.0 beta ( float ) \u2013 Coefficient of C . Defaults to 1.0 freetensor . libop . matmul . matmul ( A , B ) \u00b6 Matrix multiplcation. The result is returned Parameters: A ( VarRef ) \u2013 The left-hand-side operand B ( VarRef ) \u2013 The right-hand-side operand Returns: The resulting tensor freetensor . libop . matmul . matmul_ ( A , B , Y ) \u00b6 Matrix multiplcation. The result is written to an existing tensor Parameters: A ( VarRef ) \u2013 The left-hand-side operand B ( VarRef ) \u2013 The right-hand-side operand C ( VarRef ) \u2013 The resulting tensor freetensor.libop.pooling \u00b6 freetensor . libop . pooling . global_avg_pool ( X ) \u00b6 Global averaging pooling. The result is returned Parameters follow ONNX convention. Currently only 2-D pooling is supported freetensor . libop . pooling . global_avg_pool_ ( X , Y ) \u00b6 Global averaging pooling. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D pooling is supported freetensor . libop . pooling . max_pool ( X , auto_pad = 'NOTSET' , dilations = None , kernel_shape = None , pads = None , strides = None ) \u00b6 Maximum pooling. The result is returned Parameters follow ONNX convention. Currently only 2-D pooling is supported freetensor . libop . pooling . max_pool_ ( X , Y , auto_pad = 'NOTSET' , dilations = None , kernel_shape = None , pads = None , strides = None ) \u00b6 Maximum pooling. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D pooling is supported freetensor.libop.reduction \u00b6 freetensor . libop . reduction . all ( * _args , ** _kvs ) \u00b6 Reduction of logical and of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor freetensor . libop . reduction . all_ ( * _args , ** _kvs ) \u00b6 Reduction of logical and of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True freetensor . libop . reduction . any ( * _args , ** _kvs ) \u00b6 Reduction of logical or of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor freetensor . libop . reduction . any_ ( * _args , ** _kvs ) \u00b6 Reduction of logical or of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True freetensor . libop . reduction . reduce_max ( x , axes = None , keepdims = True ) \u00b6 Maximum of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor freetensor . libop . reduction . reduce_max_ ( x , y , axes = None , keepdims = True ) \u00b6 Maximum of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True freetensor . libop . reduction . reduce_min ( x , axes = None , keepdims = True ) \u00b6 Minimum of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor freetensor . libop . reduction . reduce_min_ ( x , y , axes = None , keepdims = True ) \u00b6 Minimum of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True freetensor . libop . reduction . reduce_prod ( * _args , ** _kvs ) \u00b6 Product of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor freetensor . libop . reduction . reduce_prod_ ( * _args , ** _kvs ) \u00b6 Product of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True freetensor . libop . reduction . reduce_sum ( * _args , ** _kvs ) \u00b6 Sum of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor freetensor . libop . reduction . reduce_sum_ ( * _args , ** _kvs ) \u00b6 Sum of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True freetensor.libop.reshape \u00b6 freetensor . libop . reshape . expand ( a , expand_shape ) \u00b6 Broadcast a tensor to a given shape, following the broadcasting rules Parameters: a ( VarRef ) \u2013 The input tensor b ( Sequence of expressions ) \u2013 The broadcasted shape Returns: The broadcasted tensor freetensor . libop . reshape . expand_ ( a , out ) \u00b6 Broadcast a tensor to an existing tensor, following the broadcasting rules Parameters: a ( VarRef ) \u2013 The input tensor b ( VarRef ) \u2013 The broadcasted tensor freetensor . libop . reshape . flatten ( x , axis = 1 ) \u00b6 Flatten a tensor to have two dimensions, and return the result NOTE: This function follows the ONNX convension that reshapes to 2-D instead of 1-D. Parameters: x ( VarRef ) \u2013 The input tensor axis ( int ) \u2013 The result tensor will have 2 dimensions. All dimensions up to axis (inclusive) will be flattend to the first dimension. All dimensions after axis (exclusive) will be flatten to the second dimension. Negative axis means counting form the last dimension Returns: VarRef \u2013 The result tensor freetensor . libop . reshape . flatten_ ( x , y , axis = 1 ) \u00b6 Flatten a tensor to have two dimensions, and write to another tensor NOTE: This function follows the ONNX convension that reshapes to 2-D instead of 1-D. Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axis ( int ) \u2013 The result tensor will have 2 dimensions. All dimensions up to axis (inclusive) will be flattend to the first dimension. All dimensions after axis (exclusive) will be flatten to the second dimension. Negative axis means counting form the last dimension freetensor . libop . reshape . flatten_onnx ( x , axis = 1 ) \u00b6 Flatten a tensor to have two dimensions, and return the result NOTE: This function follows the ONNX convension that reshapes to 2-D instead of 1-D. Parameters: x ( VarRef ) \u2013 The input tensor axis ( int ) \u2013 The result tensor will have 2 dimensions. All dimensions up to axis (inclusive) will be flattend to the first dimension. All dimensions after axis (exclusive) will be flatten to the second dimension. Negative axis means counting form the last dimension Returns: VarRef \u2013 The result tensor freetensor . libop . reshape . flatten_onnx_ ( x , y , axis = 1 ) \u00b6 Flatten a tensor to have two dimensions, and write to another tensor NOTE: This function follows the ONNX convension that reshapes to 2-D instead of 1-D. Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axis ( int ) \u2013 The result tensor will have 2 dimensions. All dimensions up to axis (inclusive) will be flattend to the first dimension. All dimensions after axis (exclusive) will be flatten to the second dimension. Negative axis means counting form the last dimension freetensor . libop . reshape . flatten_pytorch ( x , start_dim = 0 , end_dim =- 1 ) \u00b6 Flatten a tensor to have fewer dimensions, and return the result NOTE: This function follows the PyTorch convension Parameters: x ( VarRef ) \u2013 The input tensor start_dim, end_dim ( int (Optional) ) \u2013 All dimensions ranging from start_dim and end_dim (inclusive) will be flattend to 1-D. Negative axis means counting form the last dimension Returns: VarRef \u2013 The result tensor freetensor . libop . reshape . flatten_pytorch_ ( x , y , start_dim = 0 , end_dim =- 1 ) \u00b6 Flatten a tensor to have fewer dimensions, and write to another tensor NOTE: This function follows the PyTorch convension Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor start_dim, end_dim ( int (Optional) ) \u2013 All dimensions ranging from start_dim and end_dim (inclusive) will be flattend to 1-D. Negative axis means counting form the last dimension freetensor . libop . reshape . reshape ( x , shape ) \u00b6 Reshape a tensor into a different shape with the same size This operator will try to generate nested loops instead of looping over all elements in a plain loop, so schedules can be better applied. It guarantees to generates loops in the following cases: Splitting a dimension. E.g. 4 to 2x2, and there will be a 2x2 loop nest. Merging dimensions. E.g. 2x2 to 4, and there will be a 2x2 loop nest. Each non-affecting dimension will be iterated by a unique loop. E.g. 3x5x7 to 5x3x7, and there will be a 15x7 loop nest, where the \"7\" dimension will be iterated by a unique loop. Parameters: x ( VarRef ) \u2013 The input tensor shape ( list of expression ) \u2013 The target shape Returns: VarRef \u2013 The result tensor freetensor . libop . reshape . reshape_ ( x , y ) \u00b6 Fill a tensor into another tensor with the same size but maybe different shape This operator will try to generate nested loops instead of looping over all elements in a plain loop, so schedules can be better applied. It guarantees to generates loops in the following cases: Splitting a dimension. E.g. 4 to 2x2, and there will be a 2x2 loop nest. Merging dimensions. E.g. 2x2 to 4, and there will be a 2x2 loop nest. Each non-affecting dimension will be iterated by a unique loop. E.g. 3x5x7 to 5x3x7, and there will be a 15x7 loop nest, where the \"7\" dimension will be iterated by a unique loop. Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor freetensor . libop . reshape . squeeze ( x , axes ) \u00b6 Remove singleton dimensions from a tensor, and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] ) \u2013 Dimension numbers of the singleton dimensions. Negative axis means counting from the last dimension Returns: VarRef \u2013 The resulting tensor freetensor . libop . reshape . squeeze_ ( x , y , axes ) \u00b6 Remove singleton dimensions from a tensor, and write the result to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The resulting tensor axes ( Sequence[int] ) \u2013 Dimension numbers of the singleton dimensions. Negative axis means counting from the last dimension freetensor . libop . reshape . unsqueeze ( x , axes ) \u00b6 Insert singleton dimensions to a tensor, and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] ) \u2013 Dimension numbers of the new singleton dimensions. Negative axis means counting from the last dimension Returns: VarRef \u2013 The resulting tensor freetensor . libop . reshape . unsqueeze_ ( x , y , axes ) \u00b6 Insert singleton dimensions to a tensor, and write the result to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The resulting tensor axes ( Sequence[int] ) \u2013 Dimension numbers of the new singleton dimensions. Negative axis means counting from the last dimension freetensor.libop.softmax \u00b6 freetensor . libop . softmax . softmax ( x , axis =- 1 ) \u00b6 Softmax of tensor x along an axis and return the result The computation is numerically stabilized. Parameters: x ( VarRef ) \u2013 The input tensor axis ( int (Optional) ) \u2013 Axis that the softmax is performed along. Negative axis means counting from the last dimension Returns: The result tensor freetensor . libop . softmax . softmax_ ( x , y , axis =- 1 ) \u00b6 Softmax of tensor x along an axis, and write to tensor y The computation is numerically stabilized. Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axis ( int ) \u2013 Axis that the softmax is performed along. Negative axis means counting from the last dimension freetensor.libop.transpose \u00b6 freetensor . libop . transpose . transpose ( x , perm = None ) \u00b6 Transposition (out-of-place) The perm[i] -th dimension of the input becomes the i -th dimension of the output Parameters: x ( VarRef ) \u2013 The input tensor perm ( Sequence[int] ) \u2013 Permutation of the dimensions. Negative values mean counting form the last dimension. By default reversing all dimensions Returns: VarRef \u2013 The output tensor freetensor . libop . transpose . transpose_ ( x , y , perm = None ) \u00b6 Transposition (in-place) The perm[i] -th dimension of the input becomes the i -th dimension of the output Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The output tensor perm ( Sequence[int] ) \u2013 Permutation of the dimensions. Negative values mean counting form the last dimension. By default reversing all dimensions","title":"Python API"},{"location":"api/#freetensor.core","text":"","title":"core"},{"location":"api/#freetensor.core.autograd","text":"","title":"autograd"},{"location":"api/#freetensor.core.autograd.grad","text":"Reverse mode automatic differentiation (out-of-place version) It returns a forward function, a backward function, and two maps on names. The forward function computes the original results. The backward function computes the gradients. The maps map from the names of the original arguments and return values, to the names of their gradients. If tape_in_closure is True (by default), the forward function has the same interface of the original function, but it will store some intermediate tensors (the tape) in some hidden global states. The backward functions receives the same inputs as the original function plus the graidents of the outputs, and also reads from the hidden states. The outputs of the original function are no longer exist in the backward function, and the input-outputs of the original function are converted to pure inputs. As grad is an out-of-place AD interface, the backward function returns the resulting gradients as additional return values. Names of the additional arguments and return values can be looked up in the maps returned by grad . If tape_in_closure is False, global states described above will be passed by explicit arguments and return values, so you can store or manipluate these states between the forward run and the backward run. Parameters: func ( AST ) \u2013 The original function requires ( Sequence[Union[str, freetensor.core.param_ret_dict.Parameter]] ) \u2013 Name of input variables that need gradients. A parameter of a function can also be specified with a Parameter object by position provides ( Sequence[Union[str, freetensor.core.param_ret_dict.Parameter, freetensor.core.param_ret_dict.Return]] ) \u2013 Name of output variables whose gradients are known. A mutable parameter of a function can also be specified with a Parameter object by position. A return value of a function can also be specified with a Return object by position tapes ( Union[Sequence, freetensor_ffi.GradTapeMode, freetensor_ffi.TapeStrategy] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure ( bool ) \u2013 True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True reset_provided_grad ( bool ) \u2013 If true, reset gradients for all variables in provides to 0 after use. This ensures the final result is correct when computing gradients of a program part by part with multiple calls to this function. If false, do not touch the provided gradient, which makes it convenient to run for multiple rounds for timing. invert ( bool ) \u2013 If set to true, it can reduce the amount of recomputation or taping required. However, this may result in a loss of precision for floating-point numbers. Defaults user_grads ( Optional[Sequence[freetensor_ffi.StmtSetToUserGrad]] ) \u2013 For custom gradient. You do not have to explicitly set this parameter unless you are manipulating func by yourself (not getting it from the Python frontend). See UserGrad for details attach_backward ( bool ) \u2013 If True, the forward function will be the only return value, with backward function and other metadata attached to it jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances verbose ( int ) \u2013 Verbosity level Returns: Func if attach_backward else Tuple \u2013 If attach_backward is False, return a tuple. Return[0]: forward AST. Return[1]: backward AST. Return[2]: Mapping from names in requries to its gradient name. Return[3]: Mapping from names in provides to its gradient name. If attach_backward is True, only the forward AST is returned, and others can be get by .backward , .input_name_to_gradient_name and .output_name_to_gradient_name on the return value, respectively.","title":"grad()"},{"location":"api/#freetensor.core.autograd.grad_","text":"Reverse mode automatic differentiation (in-place version) It returns a forward function, a backward function, and two maps on names. The forward function computes the original results. The backward function computes the gradients. The maps map from the names of the original arguments and return values, to the names of their gradients. If tape_in_closure is True (by default), the forward function has the same interface of the original function, but it will store some intermediate tensors (the tape) in some hidden global states. The backward functions receives the same inputs as the original function plus the graidents of the outputs, and also reads from the hidden states. The outputs of the original function are no longer exist in the backward function, and the input-outputs of the original function are converted to pure inputs. As grad_ is an in-place AD interface, the backward function passes the resulting gradients by additional mutable arguments. Names of the additional arguments can be looked up in the maps returned by grad_ . If tape_in_closure is False, global states described above will be passed by explicit arguments and return values, so you can store or manipluate these states between the forward run and the backward run. Parameters: func ( AST ) \u2013 The original function requires ( Sequence[Union[str, freetensor.core.param_ret_dict.Parameter]] ) \u2013 Name of input variables that need gradients. A parameter of a function can also be specified with a Parameter object by position provides ( Sequence[Union[str, freetensor.core.param_ret_dict.Parameter, freetensor.core.param_ret_dict.Return]] ) \u2013 Name of output variables whose gradients are known. A mutable parameter of a function can also be specified with a Parameter object by position. A return value of a function can also be specified with a Return object by position tapes ( Union[Sequence, freetensor_ffi.GradTapeMode, freetensor_ffi.TapeStrategy] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure ( bool ) \u2013 True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True reset_provided_grad ( bool ) \u2013 If true, reset gradients for all variables in provides to 0 after use. This ensures the final result is correct when computing gradients of a program part by part with multiple calls to this function. If false, do not touch the provided gradient, which makes it convenient to run for multiple rounds for timing. invert ( bool ) \u2013 If set to true, it can reduce the amount of recomputation or taping required. However, this may result in a loss of precision for floating-point numbers. Defaults to true. user_grads ( Optional[Sequence[freetensor_ffi.StmtSetToUserGrad]] ) \u2013 For custom gradient. You do not have to explicitly set this parameter unless you are manipulating func by yourself (not getting it from the Python frontend). See UserGrad for details attach_backward ( bool ) \u2013 If True, the forward function will be the only return value, with backward function and other metadata attached to it jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances verbose ( int ) \u2013 Verbosity level Returns: Func if attach_backward else Tuple \u2013 If attach_backward is False, return a tuple. Return[0]: forward AST. Return[1]: backward AST. Return[2]: Mapping from names in requries to its gradient name. Return[3]: Mapping from names in provides to its gradient name. If attach_backward is True, only the forward AST is returned, and others can be get by .backward , .input_name_to_gradient_name and .output_name_to_gradient_name on the return value, respectively.","title":"grad_()"},{"location":"api/#freetensor.core.autograd.grad_body","text":"grad or grad_ on a function body (for internal tests only)","title":"grad_body()"},{"location":"api/#freetensor.core.autograd.jacrev","text":"Compute Jacobian tensors using Reverse mode automatic differentiation (out-of-place) jacrev computes one Jacobian tensor for one output and one or more inputs. Each Jacobian tensor consists of derivatives of all elements in the output tensor w.r.t. all elements in each inputs tensor. It returns a forward function, a backward function, and a map on names. The forward function computes the original results. The backward function computes the Jacobian tensors. The map maps from the names of the original arguments to the names of their Jacobian tensors. By default, the forward function has the same interface of the original function, but it will store some intermediate tensors (the tape) in some hidden global states. The backward functions receives the same inputs as the original function, and also reads from the hidden states. The outputs of the original function are no longer exist in the backward function, and the input-outputs of the original function are converted to pure inputs. As jacrev is an out-of-place interface, the backward function returns the resulting Jacobian as additional return values. Names of the additional return values can be looked up in the map returned by jacrev . Suppose the output's shape is (d1, d2, ...) , and there are two inputs, whose shapes are (e1, e2, ...) and (f1, f2, ...) , respectively. If flatten is False (by default), the Jacobian tensors' shape will be (d1, d2, ..., e1, e2, ...) and (d1, d2, ..., f1, f2, ...) , respectively. If flatten is True, there will be only one Jacbian tensor, whose shape will be (d1 * d2 * ..., e1 * e2 * ... + f1 * f2 * ...) . If tape_in_closure is False, global states described above will be passed by explicit arguments and return values, so you can store or manipluate these states between the forward run and the backward run. Parameters: func ( AST ) \u2013 The original function inputs ( Sequence[Union[str, freetensor.core.param_ret_dict.Parameter]] ) \u2013 Name of input variables that the Jacobian tensors are for. output ( Union[str, freetensor.core.param_ret_dict.Parameter, freetensor.core.param_ret_dict.Return] ) \u2013 Name of one output variables that the Jacobian tensors are for. A return value of a function can be specified with a Return object flatten ( bool ) \u2013 If True, concatenate all Jacobian tensors together, to form an (n, m) -shaped output, where n is the total number of elements in the specified output, and m is the total number of elements in the specified inputs. This requires all involved inputs having the same data type and memory type. In this case, the name of the Jacobian tensor will be \"jacrev.flatten\" , and the returned name map will be empty tapes ( Union[Sequence, freetensor_ffi.GradTapeMode, freetensor_ffi.TapeStrategy] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure ( bool ) \u2013 True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True invert ( bool ) \u2013 If set to true, it can reduce the amount of recomputation or taping required. However, this may result in a loss of precision for floating-point numbers. Defaults user_grads ( Optional[Sequence[freetensor_ffi.StmtSetToUserGrad]] ) \u2013 For custom gradient. You do not have to explicitly set this parameter unless you are manipulating func by yourself (not getting it from the Python frontend). See UserGrad for details attach_backward ( bool ) \u2013 If True, the forward function will be the only return value, with backward function and other metadata attached to it jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances verbose ( int ) \u2013 Verbosity level Returns: Func if attach_backward else Tuple \u2013 If attach_backward is False, return a tuple. Return[0]: forward AST. Return[1]: backward AST. Return[2]: Mapping from names in inputs to its Jacobian tensor name. If attach_backward is True, only the forward AST is returned, and others can be get by .backward and .input_name_to_gradient_name on the return value, respectively.","title":"jacrev()"},{"location":"api/#freetensor.core.autograd.jacrev_","text":"Compute Jacobian tensors using Reverse mode automatic differentiation (in-place) jacrev computes one Jacobian tensor for one output and one or more inputs. Each Jacobian tensor consists of derivatives of all elements in the output tensor w.r.t. all elements in each inputs tensor. It returns a forward function, a backward function, and a map on names. The forward function computes the original results. The backward function computes the Jacobian tensors. The map maps from the names of the original arguments to the names of their Jacobian tensors. By default, the forward function has the same interface of the original function, but it will store some intermediate tensors (the tape) in some hidden global states. The backward functions receives the same inputs as the original function, and also reads from the hidden states. The outputs of the original function are no longer exist in the backward function, and the input-outputs of the original function are converted to pure inputs. As jacrev_ is an in-place interface, the backward function passes the resulting gradients by additional mutable arguments. Names of the additional arguments can be looked up in the map returned by jacrev_ . Suppose the output's shape is (d1, d2, ...) , and there are two inputs, whose shapes are (e1, e2, ...) and (f1, f2, ...) , respectively. If flatten is False (by default), the Jacobian tensors' shape will be (d1, d2, ..., e1, e2, ...) and (d1, d2, ..., f1, f2, ...) , respectively. If flatten is True, there will be only one Jacbian tensor, whose shape will be (d1 * d2 * ..., e1 * e2 * ... + f1 * f2 * ...) . If tape_in_closure is False, global states described above will be passed by explicit arguments and return values, so you can store or manipluate these states between the forward run and the backward run. Parameters: func ( AST ) \u2013 The original function inputs ( Sequence[Union[str, freetensor.core.param_ret_dict.Parameter]] ) \u2013 Name of input variables that the Jacobian tensors are for. A parameter of a function can also be specified with a Parameter object by position output ( Union[str, freetensor.core.param_ret_dict.Parameter, freetensor.core.param_ret_dict.Return] ) \u2013 Name of one output variables that the Jacobian tensors are for. A mutable parameter of a function can also be specified with a Parameter object by position. A return value of a function can also be specified with a Return object by position flatten ( bool ) \u2013 If True, concatenate all Jacobian tensors together, to form an (n, m) -shaped output, where n is the total number of elements in the specified output, and m is the total number of elements in the specified inputs. This requires all involved inputs having the same data type and memory type. In this case, the name of the Jacobian tensor will be \"jacrev.flatten\" , and the returned name map will be empty tapes ( Union[Sequence, freetensor_ffi.GradTapeMode] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history tape_in_closure ( bool ) \u2013 True to pass taped tensors from the forward function to the backward function in implicit I/O parameters, i.e. in closure. False to pass these tensors as explicit I/O parameters. Default to True invert ( bool ) \u2013 If set to true, it can reduce the amount of recomputation or taping required. However, this may result in a loss of precision for floating-point numbers. Defaults user_grads ( Optional[Sequence[freetensor_ffi.StmtSetToUserGrad]] ) \u2013 For custom gradient. You do not have to explicitly set this parameter unless you are manipulating func by yourself (not getting it from the Python frontend). See UserGrad for details attach_backward ( bool ) \u2013 If True, the forward function will be the only return value, with backward function and other metadata attached to it jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances verbose ( int ) \u2013 Verbosity level Returns: Func if attach_backward else Tuple \u2013 If attach_backward is False, return a tuple. Return[0]: forward AST. Return[1]: backward AST. Return[2]: Mapping from names in inputs to its Jacobian tensor name. If attach_backward is True, only the forward AST is returned, and others can be get by .backward and .input_name_to_gradient_name on the return value, respectively.","title":"jacrev_()"},{"location":"api/#freetensor.core.codegen","text":"","title":"codegen"},{"location":"api/#freetensor.core.codegen.NativeCode","text":"Generated native code with metadata NOTE: This class does not support serialization yet. If you need serialization, serialize the Func, and re-run codegen.","title":"NativeCode"},{"location":"api/#freetensor.core.codegen.NativeCode.__contains__","text":"Legacy interface for testing if a string is in the code","title":"__contains__()"},{"location":"api/#freetensor.core.codegen.codegen","text":"Generate native code Parameters: ast ( Func ) \u2013 The AST to be lowered. It must includes function signature to determine parameters and return values. If not specified, a partial function is returned, which can be used as a decorator jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances target ( Optional[freetensor_ffi.Target] ) \u2013 The target architecture. If omitted, use the default one in config Returns: NativeCode \u2013 Return a NativeCode for the generated code if there is no JIT parameters. Return a JITTemplate that generates a NativeCode if there is at least one","title":"codegen()"},{"location":"api/#freetensor.core.config","text":"Global configurations","title":"config"},{"location":"api/#freetensor.core.config.backend_compiler_cxx","text":"backend_compiler_cxx() -> List[str] Backend compiler used to compile generated C++ code","title":"backend_compiler_cxx()"},{"location":"api/#freetensor.core.config.backend_compiler_nvcc","text":"backend_compiler_nvcc() -> List[str] Backend compiler used to compile generated CUDA code","title":"backend_compiler_nvcc()"},{"location":"api/#freetensor.core.config.backend_openmp","text":"backend_openmp() -> List[str] OpenMP library linked to the compiled executable","title":"backend_openmp()"},{"location":"api/#freetensor.core.config.debug_binary","text":"debug_binary() -> bool Check if compiling binary in debug mode","title":"debug_binary()"},{"location":"api/#freetensor.core.config.debug_cuda_with_um","text":"debug_cuda_with_um() -> bool Check if debugging with Unified Memory enabled","title":"debug_cuda_with_um()"},{"location":"api/#freetensor.core.config.default_device","text":"default_device() -> freetensor_ffi.Device Check current default device","title":"default_device()"},{"location":"api/#freetensor.core.config.default_target","text":"default_target() -> freetensor_ffi.Target Check current default target","title":"default_target()"},{"location":"api/#freetensor.core.config.fast_math","text":"fast_math() -> bool Run pass/float_simplify optimization pass, and enable fast math on backend compilers","title":"fast_math()"},{"location":"api/#freetensor.core.config.pretty_print","text":"pretty_print() -> bool Check if colored printing enabled","title":"pretty_print()"},{"location":"api/#freetensor.core.config.print_all_id","text":"print_all_id() -> bool Check if printing IDs of all statements in an AST","title":"print_all_id()"},{"location":"api/#freetensor.core.config.print_source_location","text":"print_source_location() -> bool Check if printing Python source location of all statements in an AST","title":"print_source_location()"},{"location":"api/#freetensor.core.config.set_backend_compiler_cxx","text":"set_backend_compiler_cxx(path: List[str]) -> None Set backend compiler used to compile generated C++ code, unescaped raw path expected","title":"set_backend_compiler_cxx()"},{"location":"api/#freetensor.core.config.set_backend_compiler_nvcc","text":"set_backend_compiler_nvcc(path: List[str]) -> None Set backend compiler used to compile generated CUDA code, unescaped raw path expected","title":"set_backend_compiler_nvcc()"},{"location":"api/#freetensor.core.config.set_backend_openmp","text":"set_backend_openmp(arg0: List[str]) -> None Set the OpenMP library linked to the compiled executable","title":"set_backend_openmp()"},{"location":"api/#freetensor.core.config.set_debug_binary","text":"set_debug_binary(flag: bool = True) -> None Compile with -g at backend. FreeTensor will not delete the binary file after loading it","title":"set_debug_binary()"},{"location":"api/#freetensor.core.config.set_debug_cuda_with_um","text":"set_debug_cuda_with_um(arg0: bool) -> None Allocate CUDA buffers on Unified Memory, for faster (debugging) access of GPU Array from CPU, but with slower Array allocations and more synchronizations. No performance effect on normal in-kernel computations","title":"set_debug_cuda_with_um()"},{"location":"api/#freetensor.core.config.set_default_device","text":"set_default_device(device: freetensor_ffi.Device) -> None Set default device (internal implementation of with Device )","title":"set_default_device()"},{"location":"api/#freetensor.core.config.set_default_target","text":"set_default_target(target: freetensor_ffi.Target) -> None Set default target (internal implementation of with Target )","title":"set_default_target()"},{"location":"api/#freetensor.core.config.set_fast_math","text":"set_fast_math(flag: bool = True) -> None Set to run pass/float_simplify optimization pass, and enable fast math on backend compilers (or not)","title":"set_fast_math()"},{"location":"api/#freetensor.core.config.set_pretty_print","text":"set_pretty_print(flag: bool = True) -> None Set colored printing","title":"set_pretty_print()"},{"location":"api/#freetensor.core.config.set_print_all_id","text":"set_print_all_id(flag: bool = True) -> None Print IDs of all statements in an AST","title":"set_print_all_id()"},{"location":"api/#freetensor.core.config.set_print_source_location","text":"set_print_source_location(flag: bool = True) -> None Print Python source location of all statements in an AST","title":"set_print_source_location()"},{"location":"api/#freetensor.core.config.set_werror","text":"set_werror(flag: bool = True) -> None Error on warning","title":"set_werror()"},{"location":"api/#freetensor.core.config.werror","text":"werror() -> bool Check if error-on-warning enabled","title":"werror()"},{"location":"api/#freetensor.core.config.with_cuda","text":"with_cuda() -> bool Check if FreeTensor is built with CUDA","title":"with_cuda()"},{"location":"api/#freetensor.core.config.with_mkl","text":"with_mkl() -> bool Check if FreeTensor is built with MKL","title":"with_mkl()"},{"location":"api/#freetensor.core.config.with_pytorch","text":"with_pytorch() -> bool Check if FreeTensor is built with PyTorch interface","title":"with_pytorch()"},{"location":"api/#freetensor.core.context","text":"Facility to pick statements to build an AST Classes and functions in this module are internally used by transformer to construct ASTs. They are also used by some internal tests. API of these classes and functions are subject to changes. End users are encouraged to use transformer , instead of this module.","title":"context"},{"location":"api/#freetensor.core.context.ContextStack","text":"","title":"ContextStack"},{"location":"api/#freetensor.core.context.ContextStack.get_last_stmt_id","text":"Can be used inside the staged code, to get the ID of the immediately preceding statement","title":"get_last_stmt_id()"},{"location":"api/#freetensor.core.context.ContextStack.push_append_stmt_callback","text":"Add a callback to be called with all next statements to be appended. For If statement, it can be called twice, one without \"else\" branch, and then maybe one more with \"else\" branch","title":"push_append_stmt_callback()"},{"location":"api/#freetensor.core.context.StmtRange","text":"Record a set of statement in a program, can be used for custom gradient Usage: with StmtRange() as rng: # Some statements StmtRange can be used interleaved with AST scopes. In these cases, you can directly call __enter__ and __exit__ . E.g., rng = StmtRange() rng.__enter__() # Some statements with VarDef(...) # Some scopes # Some other statements rng.__exit__(None, None, None)","title":"StmtRange"},{"location":"api/#freetensor.core.context.pop_ast","text":"Get AST and reset context Internally used by transformer and tests","title":"pop_ast()"},{"location":"api/#freetensor.core.context.pop_ast_and_user_grads","text":"Get AST and reset context. Return an extra list for custom gradients Set UserGrad for details","title":"pop_ast_and_user_grads()"},{"location":"api/#freetensor.core.driver","text":"","title":"driver"},{"location":"api/#freetensor.core.driver.Driver","text":"","title":"Driver"},{"location":"api/#freetensor.core.driver.Driver.__call__","text":"Set argument, execute the binary code, and collect the returns If there is only one return value, it is returned directly. Otherwise, the return values are packed in a ReturnValuesPack This function will introduce some overhaed handling arguments and return values. For an accurate execution time measurement, plase call self.set_args first, then self.time , and finally self.collect_returns","title":"__call__()"},{"location":"api/#freetensor.core.driver.Driver.__init__","text":"Compile a program using a backend compiler and load it into memory This class is for internal use. Please consider using build_binary Parameters: native_code ( NativeCode ) \u2013 Native code generated from codegen device ( Optional[freetensor_ffi.Device] ) \u2013 The device to run the program. If omitted, use the default device in config cxx_flags ( Sequence[str] ) \u2013 Additional C++ flags passed to the backend compiler verbose ( bool ) \u2013 True to print extra infomation","title":"__init__()"},{"location":"api/#freetensor.core.driver.Driver.collect_returns","text":"Collect return values from an invocation Return values must be collect. Otherwise there will be memory leaks If there is only one return value, it is returned directly. Otherwise, or if always_return_pack is set, the return values are packed in a ReturnValuesPack","title":"collect_returns()"},{"location":"api/#freetensor.core.driver.Driver.native_code","text":"Get native code compiled by backend compiler","title":"native_code()"},{"location":"api/#freetensor.core.driver.Driver.set_args","text":"Set argument for an invocation","title":"set_args()"},{"location":"api/#freetensor.core.driver.array","text":"Factory function for Array This function is preferred over directly calling Array 's constructor, because it accepts more data format. If data is another FreeTensor Array , the original object will be returned, with dont_drop_borrow and moved set to new values. If dtype is set and different from the original data type, the Array will be copied first to convert the data type. If data is Numpy Array or PyTorch Tensor , it will be converted to FreeTensor Array . Memory copy will be avoided in most cases, but it is inevitable if the data is strided. If dtype is set and different from the original data type, the Array or Tensor will be copied first to convert the data type. Otherwise, the data will be treated as an n-dimensional array-like object, and will be parsed according the rules in NumPy. The data type is also set accordingly, unless dtype is set. Parameters: data ( FreeTensor Array, Numpy Array, PyTorch Tensor, or other array-like objects ) \u2013 Data to be copied to or borrowed by the new Array object dtype ( ft.DataType or str ) \u2013 If data is not in dtype , convert it to dtype first before constructing the Array dont_drop_borrow ( bool ) \u2013 If true, report an error if we have to drop a borrwed data. This flag is set to true when the Array is cunstructed IMPLICITLY (not by this function) from a user object by borrowing from it, where users may expect they are acutually manipulating the their user object, instead of this Array moved ( bool ) \u2013 If true, it means we do not care about data in this Array any more after the program runs. Variables with \"input-mutable\" access type may modify the Array","title":"array()"},{"location":"api/#freetensor.core.driver.build_binary","text":"Compile a program using a backend compiler and load it into memory Parameters: code ( Optional[freetensor.core.codegen.NativeCode] ) \u2013 Native code generated by codegen . If not specified, a partial function is returned, which can be used as a decorator device ( Optional[freetensor_ffi.Device] ) \u2013 The device to run the program. If omitted, use the default device in config jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances cxx_flags ( Sequence[str] ) \u2013 Additional C++ flags passed to the backend compiler verbose ( bool ) \u2013 Verbosity level Returns: Driver or JITTemplate \u2013 Return a Driver for the executable if there is no JIT parameters. Return a JITTemplate that generates a Driver if there is at least one","title":"build_binary()"},{"location":"api/#freetensor.core.driver.move","text":"Alias for array(data, dont_drop_borrow=False, moved=True)","title":"move()"},{"location":"api/#freetensor.core.enable_attach_backward","text":"","title":"enable_attach_backward"},{"location":"api/#freetensor.core.enable_attach_backward.EnableAttachBackward","text":"Get backward object (Func, Driver, etc) and other meta data from a forward object This class is a Mixin Class. It should be inherited BEFORE other base classes in multiple inheritance.","title":"EnableAttachBackward"},{"location":"api/#freetensor.core.enable_attach_backward.EnableAttachBackward.__init__","text":"Forward all arguments to other base classes In Python, super(). init calls the next base class in the full inheritance graph of the final class, not only base classes of BackwardAttachedMixin. See https://docs.python.org/3/tutorial/classes.html#multiple-inheritance","title":"__init__()"},{"location":"api/#freetensor.core.expr","text":"Facility to build AST expressions Classes and functions in this module are not only used internally for constructing AST nodes, and also exposed to users via multi-stage programming","title":"expr"},{"location":"api/#freetensor.core.expr.AlreadyMadeReduceTo","text":"A single-value type that marks a ReduceTo node is already made, and there is no need to make another Store node In standard Python data model, functions like iadd returns the modified self, and setitem does a self-assignment. We do the augmenting assignment directly in iadd and return AlreadyMadeReduceTo, so we do not have to Store it again","title":"AlreadyMadeReduceTo"},{"location":"api/#freetensor.core.expr.UndeclaredParam","text":"Error type. For error reporting only.","title":"UndeclaredParam"},{"location":"api/#freetensor.core.expr.VarRef","text":"Variable of FreeTensor All variables in FreeTensor DSL (declared via Var , created by empty or var , returned by libop , etc.), and their slices, are VarRef objects. Operations on VarRef objects generates AST nodes","title":"VarRef"},{"location":"api/#freetensor.core.expr.VarRef.as_reduce_to","text":"as_reduce_to(self: freetensor_ffi.FrontendVar, op: freetensor_ffi.ReduceOp, metadata: freetensor_ffi.Metadata, value: freetensor_ffi.Expr, atomic: bool = False) -> freetensor_ffi.Stmt","title":"as_reduce_to()"},{"location":"api/#freetensor.core.expr.VarRef.as_store","text":"as_store(self: freetensor_ffi.FrontendVar, metadata: freetensor_ffi.Metadata, value: freetensor_ffi.Expr) -> freetensor_ffi.Stmt","title":"as_store()"},{"location":"api/#freetensor.core.expr.VarRef.select","text":"Alias for self[..., idx, ...] , where idx is at the dim -th dimension","title":"select()"},{"location":"api/#freetensor.core.expr.VarRef.select_slice","text":"Alias for self[..., begin:end, ...] , where begin:end is at the dim -th dimension","title":"select_slice()"},{"location":"api/#freetensor.core.expr.VarRef.shape","text":"Return lengths of all dimensions or the length of one dimension .shape() -> list of lengths of all dimensions .shape(dim) -> length of dimension dim , where dim can be int or Expr All lengths can be Expr (if the length is dynamically decided) or int (if statically decided)","title":"shape()"},{"location":"api/#freetensor.core.expr.VarRefFromVarDef","text":"VarRef with extra checks","title":"VarRefFromVarDef"},{"location":"api/#freetensor.core.expr.VarRefFromVarDef.as_reduce_to","text":"as_reduce_to(self: freetensor_ffi.FrontendVar, op: freetensor_ffi.ReduceOp, metadata: freetensor_ffi.Metadata, value: freetensor_ffi.Expr, atomic: bool = False) -> freetensor_ffi.Stmt","title":"as_reduce_to()"},{"location":"api/#freetensor.core.expr.VarRefFromVarDef.as_store","text":"as_store(self: freetensor_ffi.FrontendVar, metadata: freetensor_ffi.Metadata, value: freetensor_ffi.Expr) -> freetensor_ffi.Stmt","title":"as_store()"},{"location":"api/#freetensor.core.expr.VarRefFromVarDef.select","text":"Alias for self[..., idx, ...] , where idx is at the dim -th dimension","title":"select()"},{"location":"api/#freetensor.core.expr.VarRefFromVarDef.select_slice","text":"Alias for self[..., begin:end, ...] , where begin:end is at the dim -th dimension","title":"select_slice()"},{"location":"api/#freetensor.core.expr.VarRefFromVarDef.shape","text":"Return lengths of all dimensions or the length of one dimension .shape() -> list of lengths of all dimensions .shape(dim) -> length of dimension dim , where dim can be int or Expr All lengths can be Expr (if the length is dynamically decided) or int (if statically decided)","title":"shape()"},{"location":"api/#freetensor.core.expr.VarVersionRef","text":"Special VarRef used for custom gradient, generated from mark_version","title":"VarVersionRef"},{"location":"api/#freetensor.core.expr.VarVersionRef.as_reduce_to","text":"as_reduce_to(self: freetensor_ffi.FrontendVar, op: freetensor_ffi.ReduceOp, metadata: freetensor_ffi.Metadata, value: freetensor_ffi.Expr, atomic: bool = False) -> freetensor_ffi.Stmt","title":"as_reduce_to()"},{"location":"api/#freetensor.core.expr.VarVersionRef.as_store","text":"as_store(self: freetensor_ffi.FrontendVar, metadata: freetensor_ffi.Metadata, value: freetensor_ffi.Expr) -> freetensor_ffi.Stmt","title":"as_store()"},{"location":"api/#freetensor.core.expr.VarVersionRef.select","text":"Alias for self[..., idx, ...] , where idx is at the dim -th dimension","title":"select()"},{"location":"api/#freetensor.core.expr.VarVersionRef.select_slice","text":"Alias for self[..., begin:end, ...] , where begin:end is at the dim -th dimension","title":"select_slice()"},{"location":"api/#freetensor.core.expr.VarVersionRef.shape","text":"Return lengths of all dimensions or the length of one dimension .shape() -> list of lengths of all dimensions .shape(dim) -> length of dimension dim , where dim can be int or Expr All lengths can be Expr (if the length is dynamically decided) or int (if statically decided)","title":"shape()"},{"location":"api/#freetensor.core.expr.abs","text":"Absolute value For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.abs Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The absolute value","title":"abs()"},{"location":"api/#freetensor.core.expr.add","text":"lhs + rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.add Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The sum","title":"add()"},{"location":"api/#freetensor.core.expr.any","text":"Create an AnyExpr node (only for testing and type inference) Any nodes matches any expression nodes in ast.match","title":"any()"},{"location":"api/#freetensor.core.expr.cast","text":"Cast to another type Parameters: expr ( VarRef or Number ) \u2013 The operand dtype ( DataTypr or str ) \u2013 The target data type Returns: VarRef or Number \u2013 The result","title":"cast()"},{"location":"api/#freetensor.core.expr.ceil","text":"Round a float up to an interger (towards +inf) For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ceil Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result","title":"ceil()"},{"location":"api/#freetensor.core.expr.ceildiv","text":"Ceiling integer division of lhs dividing by rhs The result rounds towards positive infinity For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ceildiv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient","title":"ceildiv()"},{"location":"api/#freetensor.core.expr.cos","text":"Cosine For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.tanh Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result","title":"cos()"},{"location":"api/#freetensor.core.expr.dtype","text":"Get element data type of a variable","title":"dtype()"},{"location":"api/#freetensor.core.expr.eq","text":"lhs == rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.eq Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison","title":"eq()"},{"location":"api/#freetensor.core.expr.exp","text":"Natural exponent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.exp Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The exponent","title":"exp()"},{"location":"api/#freetensor.core.expr.floor","text":"Round a float down to an interger (towards -inf) For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.floor Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result","title":"floor()"},{"location":"api/#freetensor.core.expr.floordiv","text":"Floored integer division of lhs dividing by rhs The result rounds towards negative infinity (following Python convention, instead of C) This function is recommended over round_towards_0_div , as it enjoys more optimizations For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.floordiv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient","title":"floordiv()"},{"location":"api/#freetensor.core.expr.ge","text":"lhs >= rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ge Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison","title":"ge()"},{"location":"api/#freetensor.core.expr.gt","text":"lhs > rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.gt Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison","title":"gt()"},{"location":"api/#freetensor.core.expr.if_then_else","text":"Similar to then_case if cond else else_case NOTE: there is NO guarantee that only one branch will be executed. In some cases, both branches will be executed and the result of one of them will be picked. Therefore, please do NOT use if_then_else to guard an out-of-bound array indexing Parameters: cond ( VarRef of Number ) \u2013 Condition then_case ( VarRef or Number ) \u2013 Then-case experssion else_case ( VarRef or Number ) \u2013 Else-case expression Returns: VarRef or Number \u2013 The result","title":"if_then_else()"},{"location":"api/#freetensor.core.expr.intrinsic","text":"Invoke whatever target code Parameters: fmt ( str ) \u2013 What to run. \"%\" is filled by parameters one by one. E.g. sinf(%). Use \"%%\" to escape for \"%\". If you need two adjacent parameters, type \"(%)(%)\" or \"% %\". *params ( Sequence[Expr] ) \u2013 (Positional variadic) Parameters to fmt ret_type ( DataType or str ) \u2013 (Keyword argument only) The return type. Void for no return type. Defaults to Void has_side_effect ( bool ) \u2013 (Keyword argument only) True to indicate the intrinsic modifes something other than the return value. Defaults to false","title":"intrinsic()"},{"location":"api/#freetensor.core.expr.l_and","text":"Logical and of lhs and rhs NOTE: Short-circuit evaluation is NOT supported For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_and Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The logical and","title":"l_and()"},{"location":"api/#freetensor.core.expr.l_not","text":"Logical not For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_not Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The logical not","title":"l_not()"},{"location":"api/#freetensor.core.expr.l_or","text":"Logical or of lhs and rhs NOTE: Short-circuit evaluation is NOT supported For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.l_or Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The logical or","title":"l_or()"},{"location":"api/#freetensor.core.expr.le","text":"lhs <= rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.le Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison","title":"le()"},{"location":"api/#freetensor.core.expr.ln","text":"Natural logarithm For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ln Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The exponent","title":"ln()"},{"location":"api/#freetensor.core.expr.load_at_version","text":"Create an LoadAtVersion node (only for custom gradient) This node is only used for custom gradient. See UserGradForPrevStmt .","title":"load_at_version()"},{"location":"api/#freetensor.core.expr.lt","text":"lhs < rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.lt Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison","title":"lt()"},{"location":"api/#freetensor.core.expr.max","text":"Maximum of lhs and rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.max Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The maximum","title":"max()"},{"location":"api/#freetensor.core.expr.min","text":"Minimum of lhs and rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.min Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The minimum","title":"min()"},{"location":"api/#freetensor.core.expr.mod","text":"lhs modulus rhs The result is always non-negative (following Python convention, instead of C). This function is recommended over remainder , as it enjoys more optimizations For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.mod Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The modulo","title":"mod()"},{"location":"api/#freetensor.core.expr.mtype","text":"Get memory type of a variable","title":"mtype()"},{"location":"api/#freetensor.core.expr.mul","text":"lhs * rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.mul Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The product","title":"mul()"},{"location":"api/#freetensor.core.expr.ndim","text":"Get the number of dimensions of a variable","title":"ndim()"},{"location":"api/#freetensor.core.expr.ne","text":"lhs != rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.ne Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The comparison","title":"ne()"},{"location":"api/#freetensor.core.expr.remainder","text":"Remainder of lhs dividing rhs The result can be positive or negative (following C convention, instead of Python). End users are encouraged to use lhs % rhs instead, which follows Python convetion, and enjoys better optimization in FreeTensor For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.remainder Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The remainder","title":"remainder()"},{"location":"api/#freetensor.core.expr.round_towards_0_div","text":"C-style integer division of lhs dividing by rhs The result rounds towards 0 (following C convention, instead of Python) End users are encouraged to use lhs // rhs instead, which follows Python convetion, and enjoys better optimization in FreeTensor For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.round_towards_0_div Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient","title":"round_towards_0_div()"},{"location":"api/#freetensor.core.expr.shape","text":"shape(var, i): Get size of specified dimension of a variable shape(var): Get sizes of all dimensions of a variable","title":"shape()"},{"location":"api/#freetensor.core.expr.sigmoid","text":"Sigmoid For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sigmoid Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result","title":"sigmoid()"},{"location":"api/#freetensor.core.expr.sin","text":"Sine For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.tanh Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result","title":"sin()"},{"location":"api/#freetensor.core.expr.sqrt","text":"Square root For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sqrt Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The square root","title":"sqrt()"},{"location":"api/#freetensor.core.expr.square","text":"Square For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.square Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The square","title":"square()"},{"location":"api/#freetensor.core.expr.sub","text":"lhs - rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.sub Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The difference","title":"sub()"},{"location":"api/#freetensor.core.expr.tan","text":"Tangent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.tanh Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result","title":"tan()"},{"location":"api/#freetensor.core.expr.tanh","text":"Hyperbolic tangent For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.tanh Parameters: expr ( VarRef or Number ) \u2013 The operand Returns: VarRef or Number \u2013 The result","title":"tanh()"},{"location":"api/#freetensor.core.expr.truediv","text":"Floating point division of lhs dividing by rhs For scalar operands, it emit an expression node in AST. For non-scalar operands, it calls libop.truediv Parameters: lhs ( VarRef or Number ) \u2013 The left-hand-side operand rhs ( VarRef or Number ) \u2013 The right-hand-side operand Returns: VarRef or Number \u2013 The quotient","title":"truediv()"},{"location":"api/#freetensor.core.frontend","text":"A frontend transforming user Python functions to ASTs via staging.","title":"frontend"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload","text":"Helper class managing context in IR staging.","title":"FreeTensorOverload"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.allow_shortcut_scope","text":"Opens a scope that allows shortcut control flows in a statically deterministic context. Need to be closed by with statement.","title":"allow_shortcut_scope()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.assert_stmt","text":"Assert staging tool.","title":"assert_stmt()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.assign_stmt","text":"Customized assign wrapper. If value is instance of StagedAssignable , it's regarded as a customized assign behavior and gets executed with the assigned target variable name. This wrapper is used for initializing a variable.","title":"assign_stmt()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.break_stmt","text":"Break staging tool. Only allow break in static control flow.","title":"break_stmt()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.continue_stmt","text":"Continue staging tool. Only allow continue in static control flow.","title":"continue_stmt()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.custom_attr","text":"Customized attribute accessor. The framework first looks for a Python native attribute. If not found, it looks for this overloaded custom attribute resolver. The default implementation provides no custom attribute. Can be overridden by subclasses. Parameters: obj ( Any ) \u2013 Object to access attribute. attr ( str ) \u2013 Attribute name. Returns: Any \u2013 The attribute value.","title":"custom_attr()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.foreach","text":"Customized foreach wrapper. If value is instance of StagedIterable , its regarded as a customized foreach behavior and used to generate code for the python for loop. Otherwise, we try to execute the loop as usual.","title":"foreach()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.fullname","text":"Get distinct name.","title":"fullname()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.functiondef_wrapper","text":"Function definition wrapper. This wrapper performs extra initialization and cleanup for function definition.","title":"functiondef_wrapper()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.if_then_else_expr","text":"If-then-else expression staging tool.","title":"if_then_else_expr()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.if_then_else_stmt","text":"If-then-else statement staging tool. When predicate is deterministic in staging, only one branch is generated. Otherwise, a If node in IR is generated.","title":"if_then_else_stmt()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.load_attr","text":"Load attribute staging tool. Allows customization of reading attributes.","title":"load_attr()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.mark_position","text":"Code position handler. Defaults to a no-op. Can be overridden by subclasses. Parameters: lineno ( int ) \u2013 Line number of the next statement.","title":"mark_position()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.metadata","text":"Metadata handler. A metadata line is a comment starting with #! and followed by a metadata, represented as a string parameter. Defaults to a no-op. Can be overridden by subclasses. Parameters: content ( str ) \u2013 The metadata content.","title":"metadata()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.return_stmt","text":"Return staging tool. Only allow return in static control flow.","title":"return_stmt()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.unpack_assign_stmt","text":"Customized assign wrapper for one or more targets. If values is instance of StagedUnpackAssignable , it's regarded as a customized assign behavior and gets executed with all the assigned targets' names. Otherwise, it calls assign_stmt with each sub-assignments. Please note that names can be nested tuples like (\"a\", (\"b\", \"c\")) . Please also note that names can also be a single string like \"a\" even if values is a tuple. There is no unpacking in this case","title":"unpack_assign_stmt()"},{"location":"api/#freetensor.core.frontend.FreeTensorOverload.while_stmt","text":"While statement staging tool.","title":"while_stmt()"},{"location":"api/#freetensor.core.frontend.FreeTensorStagingError","text":"Error occurred during staging function execution (i.e. IR tree generation).","title":"FreeTensorStagingError"},{"location":"api/#freetensor.core.frontend.LifetimeScope","text":"This scope is used to register multiple scopes inside a single lifetime scope. The inner scopes might be used to register variables, etc. They will be exited in reverse order of their registration.","title":"LifetimeScope"},{"location":"api/#freetensor.core.frontend.PredefinedVarCreator","text":"","title":"PredefinedVarCreator"},{"location":"api/#freetensor.core.frontend.PredefinedVarCreator.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__class__"},{"location":"api/#freetensor.core.frontend.PredefinedVarCreator.__init__","text":"Initialize self. See help(type(self)) for accurate signature.","title":"__init__()"},{"location":"api/#freetensor.core.frontend.PredefinedVarCreator.assign","text":"Customized assign behavior. Creates a VarDef with its full name.","title":"assign()"},{"location":"api/#freetensor.core.frontend.UserGrad","text":"Define a custom gradient Follow the following steps to define custom gradient: Add some mark_version statements in the program. mark_version('y0', y) marks the specific versions of variable y at the program position of the statement and at all iterations as 'y0' . Add a UserGrad scope. 2.1. UserGrad optionally receives parameter stmt_range , recorded by the StmtRange helper class, which means the gradient is for the code specified in the range. Ignoring the parameter means setting gradient for the previous statement of the scope. 2.2. Other parameters of UserGrad sets the mapping from original variables to gradient variables. with UserGradForPrevStmt(x, y) as (dx, dy) provides VarRef dx and dy as gradient variables to be used inside the scope. In order to use the value from the forward pass in the backward pass, do not access the forward variables directly in the scope. Instead, use load_at_version expressions. load_at_version(y0, i, j) loads from y[i, j] at the specific version marked by y0 = mark_version(y) , saved from the same iteration in the forward pass . (If directly writing staged code, it is MarkVersion('y0', y) ). In other words, after AD, the position of mark_version and the dynamic loop iterator together makes up the actual version number for the tape. Build the AST with pop_ast_and_user_grads instead of pop_ast . An extra list will be returned together with the AST, which you need to pass as grad 's user_grads argument. This list records the forward-to-backward relation of the nodes. If you are directly writing staged code, use UserGradStaged instead. Parameters: *args ( Sequence[freetensor.core.expr.VarRef] ) \u2013 (Positional variadic) Mapping from original variables to gradient variables stmt_range ( StmtRange ) \u2013 The range in the original program that we are setting custom gradient for","title":"UserGrad"},{"location":"api/#freetensor.core.frontend.Var","text":"","title":"Var"},{"location":"api/#freetensor.core.frontend.Var.__class__","text":"","title":"__class__"},{"location":"api/#freetensor.core.frontend.Var.__init__","text":"Declare a variable Parameters: shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable atype ( str or AccessType ) \u2013 Access type of the variable. It specifies whether (and how) the variable is an I/O variable of the function it belongs to. Defaults to \"input\" mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used","title":"__init__()"},{"location":"api/#freetensor.core.frontend.VarCreator","text":"VarCreator(shape: Union[Sequence, freetensor.core.expr.VarRef], dtype: str, mtype: str, assigned: bool = False)","title":"VarCreator"},{"location":"api/#freetensor.core.frontend.VarCreator.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__class__"},{"location":"api/#freetensor.core.frontend.VarCreator.assign","text":"Customized assign behavior. Creates a VarDef with its full name.","title":"assign()"},{"location":"api/#freetensor.core.frontend.VersionMarker","text":"VersionMarker(var: freetensor.core.expr.VarRef, assigned: bool = False)","title":"VersionMarker"},{"location":"api/#freetensor.core.frontend.VersionMarker.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__class__"},{"location":"api/#freetensor.core.frontend.VersionMarker.assign","text":"Customized assign behavior. Creates a MarkVersion with its full name.","title":"assign()"},{"location":"api/#freetensor.core.frontend.dynamic_range","text":"Dynamic range that generates For loop in IR tree.","title":"dynamic_range"},{"location":"api/#freetensor.core.frontend.dynamic_range.__init__","text":"Initialize a dynamic range. Arguments semantic identical to builtin range .","title":"__init__()"},{"location":"api/#freetensor.core.frontend.dynamic_range.foreach","text":"Customized foreach behavior. Creates a For loop.","title":"foreach()"},{"location":"api/#freetensor.core.frontend.capture_var","text":"Capture external array as tensor variable.","title":"capture_var()"},{"location":"api/#freetensor.core.frontend.empty","text":"Create an empty variable Parameters: shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used","title":"empty()"},{"location":"api/#freetensor.core.frontend.push_for_backward","text":"Push the current value from the forward pass to be used at the backward pass This function is for custom gradients. See UserGrad for details on how to provide custom gradients. You may imagine there is a virtual stack for each variable. Each time you call x_handle = push_for_backward(x) in the forward pass, the value of x at the current iteration will be \"pushed\" to the virtual stack. You can access x_handle at the backward pass. Each time you access x_handle , you will \"pop\" the stack and get the value of x pushed at the same iteration . Since the \"stack\" is virtual, you do NOT need to \"pop\" the same count as \"push\"es: the version numbering is fully automatic. Besides, there may not be a real stack at runtime: it can be compiled to any data structure. This function will be staged to mark_version statement in the IR.","title":"push_for_backward()"},{"location":"api/#freetensor.core.frontend.var","text":"Create an with variable a given initializer Parameters: initializer ( Sequence[Sequence[...Sequence[Expr]...]] ) \u2013 (Multi-level of) sequence of expressions. Will be data of the variable shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used","title":"var()"},{"location":"api/#freetensor.core.func","text":"","title":"func"},{"location":"api/#freetensor.core.func.Func","text":"","title":"Func"},{"location":"api/#freetensor.core.func.Func.__call__","text":"Enable invoking a transformed AST in another function being transformed, via inlined_invoke","title":"__call__()"},{"location":"api/#freetensor.core.jit","text":"","title":"jit"},{"location":"api/#freetensor.core.jit.JIT","text":"Declare a function parameter as a JIT parameter A function with one or more JIT parameters will be compiled to a JIT template. It can be instantiate after the JIT paraemters are provided Usage: x: JIT or x: JIT[AnyPythonType] . The latter form has no syntactic meanings, and is only for documentation. NOTE 1: The JIT type annotation can only be used for parameter of the outer-most function intended for @transform (or @optimize , etc). It can NOT be used for inner functions intended for @inline . NOTE 2: The JIT type annoation can only annotated on parameters inside the function signature. It is NOT supported in annotations for statements.","title":"JIT"},{"location":"api/#freetensor.core.jit.JITTemplate","text":"A template that can be instantiated given concrete arguments By calling instantiate with actual arguments you are expecting to run a JIT function with, an instantiated object will be returned. Subclasses of JITTemplate should override instantiate_by_only_jit_args , and define what is actually returned. Parameters: params ( OrderedDict ) \u2013 Parameter list from inspect.signature(func).parameters jit_param_names ( Sequence ) \u2013 Sequence of names of JIT parameters in the original order defined in the function","title":"JITTemplate"},{"location":"api/#freetensor.core.jit.JITTemplate.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__class__"},{"location":"api/#freetensor.core.jit.JITTemplate.instantiate","text":"Get an instance with the arguments you are expecting to run a JIT function with","title":"instantiate()"},{"location":"api/#freetensor.core.jit.JITTemplate.instantiate_and_call","text":"Get an instance and call it with the arguments you are expecting to run a JIT function with","title":"instantiate_and_call()"},{"location":"api/#freetensor.core.jit.JITTemplate.instantiate_by_only_jit_args","text":"Get an instance with only JIT arguments This function accpets a tuple of arguments. Keyword arguments is NOT supported, so memoization can be easier, with considering the order of the arguments.","title":"instantiate_by_only_jit_args()"},{"location":"api/#freetensor.core.jit.JITTemplate.separate_args","text":"Return a list of non-JIT args, and a list of JIT args","title":"separate_args()"},{"location":"api/#freetensor.core.optimize","text":"","title":"optimize"},{"location":"api/#freetensor.core.optimize.optimize","text":"An one-click optimization from Python function to binary executable Usage: @optimize def f(...): ... It is equivalent to: @build_binary @codegen @lower @transform def f(...): ... Parameters: func ( Python function or AST ) \u2013 The user function to optimize. If not specified, a partial function will be returend, which can be used as a decorator schedule_callback ( Optional[Callable[[freetensor.core.schedule.Schedule], NoneType]] ) \u2013 Schedule(s) to apply backward_schedule_callback ( Callable[[freetensor.core.schedule.Schedule], NoneType] ) \u2013 Specify what schedule(s) to do for the backward function, if ast is returned from AD with attach_backward=True . Defaults to be the same with callback target ( Optional[freetensor_ffi.Target] ) \u2013 The target architecture. You don't have to set target if you set device device ( Optional[freetensor_ffi.Device] ) \u2013 Where to run the program default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( int ) \u2013 Verbosity level. Can be 0, 1 or 2","title":"optimize()"},{"location":"api/#freetensor.core.optimize.optimize_to_pytorch","text":"Compile a FreeTensor function to a PyTorch call, whose gradient can be recognized by PyTorch The compiled function will be a typical PyTorch's \"function\" (rather than a PyTorch's \"module\"). Technically, this means it is a wrapper function around a PyTorch's Function 's apply method Schedules (if any) must be applied to the forward function and the backward function separated. For this reason, currently only first-order gradient is supported Parameters: func ( Python function or AST ) \u2013 The user function to optimize. If not specified, a partial function will be returend, which can be used as a decorator tapes ( Union[Sequence, freetensor_ffi.GradTapeMode] ) \u2013 Intermediate variables that need to be stored from the forward pass and reused in the backward pass. This parameter can be a sequence, which contains VarDef selectors of them. It can also be a GradTapeMode , then it will determine which intermediate variables to be stored by heuristics. Avail GradTapeMode s are: All: store all variables including local scalars; None: store nothing; NoReuseOnly: store variables that only hold one version of data, which means we do not have to store each version of them in their history forward_schedule_callback ( Optional[Callable[[freetensor.core.schedule.Schedule], NoneType]] ) \u2013 Schedule(s) to apply to the forward function backward_schedule_callback ( Optional[Callable[[freetensor.core.schedule.Schedule], NoneType]] ) \u2013 Schedule(s) to apply to the backward function target ( Optional[freetensor_ffi.Target] ) \u2013 The target architecture. You don't have to set target if you set device device ( Optional[freetensor_ffi.Device] ) \u2013 Where to run the program default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( int ) \u2013 Verbosity level. Can be 0, 1 or 2","title":"optimize_to_pytorch()"},{"location":"api/#freetensor.core.param_ret_dict","text":"","title":"param_ret_dict"},{"location":"api/#freetensor.core.param_ret_dict.ParamRetDict","text":"Look an object using either a function parameter or return value's name or position","title":"ParamRetDict"},{"location":"api/#freetensor.core.param_ret_dict.ParamRetDict.__init__","text":"Either func or ( func_name and param_names and return_names ) should be provided","title":"__init__()"},{"location":"api/#freetensor.core.param_ret_dict.Parameter","text":"Alias of a parameter of a function by position instead of by name Parameter(n) represents the n-th parameter (counted from 0) Parameter() can be used if there is only one parameter","title":"Parameter"},{"location":"api/#freetensor.core.param_ret_dict.Return","text":"Alias of a return value of a function by position instead of by name Return(n) represents the n-th return value (counted from 0) Return() can be used if there is only one return value","title":"Return"},{"location":"api/#freetensor.core.passes","text":"","title":"passes"},{"location":"api/#freetensor.core.passes.lower","text":"Lower an AST using a series of passes Parameters: ast ( AST ) \u2013 The AST to be lowered. Can be a Func or a Stmt . If not specified, a partial function of lower will be returned, which can be used as a decorator target ( Optional[freetensor_ffi.Target] ) \u2013 Lower the AST to a target with target-specific passes, then the AST can be used for codegen. If not set, use the default Target in Config skip_passes ( Sequence[str] ) \u2013 Skip some pass for testing or debugging. Names in skip_passes are in underscore_style, as in Python. Please note that some passes will not be skipped even specified in these parameter, because they are indirectly called in some other passes jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances verbose ( int ) \u2013 0 = print nothing. 1 = print the lowered AST. 2 = print AST after every single passes Returns: Func or JITTemplate \u2013 Return a Func for an AST if there is no JIT parameters. Return a JITTemplate that generates a Func if there is at least one","title":"lower()"},{"location":"api/#freetensor.core.return_values_pack","text":"","title":"return_values_pack"},{"location":"api/#freetensor.core.return_values_pack.ReturnValuesPack","text":"Hold return values from a Driver invocation Return values can be retrieved in an anonymous manner: x, y, z = pack , or in a named manner: pack['x'] Please note that a ReturnValuesPack is different from a OrderedDict, as OrderedDict unpacks to keys rather than values","title":"ReturnValuesPack"},{"location":"api/#freetensor.core.return_values_pack.ReturnValuesPack.__contains__","text":"Test if a return value exists","title":"__contains__()"},{"location":"api/#freetensor.core.return_values_pack.ReturnValuesPack.__getitem__","text":"Get a return value with a name. Tuple is supported for multiple values","title":"__getitem__()"},{"location":"api/#freetensor.core.return_values_pack.ReturnValuesPack.__iter__","text":"Get all return values in the order declared in Func","title":"__iter__()"},{"location":"api/#freetensor.core.schedule","text":"","title":"schedule"},{"location":"api/#freetensor.core.schedule.IDMap","text":"A dict-like container recording an ID-to-ID mapping, representing what IDs become what IDs after a schedule An IDMap can be looked up by numerical ID, or by Stmt instances or Selector strings of the original (before applying schedule) AST","title":"IDMap"},{"location":"api/#freetensor.core.schedule.Schedule","text":"","title":"Schedule"},{"location":"api/#freetensor.core.schedule.Schedule.as_matmul","text":"Transform nested loops to be a external call to a matrix multiplication Parameters: loop ( str, ID or Stmt ) \u2013 ID of the loop Exceptions: InvalidSchedule \u2013 if the loop cannot be transformed to be a matrix multiplication","title":"as_matmul()"},{"location":"api/#freetensor.core.schedule.Schedule.ast","text":"Get the scheduled AST without function signature This is mainly for debugging and testting purpose","title":"ast()"},{"location":"api/#freetensor.core.schedule.Schedule.auto_fission_fuse","text":"(Experimental) Automatically fuse consecutive loops or vice versa using some heuristics Parameters: target ( Target ) \u2013 Target architecture","title":"auto_fission_fuse()"},{"location":"api/#freetensor.core.schedule.Schedule.auto_inline","text":"(Experimental) Automatically inline very-small VarDef nodes Parameters: target ( Target ) \u2013 Target architecture","title":"auto_inline()"},{"location":"api/#freetensor.core.schedule.Schedule.auto_parallelize","text":"(Experimental) Automatically parallelize some loops using some heuristics Parameters: target ( Target ) \u2013 Target architecture","title":"auto_parallelize()"},{"location":"api/#freetensor.core.schedule.Schedule.auto_pluto","text":"(Experimental) Automatically apply pluto-based schedules Parameters: target ( Target ) \u2013 Target architecture","title":"auto_pluto()"},{"location":"api/#freetensor.core.schedule.Schedule.auto_schedule","text":"(Experimental) Automatic scheduling using some heuristics Parameters: target ( Target ) \u2013 Target architecture","title":"auto_schedule()"},{"location":"api/#freetensor.core.schedule.Schedule.auto_set_mem_type","text":"(Experimental) Automatically set memory types using some heuristics Parameters: target ( Target ) \u2013 Target architecture","title":"auto_set_mem_type()"},{"location":"api/#freetensor.core.schedule.Schedule.auto_swap","text":"(Experimental) Automatically swap statements to enable more fission or fusion Parameters: target ( Target ) \u2013 Target architecture","title":"auto_swap()"},{"location":"api/#freetensor.core.schedule.Schedule.auto_unroll","text":"(Experimental) Automatically unroll loops using some heuristics Parameters: target ( Target ) \u2013 Target architecture","title":"auto_unroll()"},{"location":"api/#freetensor.core.schedule.Schedule.auto_use_lib","text":"(Experimental) Automatically use external libs using some heuristics Parameters: target ( Target ) \u2013 Target architecture","title":"auto_use_lib()"},{"location":"api/#freetensor.core.schedule.Schedule.blend","text":"Unroll a loop and interleave statements from each iteration E.g. for i = 0 to 2 { f(i); g(i); } will be transformed to be f(0); f(1); g(0); g(1); Virtual threads in TVM can be implemented via blend Parameters: loop ( str, ID or Stmt ) \u2013 The loop being transformed Exceptions: InvalidSchedule \u2013 if the loop is not found, the loop length is not a constant, or the dependences cannot be solved","title":"blend()"},{"location":"api/#freetensor.core.schedule.Schedule.cache","text":"Cache a variable into a new local variable All needed data will be filled into the cache first, then all reads and writes will be directed to the cache, and finally all needed data will be flushed from the cache Note for reduction: This transformation preserves the computation order. It will transform a += x a += y to a.cache = a + x + y a = a.cache If you need a \"real\" cache for reduction, which reorders the computation, use cache_reduction instead Parameters: stmt ( str, ID or Stmt ) \u2013 The statement or block (e.g. an If or a For) to be modified var ( str ) \u2013 Name of the variable to be cached mtype ( MemType ) \u2013 Where to cache Exceptions: InvalidSchedule \u2013 if the ID or name is not found Returns: (ID, ID, ID, ID) \u2013 (ID of the statement that fills the cache, ID of the statement that flushes from the cache, name of the cache variable, ID of the VarDef node of the cache variable)","title":"cache()"},{"location":"api/#freetensor.core.schedule.Schedule.cache_reduction","text":"Perform local reductions (e.g. sum) in a local variable first, and then reduce the local result to the global variable E.g. a += x a += y will be transformed to be a.cache = x + y a += a.cache Parameters: stmt ( str, ID or Stmt ) \u2013 The statement or block (e.g. an If or a For) to be modified var ( str ) \u2013 Name of the variable to be cached. Only reductions are allowed on var in stmt . Plain reads or writes are not allowed mtype ( MemType ) \u2013 Where to cache Exceptions: InvalidSchedule \u2013 if the ID or name is not found, or there are unsupported reads or writes Returns: (ID, ID, ID, ID) \u2013 (ID of the statement that initialize the cache, ID of the statement that reduces the local result to the global result, name of the cache variable, ID of the VarDef node of the cache variable)","title":"cache_reduction()"},{"location":"api/#freetensor.core.schedule.Schedule.fission","text":"Fission a loop into two loops each containing part of the statements, one followed by another To split loop into two nested loops, use split instead Statements inside the original loop will be distributed to one or both (happening if they are scope statements) loops. If a statement is originally labeled \"S\", it can be selected by \" \\(fission.0{S}\" (from the first loop) or \"\\) fission.1{S}\" (from the second loop) after fission. If one of the resulting loop has an empty body, it will be removed Parameters: loop ( str, ID or Stmt ) \u2013 The loop to be fissioned side ( FissionSide ) \u2013 If After , splitter is the last statement of the first loop. If Before , splitter is the first statement of the second loop splitter ( str (Selector string), ID, Stmt, or list of them ) \u2013 Where to fission the loop. If multiple statement are selected, fission the look before or after all of them allow_enlarge ( bool ) \u2013 If True, try to avoid dependence by enlarging some VarDef nodes. If False, raise InvalidSchedule in such cases. Exceptions: InvalidSchedule \u2013 if any dependence cannot be resolved Returns: (IDMap, IDMap) \u2013 ({old ID -> new ID in 1st loop}, {old ID -> new ID in 2nd loop}). If a loop is removed because it has an empty body, it will not be in the returned map","title":"fission()"},{"location":"api/#freetensor.core.schedule.Schedule.fork","text":"fork(self: freetensor_ffi.Schedule) -> freetensor_ffi.Schedule","title":"fork()"},{"location":"api/#freetensor.core.schedule.Schedule.func","text":"Get the scheduled function","title":"func()"},{"location":"api/#freetensor.core.schedule.Schedule.fuse","text":"Fuse two directly following loops with the same length into one To merge nested loops into one, use merge instead parallelize , unroll and vectorize properties will be reset on the fused loop Suppose the original loops are labeled \"L1\" and \"L2\", the fused loop can be selected by \"$fuse{L1, L2}\" Parameters: loop0 ( str, ID or Stmt ) \u2013 The leading loop loop1 ( str, ID or Stmt, Optional ) \u2013 The following loop. If omitted, it will try to find a following loop of loop0 strict ( bool ) \u2013 False by default. If set to True, throw an error if unable to determine whether the two loops are of the same length Exceptions: InvalidSchedule \u2013 if the two loops are not directly following, the two loops are not of the same length, or there is any dependence cannot be resolved Returns: ID \u2013 ID of the result loop","title":"fuse()"},{"location":"api/#freetensor.core.schedule.Schedule.inline","text":"Remove a variable. When the variable is used, recompute its value Parameters: vardef ( str, ID or Stmt ) \u2013 The VarDef statement of the specific variable. It can not be an I/O varible Exceptions: InvalidSchedule \u2013 if the variable cannot be completely removed","title":"inline()"},{"location":"api/#freetensor.core.schedule.Schedule.merge","text":"Merge two directly nested loops into one To fuse consecutive loops, use fuse instead parallelize , unroll and vectorize properties will be reset on the merged loop Suppose the original loops are labeled \"L1\" and \"L2\", the merged loop can be selected by \"$merge{L1, L2}\" Parameters: loop1, loop2 ( str, ID or Stmt ) \u2013 loops to be merged, can be in any order Exceptions: InvalidSchedule \u2013 if the loops are not directly nested Returns: ID \u2013 ID of the merged loop","title":"merge()"},{"location":"api/#freetensor.core.schedule.Schedule.move_to","text":"Move a statement to a new position This is a composite schedule command, which is implemented with other commands If moving a statement out of some loops, identical loops will be added around the moved statement, which is equivalent to fission these loops Parameters: stmt ( str, ID or Stmt ) \u2013 The statement to be moved side ( MoveToSide ) \u2013 Whether stmt will be BEFORE or AFTER `dst dst ( str (Selector string), ID, Stmt, or list of them ) \u2013 Insert stmt to be directly after this statement. If multiple statements are selected, move to before or after all of them Exceptions: InvalidSchedule \u2013 if there is no feasible path to move Returns: (ID, ID) \u2013 (The new ID of the moved statement, The out-most newly introduced statments including the added loops)","title":"move_to()"},{"location":"api/#freetensor.core.schedule.Schedule.parallelize","text":"Mark a loop with a parallel implementation This schedule follows a fork-join model: multiple workers (abstract threads) are created (but physically the threads may be cached in a thread pool) when the loop begins, do their jobs in parallel, and join when the loop ends OpenMP threads follow a typical fork-join model. CUDA threads run in a bulk-synchronous parallel (BSP) model, which can also be mimiked by the fork-join model: All threads start when the kernel get launched, but they only begin to do their jobs when the parallel loop begins. Nevertheless, the fork-join model needs the following extension to fully mimic a BSP model: Taking CUDA as an example, we allow binding a loop to threadIdx.x inside another loop bound to threadIdx.x , which is illegal in a classic fork-join model. For example, we may implement a matmul with collaborative fetch as below: for i : threadIdx.x # Li for j : threadIdx.y # Lj local_sum = 0 # In gpu/local memory, unique to (i, j) for k0 # Lk0 for k : threadIdx.y # Lk1_a A_cache[k] = A[i, k] # In gpu/shared, shared by different j for k : threadIdx.x # Lk1_b B_cache[k] = B[k, j] # In gpu/shared, shared by different i for k # Lk1_c sum += A_cache[k] * B_cache[k] C[i, j] = local_sum A seemingly plausible solution to avoid this extension is to reorder Lk0 to outer-most, and then move Lk1_a and Lk1_b out of Li or Lj . This resolves the nested threadIdx.x and threadIdx.y binding problem by running Li+Lk1_a , Lj+Lk1_b and Li+Lj interleavingly, instead of running Lk1_a and Lk1_b inside Li+Lj . However, this approach is illegal, because the local variable local_sum can no longer be kept inside the body of Li and Lj : It has to be reused across multiple runs of Li and Lj Please also note that we can bind one threadIdx.x to two loops only when the body statement is loop-invariant to one of them. For example, the following binding is still illegal, even in our extended fork-join model, because it violates its serial semantics: for i : threadIdx.x for j : threadIdx.x A[i, j] ++ Parameters: loop ( str, ID or Stmt ) \u2013 The loop parallel ( ParallelScope ) \u2013 Parallel scope allow_reduction ( bool ) \u2013 If false, raise InvalidSchedule if this schedule would introduce a parallel reduction Exceptions: InvalidSchedule \u2013 if the loop is not found or unable to be parallelized","title":"parallelize()"},{"location":"api/#freetensor.core.schedule.Schedule.permute","text":"Permute perfectly nested loops (directly nested loops without statements in between) with the given loop space transformation function The transformed loops follow ascending lexical order of the transformed terms returned by transformFunc when called with original iteration Parameters: loops ( array like of str, ID or Stmt ) \u2013 the list of perfectly nested loops to be permuted transform_func ( Callable[[Expr], Expr] ) \u2013 the loop space transformation function, should be bijective Returns: list of ID \u2013 the list of IDs of permuted loops","title":"permute()"},{"location":"api/#freetensor.core.schedule.Schedule.pluto_fuse","text":"Use Pluto+ algorithm to permute and fuse two loops, with as most parallelizable loops as possible at outermost levels. The two loops are required to be consequent; all directly nested levels are detected and subject to permutation. Remaining levels that cannot be fused are left inside the fused loops as two statements Parameters: loop0 ( str, ID or Stmt ) \u2013 The first loop to fuse loop1 ( str, ID or Stmt ) \u2013 The second loop to fuse nest_level_0 ( int ) \u2013 The number of nesting levels of loop 0 to be considered, defaults to maximum possible nest_level_1 ( int ) \u2013 The number of nesting levels of loop 1 to be considered, defaults to maximum possible fusable_overlap_threshold ( int ) \u2013 The minimum overlapping size of two loops to be regarded fusable. Defaults to 1 fusable_nonoverlap_tolerance ( int ) \u2013 The maximum non-overlapping size at either side of two loops to be regarded fusable. Defaults to 4 do_simplify ( bool ) \u2013 Whether the result is simplified by the way, defaults to true Exceptions: InvalidSchedule \u2013 if the loops are not consequent Returns: (ID, int) \u2013 The ID of fused loop and level of parallelizable loops","title":"pluto_fuse()"},{"location":"api/#freetensor.core.schedule.Schedule.pluto_permute","text":"Use Pluto+ algorithm to permute a single loop, with as most parallelizable loops as possible at outermost levels. Parameters: loop ( str, ID or Stmt ) \u2013 The loop to permute nest_level ( int ) \u2013 The number of nesting levels to be considered, defaults to maximum possible do_simplify ( bool ) \u2013 Whether the result is simplified by the way, defaults to true Returns: (ID, int) \u2013 The ID of permuted loop and level of parallelizable loops","title":"pluto_permute()"},{"location":"api/#freetensor.core.schedule.Schedule.reorder","text":"Reorder directly nested loops To swap consecutive loops, use swap instead Parameters: order ( array like of str, ID or Stmt ) \u2013 Vector of loops. The requested order of the loops mode ( ReorderMode ) \u2013 How to deal with imperfectly nested loops. PerfectOnly => raise an exception. MoveOutImperfect => do fission in advance to move out statements between the loops, which may enlarge intermediate tensors. MoveInImperfect => move statements between the loops inwards after adding gurads them them, which may hurt parallelism Exceptions: InvalidSchedule \u2013 if the input is invalid or there are breaking dependences","title":"reorder()"},{"location":"api/#freetensor.core.schedule.Schedule.separate_tail","text":"Seperate main iterations and tail iterations of a loop E.g. for i = 0 -> 3 { for j = 0 -> 4 { if (i * 4 + j < 10) { ... } } } Each loop will be separated into 2 parts: the body and the tail. After simplification, the program will finally be transformed to for i = 0 -> 2 { for j = 0 -> 4 { ... } } for j = 0 -> 2 { ... } Ideally, all programs can benefit from this schedule. However, this schedule may greatly increase the program size and make the compiling time way too long. Therefore, this transformation is implemented as a schedule, which can be applied optionally. (TODO: Optionally apply this schedule to part of the program) Parameters: noDuplicateVarDefs ( bool ) \u2013 If there is two VarDef nodes in two branches, it may result in doubled memory use, since different thread may go to different branch. Set this parameter to true to stop duplicating VarDef nodes.","title":"separate_tail()"},{"location":"api/#freetensor.core.schedule.Schedule.set_mem_type","text":"Change where a variable is stored Parameters: vardef ( str, ID or Stmt ) \u2013 ID of the VarDef statement of the specific variable mtype ( MemType ) \u2013 Where the variable should be stored rejectIndirectAccess ( bool ) \u2013 Registers usually do not support indirect access. If a variable is accessed indirectly, setting it to use registers is meaningless even successful. If this parameter is set to true, throw an exception if the variable being set is accessed indirectly. Specifically, two types of access are considered indirect: 1) The index is a load from another variable, or 2) The index is a loop iterator and the loop has a dynamic length (which can not be unrolled by a backend compiler). By default, this parameter is determined automatically by mtype . Exceptions: InvalidSchedule \u2013 if the variable is not found, or if rejecting an indirect access","title":"set_mem_type()"},{"location":"api/#freetensor.core.schedule.Schedule.split","text":"Split a loop into two nested loops To fission a loop into two consecutive loops, use fission instead Two modes are provided: Specify factor and leave nparts to -1. It will result in an outer loop with length ceil(n / factor) , and an inner loop with length factor , where n is the original loop length added by shift . The original iterator i will be transformed to i0 * factor + i1 , where i0 and i1 are the iterators of the new outer and inner loops, respectively Specify nparts and leave factor to -1. It will result in an outer loop with length nparts , and an inner loop with length ceil(n / nparts) , where n is the original loop length added by shift . The original iterator i will be transformed to i0 * ceil(n / nparts) + i1 , where i0 and i1 are the iterators of the new outer and inner loops, respectively Please note that the second mode will introduce an i0 * ceil(n / nparts) factor into the program, which cannot be recognized by polyhedral analysis, which may hinder some following schedules. If possible, plese use the first mode, and then reorder the inner and outer loops Suppose the original loop is labeled \"L\", the split two loops can be selected by \" \\(split.0{L}\" (the outer loop) and \"\\) split.1{L}\" (the inner loop). If one of the resulting loop is proved to have only a single iteration, it will be removed Parameters: node ( str, ID or Stmt ) \u2013 The loop to be split factor ( int ) \u2013 Length of the inner loop. Set to -1 if using nparts nparts ( int ) \u2013 Length of the outer loop. Set to -1 if using factor Exceptions: InvalidSchedule \u2013 if the loop is not found Returns: (Optional[ID], Optional[ID]) \u2013 (outer loop ID, inner loop ID), either ID can be None if the loop is proved to have only a single iteration","title":"split()"},{"location":"api/#freetensor.core.schedule.Schedule.swap","text":"Swap statements in the same block To reorder nested loops, use reorder instead Parameters: order ( List[str (Selector string), ID, List[ID], Stmt, or List[Stmt]] ) \u2013 The statements. If one item of the order list contains multiple statements, the order list will be flattened Exceptions: InvalidSchedule \u2013 if the statements are not found or the dependences cannot be solved","title":"swap()"},{"location":"api/#freetensor.core.schedule.Schedule.unroll","text":"Unroll a loop Parameters: loop ( str, ID or Stmt ) \u2013 ID of the loop immediate ( bool ) \u2013 If false (by default), postpone the unroll procedure to the backend compiler, which saves scheduling time. If true, unroll the loop immediately, which may help further simplifications based on the unrolled result. If your purpose is just to fill the instruction cache, set it to false. If you are unrolling a loop that computes array indices, set it to true Exceptions: InvalidSchedule \u2013 if the loop is not found or length of the loop is not a constant","title":"unroll()"},{"location":"api/#freetensor.core.schedule.Schedule.var_merge","text":"Merge two dimensions of a variable Parameters: vardef ( str, ID or Stmt ) \u2013 ID of the VarDef statement of the specific variable dim ( int ) \u2013 Merge the dim -th and the (dim + 1) -th dimension","title":"var_merge()"},{"location":"api/#freetensor.core.schedule.Schedule.var_reorder","text":"Reorder the dimensions of a variable Parameters: vardef ( str, ID or Stmt ) \u2013 ID of the VarDef statement of the specific variable order ( array like of str, ID or Stmt ) \u2013 Vector of integers. The new order of the dimensions Exceptions: InvalidSchedule \u2013 if the variable or the order is illegal","title":"var_reorder()"},{"location":"api/#freetensor.core.schedule.Schedule.var_split","text":"Split a dimension of a variable into two Parameters: vardef ( str, ID or Stmt ) \u2013 ID of the VarDef statement of the specific variable dim ( int ) \u2013 which dimension to be split mode ( VarSplitMode ) \u2013 When the dimension to split is not divisible by factor or nparts , the resulting shape may become larger. In FixedSize mode, the actual buffer size will not be changed, and gurads will be added to prevent out-of-bound accesses. In RelaxedSize mode, the buffer size may increase. The RelaxedSize mode cannot be applied to I/O variables factor ( int ) \u2013 Length of the inner (higher no.) dimension. Set to -1 if using nparts nparts ( int ) \u2013 Length of the outer (lower no.) loop. Set to -1 if using factor Exceptions: InvalidSchedule \u2013 if the variable or the dimension is not found","title":"var_split()"},{"location":"api/#freetensor.core.schedule.Schedule.vectorize","text":"Vectorize a loop Please note that, as vectorization is different from architecture to achitecture, the scheduler may or may not postpone it to the backend compiler. The vectorization is a best-effort schedule Parameters: loop ( str, ID or Stmt ) \u2013 ID of the loop Exceptions: InvalidSchedule \u2013 if the ID or name is not found, or the dependence requirement is not met","title":"vectorize()"},{"location":"api/#freetensor.core.schedule.schedule","text":"Apply any schedule on an AST through a user callback Parameters: ast ( Func or Stmt ) \u2013 The AST to schedule. If not specified, a partial function will be returned that cna be used as a decorator callback ( Callable[[freetensor.core.schedule.Schedule], NoneType] ) \u2013 Specify what schedule(s) to do in this callback backward_callback ( Callable[[freetensor.core.schedule.Schedule], NoneType] ) \u2013 Specify what schedule(s) to do for the backward function, if ast is returned from AD with attach_backward=True . Defaults to be the same with callback jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances verbose ( int ) \u2013 0 = print nothing. 1 = print the final AST. 2 = print an AST after each schedule Returns: Func or JITTemplate \u2013 Return a Func for an AST if there is no JIT parameters. Return a JITTemplate that generates a Func if there is at least one","title":"schedule()"},{"location":"api/#freetensor.core.staging","text":"A staging framework to support the FreeTensor frontend.","title":"staging"},{"location":"api/#freetensor.core.staging.AllowShortcutScope","text":"Allow return scope. This is a context manager that allows return in statically deterministic control flow.","title":"AllowShortcutScope"},{"location":"api/#freetensor.core.staging.BreakException","text":"Exception to be raised by StagingOverload.break_stmt. Breaks from a for loop.","title":"BreakException"},{"location":"api/#freetensor.core.staging.ContinueException","text":"Exception to be raised by StagingOverload.continue_stmt. Continues a for loop.","title":"ContinueException"},{"location":"api/#freetensor.core.staging.ReplaceAnnotations","text":"","title":"ReplaceAnnotations"},{"location":"api/#freetensor.core.staging.ReplaceAnnotations.generic_visit","text":"Called if no explicit visitor function exists for a node.","title":"generic_visit()"},{"location":"api/#freetensor.core.staging.ReplaceAnnotations.visit","text":"Visit a node.","title":"visit()"},{"location":"api/#freetensor.core.staging.ReturnException","text":"Exception to be raised by StagingOverload.return_stmt. Holds a return value that will be passed through to the function wrapper.","title":"ReturnException"},{"location":"api/#freetensor.core.staging.StagedAssignable","text":"","title":"StagedAssignable"},{"location":"api/#freetensor.core.staging.StagedAssignable.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__class__"},{"location":"api/#freetensor.core.staging.StagedPredicate","text":"","title":"StagedPredicate"},{"location":"api/#freetensor.core.staging.StagedPredicate.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__class__"},{"location":"api/#freetensor.core.staging.StagedTypeAnnotation","text":"","title":"StagedTypeAnnotation"},{"location":"api/#freetensor.core.staging.StagedTypeAnnotation.__class__","text":"","title":"__class__"},{"location":"api/#freetensor.core.staging.StagedTypeAnnotationMeta","text":"","title":"StagedTypeAnnotationMeta"},{"location":"api/#freetensor.core.staging.StagedTypeAnnotationMeta.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__base__"},{"location":"api/#freetensor.core.staging.StagedTypeAnnotationMeta.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator.","title":"register()"},{"location":"api/#freetensor.core.staging.StagedUnpackAssignable","text":"","title":"StagedUnpackAssignable"},{"location":"api/#freetensor.core.staging.StagedUnpackAssignable.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__class__"},{"location":"api/#freetensor.core.staging.StagingError","text":"Error occurred during staging function execution (i.e. IR tree generation).","title":"StagingError"},{"location":"api/#freetensor.core.staging.StagingOverload","text":"","title":"StagingOverload"},{"location":"api/#freetensor.core.staging.StagingOverload.allow_shortcut_scope","text":"Opens a scope that allows shortcut control flows in a statically deterministic context. Need to be closed by with statement.","title":"allow_shortcut_scope()"},{"location":"api/#freetensor.core.staging.StagingOverload.assert_stmt","text":"Assert staging tool.","title":"assert_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.assign_stmt","text":"Customized assign wrapper. If value is instance of StagedAssignable , it's regarded as a customized assign behavior and gets executed with the assigned target variable name. This wrapper is used for initializing a variable.","title":"assign_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.break_stmt","text":"Break staging tool. Only allow break in static control flow.","title":"break_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.continue_stmt","text":"Continue staging tool. Only allow continue in static control flow.","title":"continue_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.custom_attr","text":"Customized attribute accessor. The framework first looks for a Python native attribute. If not found, it looks for this overloaded custom attribute resolver. The default implementation provides no custom attribute. Can be overridden by subclasses. Parameters: obj ( Any ) \u2013 Object to access attribute. attr ( str ) \u2013 Attribute name. Returns: Any \u2013 The attribute value.","title":"custom_attr()"},{"location":"api/#freetensor.core.staging.StagingOverload.foreach","text":"Customized foreach wrapper. If value is instance of StagedIterable , its regarded as a customized foreach behavior and used to generate code for the python for loop. Otherwise, we try to execute the loop as usual.","title":"foreach()"},{"location":"api/#freetensor.core.staging.StagingOverload.functiondef_wrapper","text":"Function definition wrapper. This wrapper performs extra initialization and cleanup for function definition.","title":"functiondef_wrapper()"},{"location":"api/#freetensor.core.staging.StagingOverload.if_then_else_expr","text":"If-then-else expression staging tool.","title":"if_then_else_expr()"},{"location":"api/#freetensor.core.staging.StagingOverload.if_then_else_stmt","text":"If-then-else statement staging tool. When predicate is deterministic in staging, only one branch is generated. Otherwise, a If node in IR is generated.","title":"if_then_else_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.load_attr","text":"Load attribute staging tool. Allows customization of reading attributes.","title":"load_attr()"},{"location":"api/#freetensor.core.staging.StagingOverload.mark_position","text":"Code position handler. Defaults to a no-op. Can be overridden by subclasses. Parameters: lineno ( int ) \u2013 Line number of the next statement.","title":"mark_position()"},{"location":"api/#freetensor.core.staging.StagingOverload.metadata","text":"Metadata handler. A metadata line is a comment starting with #! and followed by a metadata, represented as a string parameter. Defaults to a no-op. Can be overridden by subclasses. Parameters: content ( str ) \u2013 The metadata content.","title":"metadata()"},{"location":"api/#freetensor.core.staging.StagingOverload.return_stmt","text":"Return staging tool. Only allow return in static control flow.","title":"return_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.unpack_assign_stmt","text":"Customized assign wrapper for one or more targets. If values is instance of StagedUnpackAssignable , it's regarded as a customized assign behavior and gets executed with all the assigned targets' names. Otherwise, it calls assign_stmt with each sub-assignments. Please note that names can be nested tuples like (\"a\", (\"b\", \"c\")) . Please also note that names can also be a single string like \"a\" even if values is a tuple. There is no unpacking in this case","title":"unpack_assign_stmt()"},{"location":"api/#freetensor.core.staging.StagingOverload.while_stmt","text":"While statement staging tool.","title":"while_stmt()"},{"location":"api/#freetensor.core.staging.TransformError","text":"Error occurred during AST transforming from python function to staging function that generates IR tree.","title":"TransformError"},{"location":"api/#freetensor.core.staging.Transformer","text":"Transformer(filename: 'str', base_lineno: 'int', curr_func: 'str' = None, nonlocals: 'List[List[str]]' = None)","title":"Transformer"},{"location":"api/#freetensor.core.staging.Transformer.generic_visit","text":"Called if no explicit visitor function exists for a node.","title":"generic_visit()"},{"location":"api/#freetensor.core.staging.Transformer.visit","text":"Visit a node.","title":"visit()"},{"location":"api/#freetensor.core.staging.Transformer.visit_AnnAssign","text":"Rule: x: Ty -> freetensor__annotate__x = annotate_stmt('x', Ty) if freetensor__annotate__x: x = freetensor__annotate__x : pure annotation","title":"visit_AnnAssign()"},{"location":"api/#freetensor.core.staging.Transformer.visit_Assign","text":"Rule: lhs = rhs -> lhs = unpack_assign_stmt('lhs', rhs) x.lhs = rhs -> x.lhs = unpack_assign_stmt('lhs', rhs) a, (b, c) = (x, (y, z)) -> a, (b, c) = unpack_assign_stmt(('a', ('b', 'c')), (x, (y, z))) a = b = c -> a = unpack_assign_stmt('a', c); b = unpack_assign_stmt('b', c) If unpack_assign_stmt is not overloaded, assign_stmt will be called for each item","title":"visit_Assign()"},{"location":"api/#freetensor.core.staging.Transformer.visit_Compare","text":"Expand multiple comparison into and expression.","title":"visit_Compare()"},{"location":"api/#freetensor.core.staging.Transformer.visit_For","text":"Rule: for x in iter: body -> def for_body(x): body foreach('x', iter, for_body)","title":"visit_For()"},{"location":"api/#freetensor.core.staging.Transformer.visit_If","text":"Rule: if pred: body else: orelse -> def then_body(): body def else_body(): orelse if_then_else_stmt(pred, then_body, else_body)","title":"visit_If()"},{"location":"api/#freetensor.core.staging.Transformer.visit_IfExp","text":"Rule: body if test else orelse -> if_then_else_expr(test, body, orelse)","title":"visit_IfExp()"},{"location":"api/#freetensor.core.staging.Transformer.visit_While","text":"Rule: while pred: body -> def while_body(): body while_stmt(lambda: pred, while_body)","title":"visit_While()"},{"location":"api/#freetensor.core.staging.call_helper","text":"Call helper that generates a python AST Call node with given callee (overload member) and arguments AST node.","title":"call_helper()"},{"location":"api/#freetensor.core.staging.function_helper","text":"Function helper that generates a python AST FunctionDef node with given name, arguments name, and body.","title":"function_helper()"},{"location":"api/#freetensor.core.stmt","text":"Facility to build AST statements Classes and functions in this module are internally used by transformer to construct ASTs. They are also used by some internal tests. API of these classes and functions are subject to changes. End users are encouraged to use transformer , instead of this module. Classes and functions in this module are all in BigCamel naming style, to distinguish from expressions in expr.py","title":"stmt"},{"location":"api/#freetensor.core.stmt.Assert","text":"Scope used to create an Assert node This scope is internally used by transformer and tests E.g.: with Assert(i > 0): ... # Assertion body","title":"Assert"},{"location":"api/#freetensor.core.stmt.Else","text":"Scope used to create an else branch of an If node This scope is internally used by transformer and tests E.g.: with If(i > 0): ... # True branch with Else(): ... # Else branch","title":"Else"},{"location":"api/#freetensor.core.stmt.For","text":"Scope used to create a For node This scope is internally used by transformer and tests E.g.: with For('i', 0, n) as i: ... # Loop body","title":"For"},{"location":"api/#freetensor.core.stmt.If","text":"Scope used to create an If node This scope is internally used by transformer and tests E.g.: with If(i > 0): ... # Branch body","title":"If"},{"location":"api/#freetensor.core.stmt.Invoke","text":"Inlined invocation of another AST Invoke is used as a scope ( with Invoke(...) as returned_vars ), so that variables returned by the callee can be used in the socpe Invoke can be used for invoking a gradient function, which has already been lowered as an AST. Please note that once a user function has been lowered as an AST, the dimensionalities of its tensors get fixed. Therefore, to invoke ordinary user functions, please use inline in transformer instead, which supports generic types","title":"Invoke"},{"location":"api/#freetensor.core.stmt.NamedScope","text":"Scope used to create an StmtSeq node with an explicit labels E.g.: with NamedScope(): ... # body This scope is used for testing only. StmtSeq nodes can be deleted in many lowering passes","title":"NamedScope"},{"location":"api/#freetensor.core.stmt.UserGradStaged","text":"Internal staged implementation of UserGrad","title":"UserGradStaged"},{"location":"api/#freetensor.core.stmt.Any","text":"Create an Any node (only for testing) Any nodes matches any statement nodes in ast.match","title":"Any()"},{"location":"api/#freetensor.core.stmt.Eval","text":"Create an Eval node This scope is internally used by transformer and tests","title":"Eval()"},{"location":"api/#freetensor.core.stmt.MarkLabel","text":"Mark the ID of the following statement This scope is internally used by transformer and tests","title":"MarkLabel()"},{"location":"api/#freetensor.core.stmt.MarkVersion","text":"Create an MarkVersion node (only for custom gradient) This node is only used for custom gradient. See UserGrad .","title":"MarkVersion()"},{"location":"api/#freetensor.core.stmt.VarDef","text":"A factory function that creates a VarDef or a series of nested VarDef s This scope is internally used by transformer and tests","title":"VarDef()"},{"location":"api/#freetensor.core.transform","text":"","title":"transform"},{"location":"api/#freetensor.core.transform.inline","text":"Enable a user function to be called by a transformed function at run time Parameters: func ( Python function ) \u2013 The user function src ( str (Optional) ) \u2013 The source code of func . This parameter is only required if the source code cannot be get automatically, e.g., if func is generated from an exec default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True verbose ( bool ) \u2013 True to print the generated Python code that is used for transforming","title":"inline()"},{"location":"api/#freetensor.core.transform.transform","text":"Transform a user function to an AST Parameters: func ( Python function ) \u2013 The user function to transform. If not specified, a partial function will be returend, which can be used as a decorator default_dynamic_range ( bool ) \u2013 If True, the built-in range is replaced with freetensor.dynamic_range. Defaults to True bind ( Mapping[str, Any] ) \u2013 Bind some parameters to specific values before transformations. Accpeting a parameter-name-to-value dict. jit_cache ( Callable[[Callable], Callable] ) \u2013 Function decorator used to cache JIT instances target ( Target ) \u2013 If not None, set config.default_target when transforming. This affects the default memory type use to create variables from Var , empty and etc. verbose ( int ) \u2013 0 = print nothing. 1 = print the resulting AST. 2 = 1 + print the generated Python code that is used for transforming Returns: Func or JITTemplate \u2013 Return a Func for an AST if there is no JIT parameters. Return a JITTemplate that generates a Func if there is at least one","title":"transform()"},{"location":"api/#freetensor.core.utils","text":"","title":"utils"},{"location":"api/#freetensor.core.utils.as_decorator","text":"Enable a multi-parameter function f to be used as a decorator Suppose g = as_decorator(f) , enable the following usages: @g def h(...): ... @g(a=a, b=b, c=c) def h(...): ... Formally, g will have the same parameters as f . f 's first parameter should be the function it decorate, say h , and may have other parameters with default values. If h is set when called, g will return the decorated function, just as f does. If h is not set, g will return an f 's partial function with all other parameters set, and the partial function can then be decorate another h again.","title":"as_decorator()"},{"location":"api/#freetensor.libop","text":"","title":"libop"},{"location":"api/#freetensor.libop.assign","text":"","title":"assign"},{"location":"api/#freetensor.libop.assign.add_to","text":"(Broadcasted) add to a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor","title":"add_to()"},{"location":"api/#freetensor.libop.assign.assign","text":"(Broadcasted) assign to a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor","title":"assign()"},{"location":"api/#freetensor.libop.assign.floordiv_to","text":"(Broadcasted) rounding-towards-negative-infinity integer division (following Python convention, but not C) from a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor","title":"floordiv_to()"},{"location":"api/#freetensor.libop.assign.mod_to","text":"(Broadcasted) modulo (results are non-negative, following Python convention, but not C) from a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor","title":"mod_to()"},{"location":"api/#freetensor.libop.assign.mul_to","text":"(Broadcasted) multiply to a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor","title":"mul_to()"},{"location":"api/#freetensor.libop.assign.sub_to","text":"(Broadcasted) subtract from a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor","title":"sub_to()"},{"location":"api/#freetensor.libop.assign.truediv_to","text":"(Broadcasted) floating-point division from a tensor two another tensor Parameters: y ( VarRef ) \u2013 The target tensor x ( VarRef ) \u2013 The source tensor","title":"truediv_to()"},{"location":"api/#freetensor.libop.concat","text":"","title":"concat"},{"location":"api/#freetensor.libop.concat.concat","text":"Concatenate a list of tensors into a single tensor on an existing axis (out-of-place) All input tensors must have the same shape, except for the dimension size of the axis to concatenate on. All input tensors must have the same data type and memory type. Parameters: inputs ( Sequence[freetensor.core.expr.VarRef] ) \u2013 Tensors for concatenation axis ( int ) \u2013 Dimension number for concatenation. Negative axis means counting from the last dimension Returns: VarRef \u2013 Concatenation result","title":"concat()"},{"location":"api/#freetensor.libop.concat.concat_","text":"Concatenate a list of tensors into a single tensor on an existing axis (in-place) All input tensors must have the same shape, except for the dimension size of the axis to concatenate on. All input tensors must have the same data type and memory type. Parameters: inputs ( Sequence[freetensor.core.expr.VarRef] ) \u2013 Tensors for concatenation output ( VarRef ) \u2013 Concatenation result axis ( int ) \u2013 Dimension number for concatenation. Negative axis means counting from the last dimension","title":"concat_()"},{"location":"api/#freetensor.libop.concat.stack","text":"Concatenate a list of tensors into a single tensor on a new axis (out-of-place) All input tensors must have the same shape, data type and memory type. Parameters: inputs ( Sequence[freetensor.core.expr.VarRef] ) \u2013 Tensors for concatenation axis ( int ) \u2013 Dimension number for concatenation. Negative axis means counting from the last dimension Returns: VarRef \u2013 Concatenation result","title":"stack()"},{"location":"api/#freetensor.libop.concat.stack_","text":"Concatenate a list of tensors into a single tensor on a new axis (in-place) All input tensors must have the same shape, data type and memory type. Parameters: inputs ( Sequence[freetensor.core.expr.VarRef] ) \u2013 Tensors for concatenation output ( VarRef ) \u2013 Concatenation result axis ( int ) \u2013 Dimension number for concatenation. Negative axis means counting from the last dimension","title":"stack_()"},{"location":"api/#freetensor.libop.constant","text":"","title":"constant"},{"location":"api/#freetensor.libop.constant.ones","text":"Create a one-valued tensor Parameters: shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used Returns: The one-valued tensor","title":"ones()"},{"location":"api/#freetensor.libop.constant.ones_","text":"Fill ones to a tensor Parameters: y ( VarRef ) \u2013 The tensor to fill","title":"ones_()"},{"location":"api/#freetensor.libop.constant.zeros","text":"Create a zero tensor Parameters: shape ( Sequence[Expr] or Var ) \u2013 Shape of the variable. A variable can be created using a literal shape, or another fixed-length VarRef as a shape dtype ( str or DataType ) \u2013 Data type of the variable mtype ( str or MemType (Optional) ) \u2013 Memory type of the variable. If omitted, the main memory type of the default Target in config will be used Returns: The zero tensor","title":"zeros()"},{"location":"api/#freetensor.libop.constant.zeros_","text":"Fill zeros to a tensor Parameters: y ( VarRef ) \u2013 The tensor to fill","title":"zeros_()"},{"location":"api/#freetensor.libop.conv","text":"","title":"conv"},{"location":"api/#freetensor.libop.conv.conv","text":"Convolution. The result is returned Parameters follow ONNX convention. Currently only 2-D convolution is supported","title":"conv()"},{"location":"api/#freetensor.libop.conv.conv_","text":"Convolution. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D convolution is supported","title":"conv_()"},{"location":"api/#freetensor.libop.element_wise","text":"","title":"element_wise"},{"location":"api/#freetensor.libop.element_wise.abs","text":"Element-wise absolute value of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"abs()"},{"location":"api/#freetensor.libop.element_wise.abs_","text":"Element-wise absolute value of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor","title":"abs_()"},{"location":"api/#freetensor.libop.element_wise.add","text":"(Broadcasted) element-wise addition of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"add()"},{"location":"api/#freetensor.libop.element_wise.add_","text":"(Broadcasted) element-wise addition of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"add_()"},{"location":"api/#freetensor.libop.element_wise.binary_op","text":"(Broadcasted) any element-wise operation on two tensors and return the result Parameters: op ( Callable ) \u2013 The operation applied to each item a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"binary_op()"},{"location":"api/#freetensor.libop.element_wise.binary_op_","text":"(Broadcasted) any element-wise operation on two tensors. The result is written to another tensor Parameters: op ( Callable ) \u2013 The operation applied to each item a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"binary_op_()"},{"location":"api/#freetensor.libop.element_wise.ceil","text":"Element-wise ceil of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"ceil()"},{"location":"api/#freetensor.libop.element_wise.ceil_","text":"Element-wise ceil of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor","title":"ceil_()"},{"location":"api/#freetensor.libop.element_wise.ceildiv","text":"(Broadcasted) element-wise rounding-towards-positive-infinity integer division of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"ceildiv()"},{"location":"api/#freetensor.libop.element_wise.ceildiv_","text":"(Broadcasted) element-wise rounding-towards-positive-infinity integer division of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"ceildiv_()"},{"location":"api/#freetensor.libop.element_wise.cos","text":"Element-wise cos of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"cos()"},{"location":"api/#freetensor.libop.element_wise.cos_","text":"Element-wise cos of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor","title":"cos_()"},{"location":"api/#freetensor.libop.element_wise.eq","text":"(Broadcasted) element-wise equal of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"eq()"},{"location":"api/#freetensor.libop.element_wise.eq_","text":"(Broadcasted) element-wise equal of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"eq_()"},{"location":"api/#freetensor.libop.element_wise.exp","text":"Element-wise natrual exponent of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"exp()"},{"location":"api/#freetensor.libop.element_wise.exp_","text":"Element-wise natrual exponent of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor","title":"exp_()"},{"location":"api/#freetensor.libop.element_wise.floor","text":"Element-wise floor of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"floor()"},{"location":"api/#freetensor.libop.element_wise.floor_","text":"Element-wise floor of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor","title":"floor_()"},{"location":"api/#freetensor.libop.element_wise.floordiv","text":"(Broadcasted) element-wise rounding-towards-negative-infinity integer division (following Python convention, but not C, recommended for performance) of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"floordiv()"},{"location":"api/#freetensor.libop.element_wise.floordiv_","text":"(Broadcasted) element-wise rounding-towards-negative-infinity integer division (following Python convention, but not C, recommended for performance) of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"floordiv_()"},{"location":"api/#freetensor.libop.element_wise.ge","text":"(Broadcasted) element-wise greater-than-or-equal-to of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"ge()"},{"location":"api/#freetensor.libop.element_wise.ge_","text":"(Broadcasted) element-wise greater-than-or-equal-to of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"ge_()"},{"location":"api/#freetensor.libop.element_wise.gt","text":"(Broadcasted) element-wise greater-than of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"gt()"},{"location":"api/#freetensor.libop.element_wise.gt_","text":"(Broadcasted) element-wise greater-than of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"gt_()"},{"location":"api/#freetensor.libop.element_wise.l_and","text":"(Broadcasted) element-wise logical and of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"l_and()"},{"location":"api/#freetensor.libop.element_wise.l_and_","text":"(Broadcasted) element-wise logical and of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"l_and_()"},{"location":"api/#freetensor.libop.element_wise.l_not","text":"Element-wise logical not of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"l_not()"},{"location":"api/#freetensor.libop.element_wise.l_not_","text":"Element-wise logical not of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor","title":"l_not_()"},{"location":"api/#freetensor.libop.element_wise.l_or","text":"(Broadcasted) element-wise logical or of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"l_or()"},{"location":"api/#freetensor.libop.element_wise.l_or_","text":"(Broadcasted) element-wise logical or of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"l_or_()"},{"location":"api/#freetensor.libop.element_wise.le","text":"(Broadcasted) element-wise less-than-or-equal-to of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"le()"},{"location":"api/#freetensor.libop.element_wise.le_","text":"(Broadcasted) element-wise less-than-or-equal-to of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"le_()"},{"location":"api/#freetensor.libop.element_wise.ln","text":"Element-wise natrual logarithm of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"ln()"},{"location":"api/#freetensor.libop.element_wise.ln_","text":"Element-wise natrual logarithm of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor","title":"ln_()"},{"location":"api/#freetensor.libop.element_wise.lt","text":"(Broadcasted) element-wise less-than of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"lt()"},{"location":"api/#freetensor.libop.element_wise.lt_","text":"(Broadcasted) element-wise less-than of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"lt_()"},{"location":"api/#freetensor.libop.element_wise.max","text":"(Broadcasted) element-wise maximum of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"max()"},{"location":"api/#freetensor.libop.element_wise.max_","text":"(Broadcasted) element-wise maximum of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"max_()"},{"location":"api/#freetensor.libop.element_wise.min","text":"(Broadcasted) element-wise minimum of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"min()"},{"location":"api/#freetensor.libop.element_wise.min_","text":"(Broadcasted) element-wise minimum of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"min_()"},{"location":"api/#freetensor.libop.element_wise.mod","text":"(Broadcasted) element-wise modulo (results are non-negative, following Python convention, but not C, recommended for performance) of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"mod()"},{"location":"api/#freetensor.libop.element_wise.mod_","text":"(Broadcasted) element-wise modulo (results are non-negative, following Python convention, but not C, recommended for performance) of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"mod_()"},{"location":"api/#freetensor.libop.element_wise.mul","text":"(Broadcasted) element-wise multiplication of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"mul()"},{"location":"api/#freetensor.libop.element_wise.mul_","text":"(Broadcasted) element-wise multiplication of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"mul_()"},{"location":"api/#freetensor.libop.element_wise.ne","text":"(Broadcasted) element-wise non-equal of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"ne()"},{"location":"api/#freetensor.libop.element_wise.ne_","text":"(Broadcasted) element-wise non-equal of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"ne_()"},{"location":"api/#freetensor.libop.element_wise.neg","text":"Element-wise negation of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"neg()"},{"location":"api/#freetensor.libop.element_wise.neg_","text":"Element-wise negation of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor","title":"neg_()"},{"location":"api/#freetensor.libop.element_wise.relu","text":"Element-wise ReLU of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"relu()"},{"location":"api/#freetensor.libop.element_wise.relu_","text":"Element-wise ReLU of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor","title":"relu_()"},{"location":"api/#freetensor.libop.element_wise.remainder","text":"(Broadcasted) element-wise remainder (results can be positive or negative, following C convention, but not Python, NOT recommended for performance) of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"remainder()"},{"location":"api/#freetensor.libop.element_wise.remainder_","text":"(Broadcasted) element-wise remainder (results can be positive or negative, following C convention, but not Python, NOT recommended for performance) of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"remainder_()"},{"location":"api/#freetensor.libop.element_wise.round_towards_0_div","text":"(Broadcasted) element-wise rounding-towards-0 integer division (following C convention, but not Python, NOT recommended for performance) of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"round_towards_0_div()"},{"location":"api/#freetensor.libop.element_wise.round_towards_0_div_","text":"(Broadcasted) element-wise rounding-towards-0 integer division (following C convention, but not Python, NOT recommended for performance) of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"round_towards_0_div_()"},{"location":"api/#freetensor.libop.element_wise.sigmoid","text":"Element-wise sigmoid of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"sigmoid()"},{"location":"api/#freetensor.libop.element_wise.sigmoid_","text":"Element-wise sigmoid of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor","title":"sigmoid_()"},{"location":"api/#freetensor.libop.element_wise.sin","text":"Element-wise sin of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"sin()"},{"location":"api/#freetensor.libop.element_wise.sin_","text":"Element-wise sin of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor","title":"sin_()"},{"location":"api/#freetensor.libop.element_wise.sqrt","text":"Element-wise square root of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"sqrt()"},{"location":"api/#freetensor.libop.element_wise.sqrt_","text":"Element-wise square root of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor","title":"sqrt_()"},{"location":"api/#freetensor.libop.element_wise.square","text":"Element-wise square of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"square()"},{"location":"api/#freetensor.libop.element_wise.square_","text":"Element-wise square of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor","title":"square_()"},{"location":"api/#freetensor.libop.element_wise.sub","text":"(Broadcasted) element-wise subtraction of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"sub()"},{"location":"api/#freetensor.libop.element_wise.sub_","text":"(Broadcasted) element-wise subtraction of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"sub_()"},{"location":"api/#freetensor.libop.element_wise.tan","text":"Element-wise tan of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"tan()"},{"location":"api/#freetensor.libop.element_wise.tan_","text":"Element-wise tan of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor","title":"tan_()"},{"location":"api/#freetensor.libop.element_wise.tanh","text":"Element-wise tanh of a tensor and return the result Parameters: x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"tanh()"},{"location":"api/#freetensor.libop.element_wise.tanh_","text":"Element-wise tanh of a tensor. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor out ( VarRef ) \u2013 The result tensor","title":"tanh_()"},{"location":"api/#freetensor.libop.element_wise.truediv","text":"(Broadcasted) element-wise floating-point division of two tensors and return the result Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand Returns: VarRef \u2013 The result tensor","title":"truediv()"},{"location":"api/#freetensor.libop.element_wise.truediv_","text":"(Broadcasted) element-wise floating-point division of two tensors. The result is written to another tensor Parameters: a ( VarRef ) \u2013 Left-hand-side operand b ( VarRef ) \u2013 Right-hand-side operand out ( VarRef ) \u2013 The result tensor","title":"truediv_()"},{"location":"api/#freetensor.libop.element_wise.unary_op","text":"Any element-wise operation on a tensor and return the result Parameters: op ( Callable ) \u2013 The operation applied to each item x ( VarRef ) \u2013 The input tensor Returns: VarRef \u2013 The result tensor","title":"unary_op()"},{"location":"api/#freetensor.libop.element_wise.unary_op_","text":"Any element-wise operation on a tensor. The result is written to another tensor Parameters: op ( Callable ) \u2013 The operation applied to each item x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor","title":"unary_op_()"},{"location":"api/#freetensor.libop.logsumexp","text":"","title":"logsumexp"},{"location":"api/#freetensor.libop.logsumexp.logsumexp","text":"Compute ln sum_i exp(x_i) , where i is along an axis. Return the result. The computation is numerically stabilized. Parameters: x ( VarRef ) \u2013 The input tensor axis ( int ) \u2013 Axis that the reduction is performed along. Negative axis means counting from the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor","title":"logsumexp()"},{"location":"api/#freetensor.libop.logsumexp.logsumexp_","text":"Compute ln sum_i exp(x_i) , where i is along an axis. Write to tensor y . The computation is numerically stabilized. Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axis ( int ) \u2013 Axis that the reduction is performed along. Negative axis means counting from the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True","title":"logsumexp_()"},{"location":"api/#freetensor.libop.matmul","text":"","title":"matmul"},{"location":"api/#freetensor.libop.matmul.einsum","text":"Einstein summation. The result is returned Parameters: fmt ( str ) \u2013 The format string. E.g. \"ik,kj->ij\" represents a matrix multiplcation args ( Sequence[VarRef] ) \u2013 All inputs arguments. E.g. if fmt is \"ik,kj->ij\" , it iterates axis i and k of args[0] , axis k and j of args[1] , axis i and j of the returned value Returns: The result tensor","title":"einsum()"},{"location":"api/#freetensor.libop.matmul.einsum_","text":"Einstein summation. The result is written to the last argument Parameters: fmt ( str ) \u2013 The format string. E.g. \"ik,kj->ij\" represents a matrix multiplcation args ( Sequence[VarRef] ) \u2013 All arguments including inputs and the output. E.g. if fmt is \"ik,kj->ij\" , it iterates axis i and k of args[0] , axis k and j of args[1] , axis i and j of args[2]","title":"einsum_()"},{"location":"api/#freetensor.libop.matmul.gemm","text":"General matrix multiplcation following BLAS convention and return the result It performs Y = alpha tr?(A) @ tr?(B) + C , where @ represents matrix multiplication, tr? represents an optional transposition Parameters: A ( VarRef ) \u2013 The left-hand-side operand of matrix multiplication B ( VarRef ) \u2013 The right-hand-side operand of matrix multiplication C ( VarRef (Optional) ) \u2013 The bias tensor trans_A ( bool ) \u2013 If true, transpose A . Defaults to False trans_B ( bool ) \u2013 If true, transpose B . Defaults to False alpha ( float ) \u2013 Coefficient of tr?(A) @ tr?(B) . Defaults to 1.0 beta ( float ) \u2013 Coefficient of C . Defaults to 1.0 Returns: The resulting tensor","title":"gemm()"},{"location":"api/#freetensor.libop.matmul.gemm_","text":"General matrix multiplcation following BLAS convention. The result is written to an existing tensor It performs Y = alpha tr?(A) @ tr?(B) + C , where @ represents matrix multiplication, tr? represents an optional transposition Parameters: A ( VarRef ) \u2013 The left-hand-side operand of matrix multiplication B ( VarRef ) \u2013 The right-hand-side operand of matrix multiplication C ( VarRef (Optional) ) \u2013 The bias tensor Y ( VarRef ) \u2013 The resulting tensor trans_A ( bool ) \u2013 If true, transpose A . Defaults to False trans_B ( bool ) \u2013 If true, transpose B . Defaults to False alpha ( float ) \u2013 Coefficient of tr?(A) @ tr?(B) . Defaults to 1.0 beta ( float ) \u2013 Coefficient of C . Defaults to 1.0","title":"gemm_()"},{"location":"api/#freetensor.libop.matmul.matmul","text":"Matrix multiplcation. The result is returned Parameters: A ( VarRef ) \u2013 The left-hand-side operand B ( VarRef ) \u2013 The right-hand-side operand Returns: The resulting tensor","title":"matmul()"},{"location":"api/#freetensor.libop.matmul.matmul_","text":"Matrix multiplcation. The result is written to an existing tensor Parameters: A ( VarRef ) \u2013 The left-hand-side operand B ( VarRef ) \u2013 The right-hand-side operand C ( VarRef ) \u2013 The resulting tensor","title":"matmul_()"},{"location":"api/#freetensor.libop.pooling","text":"","title":"pooling"},{"location":"api/#freetensor.libop.pooling.global_avg_pool","text":"Global averaging pooling. The result is returned Parameters follow ONNX convention. Currently only 2-D pooling is supported","title":"global_avg_pool()"},{"location":"api/#freetensor.libop.pooling.global_avg_pool_","text":"Global averaging pooling. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D pooling is supported","title":"global_avg_pool_()"},{"location":"api/#freetensor.libop.pooling.max_pool","text":"Maximum pooling. The result is returned Parameters follow ONNX convention. Currently only 2-D pooling is supported","title":"max_pool()"},{"location":"api/#freetensor.libop.pooling.max_pool_","text":"Maximum pooling. The result is written to another tensor Parameters follow ONNX convention. Currently only 2-D pooling is supported","title":"max_pool_()"},{"location":"api/#freetensor.libop.reduction","text":"","title":"reduction"},{"location":"api/#freetensor.libop.reduction.all","text":"Reduction of logical and of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor","title":"all()"},{"location":"api/#freetensor.libop.reduction.all_","text":"Reduction of logical and of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True","title":"all_()"},{"location":"api/#freetensor.libop.reduction.any","text":"Reduction of logical or of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor","title":"any()"},{"location":"api/#freetensor.libop.reduction.any_","text":"Reduction of logical or of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True","title":"any_()"},{"location":"api/#freetensor.libop.reduction.reduce_max","text":"Maximum of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor","title":"reduce_max()"},{"location":"api/#freetensor.libop.reduction.reduce_max_","text":"Maximum of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True","title":"reduce_max_()"},{"location":"api/#freetensor.libop.reduction.reduce_min","text":"Minimum of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor","title":"reduce_min()"},{"location":"api/#freetensor.libop.reduction.reduce_min_","text":"Minimum of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True","title":"reduce_min_()"},{"location":"api/#freetensor.libop.reduction.reduce_prod","text":"Product of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor","title":"reduce_prod()"},{"location":"api/#freetensor.libop.reduction.reduce_prod_","text":"Product of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True","title":"reduce_prod_()"},{"location":"api/#freetensor.libop.reduction.reduce_sum","text":"Sum of a tensor through one or more dimensions and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True Returns: VarRef \u2013 The result tensor","title":"reduce_sum()"},{"location":"api/#freetensor.libop.reduction.reduce_sum_","text":"Sum of a tensor through one or more dimensions. The result is written to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axes ( Sequence[int] (Optional) ) \u2013 Which dimensions to reduce through. Defaults to None, standing for all dimensions, i.e., reduce the tensor to a scalar. Negative axis means counting form the last dimension keepdims ( bool (Optional) ) \u2013 Keep the reduced dimensions as singleton dimensions. Defaults to True","title":"reduce_sum_()"},{"location":"api/#freetensor.libop.reshape","text":"","title":"reshape"},{"location":"api/#freetensor.libop.reshape.expand","text":"Broadcast a tensor to a given shape, following the broadcasting rules Parameters: a ( VarRef ) \u2013 The input tensor b ( Sequence of expressions ) \u2013 The broadcasted shape Returns: The broadcasted tensor","title":"expand()"},{"location":"api/#freetensor.libop.reshape.expand_","text":"Broadcast a tensor to an existing tensor, following the broadcasting rules Parameters: a ( VarRef ) \u2013 The input tensor b ( VarRef ) \u2013 The broadcasted tensor","title":"expand_()"},{"location":"api/#freetensor.libop.reshape.flatten","text":"Flatten a tensor to have two dimensions, and return the result NOTE: This function follows the ONNX convension that reshapes to 2-D instead of 1-D. Parameters: x ( VarRef ) \u2013 The input tensor axis ( int ) \u2013 The result tensor will have 2 dimensions. All dimensions up to axis (inclusive) will be flattend to the first dimension. All dimensions after axis (exclusive) will be flatten to the second dimension. Negative axis means counting form the last dimension Returns: VarRef \u2013 The result tensor","title":"flatten()"},{"location":"api/#freetensor.libop.reshape.flatten_","text":"Flatten a tensor to have two dimensions, and write to another tensor NOTE: This function follows the ONNX convension that reshapes to 2-D instead of 1-D. Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axis ( int ) \u2013 The result tensor will have 2 dimensions. All dimensions up to axis (inclusive) will be flattend to the first dimension. All dimensions after axis (exclusive) will be flatten to the second dimension. Negative axis means counting form the last dimension","title":"flatten_()"},{"location":"api/#freetensor.libop.reshape.flatten_onnx","text":"Flatten a tensor to have two dimensions, and return the result NOTE: This function follows the ONNX convension that reshapes to 2-D instead of 1-D. Parameters: x ( VarRef ) \u2013 The input tensor axis ( int ) \u2013 The result tensor will have 2 dimensions. All dimensions up to axis (inclusive) will be flattend to the first dimension. All dimensions after axis (exclusive) will be flatten to the second dimension. Negative axis means counting form the last dimension Returns: VarRef \u2013 The result tensor","title":"flatten_onnx()"},{"location":"api/#freetensor.libop.reshape.flatten_onnx_","text":"Flatten a tensor to have two dimensions, and write to another tensor NOTE: This function follows the ONNX convension that reshapes to 2-D instead of 1-D. Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axis ( int ) \u2013 The result tensor will have 2 dimensions. All dimensions up to axis (inclusive) will be flattend to the first dimension. All dimensions after axis (exclusive) will be flatten to the second dimension. Negative axis means counting form the last dimension","title":"flatten_onnx_()"},{"location":"api/#freetensor.libop.reshape.flatten_pytorch","text":"Flatten a tensor to have fewer dimensions, and return the result NOTE: This function follows the PyTorch convension Parameters: x ( VarRef ) \u2013 The input tensor start_dim, end_dim ( int (Optional) ) \u2013 All dimensions ranging from start_dim and end_dim (inclusive) will be flattend to 1-D. Negative axis means counting form the last dimension Returns: VarRef \u2013 The result tensor","title":"flatten_pytorch()"},{"location":"api/#freetensor.libop.reshape.flatten_pytorch_","text":"Flatten a tensor to have fewer dimensions, and write to another tensor NOTE: This function follows the PyTorch convension Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor start_dim, end_dim ( int (Optional) ) \u2013 All dimensions ranging from start_dim and end_dim (inclusive) will be flattend to 1-D. Negative axis means counting form the last dimension","title":"flatten_pytorch_()"},{"location":"api/#freetensor.libop.reshape.reshape","text":"Reshape a tensor into a different shape with the same size This operator will try to generate nested loops instead of looping over all elements in a plain loop, so schedules can be better applied. It guarantees to generates loops in the following cases: Splitting a dimension. E.g. 4 to 2x2, and there will be a 2x2 loop nest. Merging dimensions. E.g. 2x2 to 4, and there will be a 2x2 loop nest. Each non-affecting dimension will be iterated by a unique loop. E.g. 3x5x7 to 5x3x7, and there will be a 15x7 loop nest, where the \"7\" dimension will be iterated by a unique loop. Parameters: x ( VarRef ) \u2013 The input tensor shape ( list of expression ) \u2013 The target shape Returns: VarRef \u2013 The result tensor","title":"reshape()"},{"location":"api/#freetensor.libop.reshape.reshape_","text":"Fill a tensor into another tensor with the same size but maybe different shape This operator will try to generate nested loops instead of looping over all elements in a plain loop, so schedules can be better applied. It guarantees to generates loops in the following cases: Splitting a dimension. E.g. 4 to 2x2, and there will be a 2x2 loop nest. Merging dimensions. E.g. 2x2 to 4, and there will be a 2x2 loop nest. Each non-affecting dimension will be iterated by a unique loop. E.g. 3x5x7 to 5x3x7, and there will be a 15x7 loop nest, where the \"7\" dimension will be iterated by a unique loop. Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor","title":"reshape_()"},{"location":"api/#freetensor.libop.reshape.squeeze","text":"Remove singleton dimensions from a tensor, and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] ) \u2013 Dimension numbers of the singleton dimensions. Negative axis means counting from the last dimension Returns: VarRef \u2013 The resulting tensor","title":"squeeze()"},{"location":"api/#freetensor.libop.reshape.squeeze_","text":"Remove singleton dimensions from a tensor, and write the result to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The resulting tensor axes ( Sequence[int] ) \u2013 Dimension numbers of the singleton dimensions. Negative axis means counting from the last dimension","title":"squeeze_()"},{"location":"api/#freetensor.libop.reshape.unsqueeze","text":"Insert singleton dimensions to a tensor, and return the result Parameters: x ( VarRef ) \u2013 The input tensor axes ( Sequence[int] ) \u2013 Dimension numbers of the new singleton dimensions. Negative axis means counting from the last dimension Returns: VarRef \u2013 The resulting tensor","title":"unsqueeze()"},{"location":"api/#freetensor.libop.reshape.unsqueeze_","text":"Insert singleton dimensions to a tensor, and write the result to another tensor Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The resulting tensor axes ( Sequence[int] ) \u2013 Dimension numbers of the new singleton dimensions. Negative axis means counting from the last dimension","title":"unsqueeze_()"},{"location":"api/#freetensor.libop.softmax","text":"","title":"softmax"},{"location":"api/#freetensor.libop.softmax.softmax","text":"Softmax of tensor x along an axis and return the result The computation is numerically stabilized. Parameters: x ( VarRef ) \u2013 The input tensor axis ( int (Optional) ) \u2013 Axis that the softmax is performed along. Negative axis means counting from the last dimension Returns: The result tensor","title":"softmax()"},{"location":"api/#freetensor.libop.softmax.softmax_","text":"Softmax of tensor x along an axis, and write to tensor y The computation is numerically stabilized. Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The result tensor axis ( int ) \u2013 Axis that the softmax is performed along. Negative axis means counting from the last dimension","title":"softmax_()"},{"location":"api/#freetensor.libop.transpose","text":"","title":"transpose"},{"location":"api/#freetensor.libop.transpose.transpose","text":"Transposition (out-of-place) The perm[i] -th dimension of the input becomes the i -th dimension of the output Parameters: x ( VarRef ) \u2013 The input tensor perm ( Sequence[int] ) \u2013 Permutation of the dimensions. Negative values mean counting form the last dimension. By default reversing all dimensions Returns: VarRef \u2013 The output tensor","title":"transpose()"},{"location":"api/#freetensor.libop.transpose.transpose_","text":"Transposition (in-place) The perm[i] -th dimension of the input becomes the i -th dimension of the output Parameters: x ( VarRef ) \u2013 The input tensor y ( VarRef ) \u2013 The output tensor perm ( Sequence[int] ) \u2013 Permutation of the dimensions. Negative values mean counting form the last dimension. By default reversing all dimensions","title":"transpose_()"},{"location":"about/contrib/","text":"Contributing \u00b6 Pull Requests are welcome! Please configure (or install some plugins for) your editor, to support clang-format , yapf and editorconfig , for code formating. And please note that we use different naming styles in Python and C++ parts.","title":"Contributing"},{"location":"about/pub/","text":"Publication \u00b6 Shizhi Tang, Jidong Zhai, Haojie Wang, Lin Jiang, Liyan Zheng, Zhenhao Yuan, and Chen Zhang. 2022. FreeTensor: A Free-Form DSL with Holistic Optimizations for Irregular Tensor Programs. In Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation (PLDI \u201922), June 13-17, 2022, San Diego, CA, USA . ACM, New York, NY, USA, 16 pages. https://doi.org/10.1145/3519939.3523448. ( Download ) @inproceedings{10.1145/3519939.3523448, author = {Tang, Shizhi and Zhai, Jidong and Wang, Haojie and Jiang, Lin and Zheng, Liyan and Yuan, Zhenhao and Zhang, Chen}, title = {FreeTensor: A Free-Form DSL with Holistic Optimizations for Irregular Tensor Programs}, year = {2022}, isbn = {9781450392655}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3519939.3523448}, doi = {10.1145/3519939.3523448}, booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation}, pages = {872\u2013887}, numpages = {16}, keywords = {tensor computing, optimizing compilers, DSL}, location = {San Diego, CA, USA}, series = {PLDI 2022} } Evaluation code can be found in this repository . NOTE: API of FreeTensor has been changed since submission. To reproduce the exact result in the paper, please consider the Artifact Evaluation version of FreeTensor, published here .","title":"Publication"},{"location":"guide/","text":"Get Started \u00b6 Build and Run Your First Program with FreeTenor Optimize a Program with Schedules Optimize a Program with Hints Running on a GPU Automatic Differentiation","title":"Get Started"},{"location":"guide/ad/","text":"Automatic Differentiation \u00b6 Reverse-Mode AD Providing Your Custom Gradients Why or When do We Need Custom Gradients How to Write Custom Gradients in FreeTensor Additional Descriptions on push_for_backward Automatic Differentiation (AD) transforms a program to another program that computes the original one's derivative or gradient. FreeTensor supports Reverse-Mode AD, and there is a plan to support Forward-Mode AD in the future. Reverse-Mode AD \u00b6 Suppose there is a program x -> y -> z -> w that computes an output w from intermediate variables z and y , and an input variable x . Reverse-Mode AD generates a gradient program dw/dw=1 -> dw/dz -> dw/dy -> dw/dx that computes dw/dx by Chain Rule. y , z and w may be saved in a \"tape\" when evaluation the original program, to be reused in the gradient one. If FreeTensor is built with WITH_PYTORCH=ON , you can skip this section and turn to the @optimize_to_pytorch integration , which integrates seamlessly with PyTorch's autograd mechanism, but will incur some runtime overhead. Here is an example of Reverse-Mode AD in FreeTensor: import freetensor as ft import numpy as np n = 4 @ft.optimize @ft.grad(requires=['a', 'b'], provides=[ft.Return()], attach_backward=True) def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.zeros((), \"float32\") for i in range(n): y[()] += a[i] * b[i] return y a = np.array([0, 1, 2, 3], dtype=\"float32\") b = np.array([3, 2, 1, 0], dtype=\"float32\") y = test(a, b) print(y.numpy()) dzdy = np.array(1, dtype='float32') input_grads = test.input_name_to_gradient_name output_grads = test.output_name_to_gradient_name dzda, dzdb = test.backward( **{output_grads[ft.Return()]: dzdy})[input_grads['a'], input_grads['b']] print(dzda.numpy()) print(dzdb.numpy()) You need to call ft.grad (or the inplace version ft.grad_ ) to generate a forward function and a backward function. In this example, the backward function is attached as the test.backward property because attach_backward is set to True . You can set it to False and ft.grad will return both functions. Please note that test is updated by ft.grad and becomes different than the original function, as it may save some intermediate tensors to a global tape , and it must be executed before the backward test.backward . Note on JIT JIT is only supported when attach_backward = True . After that, you call ft.optimize to optimize and compile the program just as in previous examples. This time it is done for both test and test.backward . Finally, you execute test and test.backward . The parameters and return values of test.backward are the gradients of a , b and y , which have their own names. To set and get these parameters and return values, you look up for them in two dictionaries test.input_name_to_gradient_name and test.output_name_to_gradient_name (in type ft.ParamRetDict . These two dictionaries accept either a name of a parameter, or a special ft.Return to specify a return value. When invoking test.backward , parameters can be set via keyword arguments, and return values can be collect via a bracket (from a special type ft.ReturnValuesPack ). These two maps are attached to test because attach_backward is True . Otherwise, they are returned as return values from ft.grad . Intermediate variables are not always have to be saved to the \"tape\" from the forward function. If a variable is need in the backward function but not saved, it will be re-computed, which is sometimes even faster than saving it due to better locality. By default, FreeTensor uses heuristics to determine which variable to save. To get better performance, you may want to control which intermediate variables should be saved by setting an optional tapes parameter in ft.grad . tapes can either be a different mode, or a explicit list of AST node IDs of all VarDef nodes of the variables you want to save. Providing Your Custom Gradients \u00b6 Why or When do We Need Custom Gradients \u00b6 Sometimes neither reverse-mode or forward-mode AD produces the most elegant form of gradients. FreeTensor allows you to provide your own gradients for part of the program. Take softmax as an example: The \\(\\mathbf{y} = softmax(\\mathbf{x})\\) function is mathematically defined by the following steps: \\[\\begin{align} e_i &= \\mathrm{e}^{x_i} \\label{eq:softmax-1} \\\\ s &= \\sum_i{e_i} \\label{eq:softmax-2} \\\\ y_i &= \\frac{e_i}{s} \\label{eq:softmax-3} \\end{align}\\] Suppose the final output of the program (the loss) is \\(z\\) . If using reverse-mode AD, the gradient of the input: \\(\\frac{\\partial z}{\\partial x}\\) can be computed by the following steps: \\[\\begin{align} \\frac{\\partial z}{\\partial s} &= -\\sum_i{\\frac{\\partial z}{\\partial y_i} \\frac{y_i}{s}} \\label{eq:softmax-grad-1} \\\\ \\frac{\\partial z}{\\partial e_i} &= \\frac{\\partial z}{\\partial y_i} \\frac{1}{s} + \\frac{\\partial z}{\\partial s} \\label{eq:softmax-grad-2} \\\\ \\frac{\\partial z}{\\partial x_i} &= \\frac{\\partial z}{\\partial e_i} e_i \\label{eq:softmax-grad-3} \\end{align}\\] However, usually we can NOT compute softmax by Equation \\(\\eqref{eq:softmax-1}\\eqref{eq:softmax-2}\\eqref{eq:softmax-3}\\) for numerical stability issues. Pratically, we compute softmax with additional normalization on \\(\\mathbf{x}\\) : \\[\\begin{align} m &= \\max_i{x_i} \\label{eq:softmax-norm-1} \\\\ e_i &= \\mathrm{e}^{x_i - m} \\label{eq:softmax-norm-2} \\\\ s &= \\sum_i{e_i} \\label{eq:softmax-norm-3} \\\\ y_i &= \\frac{e_i}{s} \\label{eq:softmax-norm-4} \\end{align}\\] If we directly apply reverse-mode AD on Equation \\(\\eqref{eq:softmax-norm-1}\\eqref{eq:softmax-norm-2}\\eqref{eq:softmax-norm-3}\\eqref{eq:softmax-norm-4}\\) , the backward program will be like: \\[\\begin{align} \\frac{\\partial z}{\\partial s} &= -\\sum_i{\\frac{\\partial z}{\\partial y_i} \\frac{y_i}{s}} \\\\ \\frac{\\partial z}{\\partial e_i} &= \\frac{\\partial z}{\\partial y_i} \\frac{1}{s} + \\frac{\\partial z}{\\partial s} \\\\ \\frac{\\partial z}{\\partial m} &= -\\sum_i{\\frac{\\partial z}{\\partial e_i}e_i} \\\\ \\frac{\\partial z}{\\partial x_i} &= \\frac{\\partial z}{\\partial e_i} e_i + \\begin{cases}\\frac{\\partial z}{\\partial m}, &i = \\arg\\max_j{x_j} \\\\ 0, &i \\neq \\arg\\max_j{x_j}\\end{cases} \\end{align}\\] You may have found that there is an extra \\(\\frac{\\partial z}{\\partial m}\\) involved. Apparently, the gradient should be the same no matter if we do the normalization. This is because \\(\\frac{\\partial z}{\\partial m}\\) actually always equals to \\(0\\) . FreeTensor can not dig out this mathematical property, so the computation on \\(\\frac{\\partial z}{\\partial m}\\) will remain and will be wasted. How to Write Custom Gradients in FreeTensor \u00b6 The following examples will demonstrate how to provide your own custom gradients, to override the default AD behaviour. Please note that this is only for demonstration. If you are just going to use softmax, call it from libop.softmax , which has already implemented the following code. First we show a softmax implementation with full AD: import freetensor as ft import torch n = 4 @ft.optimize # Set verbose=1 to see the code @ft.grad(requires=['x'], provides=[ft.Return()], attach_backward=True) def test(x: ft.Var[(n,), \"float32\"]): # Automatically decide gradients for this statement m = ft.reduce_max(x, axes=[-1]) e = ft.exp(x - m) s = ft.reduce_sum(e, axes=[-1]) y = e / s return y # Check forward result x = torch.rand(n, dtype=torch.float32) x.requires_grad = True y_ft = test(x).torch() y_torch = torch.softmax(x, axis=-1) assert torch.all(torch.isclose(y_ft, y_torch)) # Check backward result dzdy = torch.rand(n, dtype=torch.float32) y_torch.grad = dzdy.clone() input_grads = test.input_name_to_gradient_name output_grads = test.output_name_to_gradient_name dzdx_ft = test.backward(**{output_grads[ft.Return()]: dzdy}).torch() y_torch.backward(y_torch.grad) dzdx_torch = x.grad assert torch.all(torch.isclose(dzdx_ft, dzdx_torch, 1e-4, 1e-7)) Then, we add our own gradient to it: import freetensor as ft import torch n = 4 @ft.optimize # Set verbose=1 to see the code @ft.grad(requires=['x'], provides=[ft.Return()], attach_backward=True) def test(x: ft.Var[(n,), \"float32\"]): # Mark the range that you want to provide graident for, with `StmtRange` with ft.StmtRange() as rng: m = ft.reduce_max(x, axes=[-1]) e = ft.exp(x - m) s = ft.reduce_sum(e, axes=[-1]) y = e / s # Call `push_for_backward` so we can use forward values in backward e_now = ft.push_for_backward(e) s_now = ft.push_for_backward(s) y_now = ft.push_for_backward(y) # Define gradient in `UserGrad` with ft.UserGrad(x, y, stmt_range=rng) as (dzdx, dzdy): # Retrieve forward value from `y_now`, NOT `y` dzds = -ft.reduce_sum(dzdy * y_now, axes=[-1]) / s_now dzde = dzdy / s_now + dzds dzdx[...] += dzde * e_now # Use `+=` here return y # Check forward result x = torch.rand(n, dtype=torch.float32) x.requires_grad = True y_ft = test(x).torch() y_torch = torch.softmax(x, axis=-1) assert torch.all(torch.isclose(y_ft, y_torch)) # Check backward result dzdy = torch.rand(n, dtype=torch.float32) y_torch.grad = dzdy.clone() input_grads = test.input_name_to_gradient_name output_grads = test.output_name_to_gradient_name dzdx_ft = test.backward(**{output_grads[ft.Return()]: dzdy}).torch() y_torch.backward(y_torch.grad) dzdx_torch = x.grad assert torch.all(torch.isclose(dzdx_ft, dzdx_torch, 1e-4, 1e-7)) First, we mark the range of code that we want to provide gradient for, with ft.StmtRange , as a name rng . In the range, we write the code to compute softmax as usual. Additionaly, for the values that we want to reuse in the gradient, we call ft.push_for_backward to save it. push_for_backward returns a handle that you can use as a usual tensor in the gradient code. If your StmtRange is inside an outer loop, the handle will always reflect the correct iteration (see the next example). Besides, push_for_backward does not mean the value will be physically saved in tape: it only means the value will be logically reused in the backward, no matter by saving or by recomputing. push_for_backward is orthogonal with the tapes parameter in ft.grad . Next, we define our custom gradient with a ft.UserGrad scope. The scopes receives a special parameter stmt_range , which should be set to the StmtRange we have just defined. Beside stmt_range , UserGrand receives an arbitrary number of parameters, in this case, x and y , and returns the same number of variables, dzdx and dzdy , so we have the mapping between each variable and its gradient. What we are going to do is update dzdx from dzdy . We define our gradient code in the UserGrad code of Equation \\(\\eqref{eq:softmax-grad-1}\\eqref{eq:softmax-grad-2}\\eqref{eq:softmax-grad-3}\\) . We want to use the forward value y , s and e . But do NOT directly use its name, use the push_for_backward handler y_now , s_now and e_now instead. Finally, plase note that we update dzdx with += instead of = , because we may be only computing a partial derivative: there may be other functions of x other than y . And it is all done. Additional Descriptions on push_for_backward \u00b6 We have mentioned push_for_backward will automatically handle multiple versions of a variable. If you are familiar with PyTorch, you may have found the name is similar to PyTorch's save_for_backward . Here, versioning is the major difference: ft.push_for_backward can be called multiple times on a variable, to save multiple version (or snapshot of it), while the variable can keep changing. Here is an additional example: a softmax written in a loop form, where we receives a 2-d input, and apply softmax on the second dimension. Again, this is only for demonstration, and there are multiple ways to implement a softmax. import freetensor as ft import torch n = 4 @ft.optimize # Set verbose=1 to see the code @ft.grad(requires=['x'], provides=[ft.Return()], attach_backward=True) def test(x: ft.Var[(n, n), \"float32\"]): y = ft.empty((n, n), \"float32\") for i in range(n): # Mark the range that you want to provide graident for, with `StmtRange` with ft.StmtRange() as rng: # `m`, `e` and `s` are local to `i` m = ft.reduce_max(x[i], axes=[-1]) e = ft.exp(x[i] - m) s = ft.reduce_sum(e, axes=[-1]) y[i] = e / s # Call `push_for_backward` so we can use forward values in backward e_now = ft.push_for_backward(e) s_now = ft.push_for_backward(s) y_now = ft.push_for_backward(y) # Define gradient in `UserGrad` with ft.UserGrad(x, y, stmt_range=rng) as (dzdx, dzdy): # Retrieve forward value from `y_now`, NOT `y` dzds = -ft.reduce_sum(dzdy[i] * y_now[i], axes=[-1]) / s_now dzde = dzdy[i] / s_now + dzds dzdx[i] += dzde * e_now # Use `+=` here return y # Check forward result x = torch.rand(n, n, dtype=torch.float32) x.requires_grad = True y_ft = test(x).torch() y_torch = torch.softmax(x, axis=-1) assert torch.all(torch.isclose(y_ft, y_torch)) # Check backward result dzdy = torch.rand(n, n, dtype=torch.float32) y_torch.grad = dzdy.clone() input_grads = test.input_name_to_gradient_name output_grads = test.output_name_to_gradient_name dzdx_ft = test.backward(**{output_grads[ft.Return()]: dzdy}).torch() y_torch.backward(y_torch.grad) dzdx_torch = x.grad assert torch.all(torch.isclose(dzdx_ft, dzdx_torch, 1e-4, 1e-7)) Here our gradient scope is inside a loop, where m , e and s are local to the loop iteration. When we load the value from their push_for_backward handlers, we get the version of value at the exact iteration we need.","title":"Automatic Differentiation"},{"location":"guide/ad/#reverse-mode-ad","text":"Suppose there is a program x -> y -> z -> w that computes an output w from intermediate variables z and y , and an input variable x . Reverse-Mode AD generates a gradient program dw/dw=1 -> dw/dz -> dw/dy -> dw/dx that computes dw/dx by Chain Rule. y , z and w may be saved in a \"tape\" when evaluation the original program, to be reused in the gradient one. If FreeTensor is built with WITH_PYTORCH=ON , you can skip this section and turn to the @optimize_to_pytorch integration , which integrates seamlessly with PyTorch's autograd mechanism, but will incur some runtime overhead. Here is an example of Reverse-Mode AD in FreeTensor: import freetensor as ft import numpy as np n = 4 @ft.optimize @ft.grad(requires=['a', 'b'], provides=[ft.Return()], attach_backward=True) def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.zeros((), \"float32\") for i in range(n): y[()] += a[i] * b[i] return y a = np.array([0, 1, 2, 3], dtype=\"float32\") b = np.array([3, 2, 1, 0], dtype=\"float32\") y = test(a, b) print(y.numpy()) dzdy = np.array(1, dtype='float32') input_grads = test.input_name_to_gradient_name output_grads = test.output_name_to_gradient_name dzda, dzdb = test.backward( **{output_grads[ft.Return()]: dzdy})[input_grads['a'], input_grads['b']] print(dzda.numpy()) print(dzdb.numpy()) You need to call ft.grad (or the inplace version ft.grad_ ) to generate a forward function and a backward function. In this example, the backward function is attached as the test.backward property because attach_backward is set to True . You can set it to False and ft.grad will return both functions. Please note that test is updated by ft.grad and becomes different than the original function, as it may save some intermediate tensors to a global tape , and it must be executed before the backward test.backward . Note on JIT JIT is only supported when attach_backward = True . After that, you call ft.optimize to optimize and compile the program just as in previous examples. This time it is done for both test and test.backward . Finally, you execute test and test.backward . The parameters and return values of test.backward are the gradients of a , b and y , which have their own names. To set and get these parameters and return values, you look up for them in two dictionaries test.input_name_to_gradient_name and test.output_name_to_gradient_name (in type ft.ParamRetDict . These two dictionaries accept either a name of a parameter, or a special ft.Return to specify a return value. When invoking test.backward , parameters can be set via keyword arguments, and return values can be collect via a bracket (from a special type ft.ReturnValuesPack ). These two maps are attached to test because attach_backward is True . Otherwise, they are returned as return values from ft.grad . Intermediate variables are not always have to be saved to the \"tape\" from the forward function. If a variable is need in the backward function but not saved, it will be re-computed, which is sometimes even faster than saving it due to better locality. By default, FreeTensor uses heuristics to determine which variable to save. To get better performance, you may want to control which intermediate variables should be saved by setting an optional tapes parameter in ft.grad . tapes can either be a different mode, or a explicit list of AST node IDs of all VarDef nodes of the variables you want to save.","title":"Reverse-Mode AD"},{"location":"guide/ad/#providing-your-custom-gradients","text":"","title":"Providing Your Custom Gradients"},{"location":"guide/ad/#why-or-when-do-we-need-custom-gradients","text":"Sometimes neither reverse-mode or forward-mode AD produces the most elegant form of gradients. FreeTensor allows you to provide your own gradients for part of the program. Take softmax as an example: The \\(\\mathbf{y} = softmax(\\mathbf{x})\\) function is mathematically defined by the following steps: \\[\\begin{align} e_i &= \\mathrm{e}^{x_i} \\label{eq:softmax-1} \\\\ s &= \\sum_i{e_i} \\label{eq:softmax-2} \\\\ y_i &= \\frac{e_i}{s} \\label{eq:softmax-3} \\end{align}\\] Suppose the final output of the program (the loss) is \\(z\\) . If using reverse-mode AD, the gradient of the input: \\(\\frac{\\partial z}{\\partial x}\\) can be computed by the following steps: \\[\\begin{align} \\frac{\\partial z}{\\partial s} &= -\\sum_i{\\frac{\\partial z}{\\partial y_i} \\frac{y_i}{s}} \\label{eq:softmax-grad-1} \\\\ \\frac{\\partial z}{\\partial e_i} &= \\frac{\\partial z}{\\partial y_i} \\frac{1}{s} + \\frac{\\partial z}{\\partial s} \\label{eq:softmax-grad-2} \\\\ \\frac{\\partial z}{\\partial x_i} &= \\frac{\\partial z}{\\partial e_i} e_i \\label{eq:softmax-grad-3} \\end{align}\\] However, usually we can NOT compute softmax by Equation \\(\\eqref{eq:softmax-1}\\eqref{eq:softmax-2}\\eqref{eq:softmax-3}\\) for numerical stability issues. Pratically, we compute softmax with additional normalization on \\(\\mathbf{x}\\) : \\[\\begin{align} m &= \\max_i{x_i} \\label{eq:softmax-norm-1} \\\\ e_i &= \\mathrm{e}^{x_i - m} \\label{eq:softmax-norm-2} \\\\ s &= \\sum_i{e_i} \\label{eq:softmax-norm-3} \\\\ y_i &= \\frac{e_i}{s} \\label{eq:softmax-norm-4} \\end{align}\\] If we directly apply reverse-mode AD on Equation \\(\\eqref{eq:softmax-norm-1}\\eqref{eq:softmax-norm-2}\\eqref{eq:softmax-norm-3}\\eqref{eq:softmax-norm-4}\\) , the backward program will be like: \\[\\begin{align} \\frac{\\partial z}{\\partial s} &= -\\sum_i{\\frac{\\partial z}{\\partial y_i} \\frac{y_i}{s}} \\\\ \\frac{\\partial z}{\\partial e_i} &= \\frac{\\partial z}{\\partial y_i} \\frac{1}{s} + \\frac{\\partial z}{\\partial s} \\\\ \\frac{\\partial z}{\\partial m} &= -\\sum_i{\\frac{\\partial z}{\\partial e_i}e_i} \\\\ \\frac{\\partial z}{\\partial x_i} &= \\frac{\\partial z}{\\partial e_i} e_i + \\begin{cases}\\frac{\\partial z}{\\partial m}, &i = \\arg\\max_j{x_j} \\\\ 0, &i \\neq \\arg\\max_j{x_j}\\end{cases} \\end{align}\\] You may have found that there is an extra \\(\\frac{\\partial z}{\\partial m}\\) involved. Apparently, the gradient should be the same no matter if we do the normalization. This is because \\(\\frac{\\partial z}{\\partial m}\\) actually always equals to \\(0\\) . FreeTensor can not dig out this mathematical property, so the computation on \\(\\frac{\\partial z}{\\partial m}\\) will remain and will be wasted.","title":"Why or When do We Need Custom Gradients"},{"location":"guide/ad/#how-to-write-custom-gradients-in-freetensor","text":"The following examples will demonstrate how to provide your own custom gradients, to override the default AD behaviour. Please note that this is only for demonstration. If you are just going to use softmax, call it from libop.softmax , which has already implemented the following code. First we show a softmax implementation with full AD: import freetensor as ft import torch n = 4 @ft.optimize # Set verbose=1 to see the code @ft.grad(requires=['x'], provides=[ft.Return()], attach_backward=True) def test(x: ft.Var[(n,), \"float32\"]): # Automatically decide gradients for this statement m = ft.reduce_max(x, axes=[-1]) e = ft.exp(x - m) s = ft.reduce_sum(e, axes=[-1]) y = e / s return y # Check forward result x = torch.rand(n, dtype=torch.float32) x.requires_grad = True y_ft = test(x).torch() y_torch = torch.softmax(x, axis=-1) assert torch.all(torch.isclose(y_ft, y_torch)) # Check backward result dzdy = torch.rand(n, dtype=torch.float32) y_torch.grad = dzdy.clone() input_grads = test.input_name_to_gradient_name output_grads = test.output_name_to_gradient_name dzdx_ft = test.backward(**{output_grads[ft.Return()]: dzdy}).torch() y_torch.backward(y_torch.grad) dzdx_torch = x.grad assert torch.all(torch.isclose(dzdx_ft, dzdx_torch, 1e-4, 1e-7)) Then, we add our own gradient to it: import freetensor as ft import torch n = 4 @ft.optimize # Set verbose=1 to see the code @ft.grad(requires=['x'], provides=[ft.Return()], attach_backward=True) def test(x: ft.Var[(n,), \"float32\"]): # Mark the range that you want to provide graident for, with `StmtRange` with ft.StmtRange() as rng: m = ft.reduce_max(x, axes=[-1]) e = ft.exp(x - m) s = ft.reduce_sum(e, axes=[-1]) y = e / s # Call `push_for_backward` so we can use forward values in backward e_now = ft.push_for_backward(e) s_now = ft.push_for_backward(s) y_now = ft.push_for_backward(y) # Define gradient in `UserGrad` with ft.UserGrad(x, y, stmt_range=rng) as (dzdx, dzdy): # Retrieve forward value from `y_now`, NOT `y` dzds = -ft.reduce_sum(dzdy * y_now, axes=[-1]) / s_now dzde = dzdy / s_now + dzds dzdx[...] += dzde * e_now # Use `+=` here return y # Check forward result x = torch.rand(n, dtype=torch.float32) x.requires_grad = True y_ft = test(x).torch() y_torch = torch.softmax(x, axis=-1) assert torch.all(torch.isclose(y_ft, y_torch)) # Check backward result dzdy = torch.rand(n, dtype=torch.float32) y_torch.grad = dzdy.clone() input_grads = test.input_name_to_gradient_name output_grads = test.output_name_to_gradient_name dzdx_ft = test.backward(**{output_grads[ft.Return()]: dzdy}).torch() y_torch.backward(y_torch.grad) dzdx_torch = x.grad assert torch.all(torch.isclose(dzdx_ft, dzdx_torch, 1e-4, 1e-7)) First, we mark the range of code that we want to provide gradient for, with ft.StmtRange , as a name rng . In the range, we write the code to compute softmax as usual. Additionaly, for the values that we want to reuse in the gradient, we call ft.push_for_backward to save it. push_for_backward returns a handle that you can use as a usual tensor in the gradient code. If your StmtRange is inside an outer loop, the handle will always reflect the correct iteration (see the next example). Besides, push_for_backward does not mean the value will be physically saved in tape: it only means the value will be logically reused in the backward, no matter by saving or by recomputing. push_for_backward is orthogonal with the tapes parameter in ft.grad . Next, we define our custom gradient with a ft.UserGrad scope. The scopes receives a special parameter stmt_range , which should be set to the StmtRange we have just defined. Beside stmt_range , UserGrand receives an arbitrary number of parameters, in this case, x and y , and returns the same number of variables, dzdx and dzdy , so we have the mapping between each variable and its gradient. What we are going to do is update dzdx from dzdy . We define our gradient code in the UserGrad code of Equation \\(\\eqref{eq:softmax-grad-1}\\eqref{eq:softmax-grad-2}\\eqref{eq:softmax-grad-3}\\) . We want to use the forward value y , s and e . But do NOT directly use its name, use the push_for_backward handler y_now , s_now and e_now instead. Finally, plase note that we update dzdx with += instead of = , because we may be only computing a partial derivative: there may be other functions of x other than y . And it is all done.","title":"How to Write Custom Gradients in FreeTensor"},{"location":"guide/ad/#additional-descriptions-on-push_for_backward","text":"We have mentioned push_for_backward will automatically handle multiple versions of a variable. If you are familiar with PyTorch, you may have found the name is similar to PyTorch's save_for_backward . Here, versioning is the major difference: ft.push_for_backward can be called multiple times on a variable, to save multiple version (or snapshot of it), while the variable can keep changing. Here is an additional example: a softmax written in a loop form, where we receives a 2-d input, and apply softmax on the second dimension. Again, this is only for demonstration, and there are multiple ways to implement a softmax. import freetensor as ft import torch n = 4 @ft.optimize # Set verbose=1 to see the code @ft.grad(requires=['x'], provides=[ft.Return()], attach_backward=True) def test(x: ft.Var[(n, n), \"float32\"]): y = ft.empty((n, n), \"float32\") for i in range(n): # Mark the range that you want to provide graident for, with `StmtRange` with ft.StmtRange() as rng: # `m`, `e` and `s` are local to `i` m = ft.reduce_max(x[i], axes=[-1]) e = ft.exp(x[i] - m) s = ft.reduce_sum(e, axes=[-1]) y[i] = e / s # Call `push_for_backward` so we can use forward values in backward e_now = ft.push_for_backward(e) s_now = ft.push_for_backward(s) y_now = ft.push_for_backward(y) # Define gradient in `UserGrad` with ft.UserGrad(x, y, stmt_range=rng) as (dzdx, dzdy): # Retrieve forward value from `y_now`, NOT `y` dzds = -ft.reduce_sum(dzdy[i] * y_now[i], axes=[-1]) / s_now dzde = dzdy[i] / s_now + dzds dzdx[i] += dzde * e_now # Use `+=` here return y # Check forward result x = torch.rand(n, n, dtype=torch.float32) x.requires_grad = True y_ft = test(x).torch() y_torch = torch.softmax(x, axis=-1) assert torch.all(torch.isclose(y_ft, y_torch)) # Check backward result dzdy = torch.rand(n, n, dtype=torch.float32) y_torch.grad = dzdy.clone() input_grads = test.input_name_to_gradient_name output_grads = test.output_name_to_gradient_name dzdx_ft = test.backward(**{output_grads[ft.Return()]: dzdy}).torch() y_torch.backward(y_torch.grad) dzdx_torch = x.grad assert torch.all(torch.isclose(dzdx_ft, dzdx_torch, 1e-4, 1e-7)) Here our gradient scope is inside a loop, where m , e and s are local to the loop iteration. When we load the value from their push_for_backward handlers, we get the version of value at the exact iteration we need.","title":"Additional Descriptions on push_for_backward"},{"location":"guide/build-and-run/","text":"Build and Run \u00b6 Dependencies Build Run a Program with FreeTensor Global Configurations Run the Tests Build this Document Dependencies \u00b6 Linux Python (>= 3.8, for the Python frontend) C++ compiler (GCC >= 11 or Clang >= 16, to have enough C++20 support and the \"unroll\" pragma) CUDA (>= 11.4.1, to support GCC 11, Optional, only supported with GCC) MKL (Optional) PyTorch (Optional, see below) Java (= 11, Build-time dependency only) Other Python dependencies: pip3 install --user numpy sourceinspect astor Pygments Note on Python version Because we are analyzing Python AST, which is sensitive to Python version, there may be potential bugs for Python strictly later than 3.8. Please file an issue if something goes wrong Conflict with PyTorch FreeTensor can be used together with PyTorch, and FreeTensor provides an optional integration with PyTorch. With or without this integration, FreeTensor may conflict with PyTorch if they both linked against some common dependencies but these dependencies are of different versions. This conflict can lead to weird error both at compile time and run time, and silent performance drop. Thus we highly recommand to build FreeTensor and PyTorch locally in the same environment. We also provide scripts to build a docker container for this purpose (see below). Build \u00b6 First, clone this repo. Don't forget there are some submodules. git clone --recursive <path/to/this/repo> Then, build. mkdir build cd build cmake .. make -j # Or use Ninja There are some options to cmake : -DFT_WITH_CUDA=ON/OFF : build with/without CUDA (defaults to ON ). -DFT_WITH_MKL=ON/<path/to/mkl/root>/OFF : build with MKL (defaults to OFF ). The path accepts by CMake should be a raw unescaped path; i.e. -DFT_WITH_MKL=\"/some path\" is good since the quotes are resolved by the shell but -DFT_WITH_MKL=\\\"/some\\ path\\\" is not. -DFT_WITH_PYTORCH=ON/OFF : build with/without PyTorch integration (including copy-free interface from/to PyTorch), requring PyTorch installed on the system (defaults to OFF ). -DFT_DEBUG_BLAME_AST=ON (for developers): enables tracing to tell by which pass a specific AST node is modified. -DFT_DEBUG_PROFILE=ON (for developers): profiles some heavy functions in the compiler. -DFT_DEBUG_SANITIZE=<sanitizer_name> (for developers): build with GCC sanitizer (set it to a sanitizer name to use, e.g. address). It will build a shared library with a name like freetensor_ffi.cpython-37m-x86_64-linux-gnu.so , which can be used in Python via import freetensor . Alternatively, you can build our docker images by make -f docker.Makefile <variant> , where <variant> can be: minimal-dev , for -DFT_WITH_CUDA=OFF -DFT_WITH_MKL=OFF -DFT_WITH_PYTORCH=OFF , or cuda-mkl-dev , for -DFT_WITH_CUDA=ON -DFT_WITH_MKL=ON -DFT_WITH_PYTORCH=OFF , or cuda-mkl-pytorch-dev , for -DFT_WITH_CUDA=ON -DFT_WITH_MKL=ON -DFT_WITH_PYTORCH=ON . Run a Program with FreeTensor \u00b6 To run any program with FreeTensor, one should add the python/ and build/ directory to PYTHONPATH first. E.g. to run a python program a.py with FreeTensor in the build/ directory, PYTHONPATH=../python:../build:$PYTHONPATH python3 a.py If you are running our docker images, you don't have to set PYTHONPATH . Global Configurations \u00b6 There are serveral global configurations can be set via environment variables: FT_PRETTY_PRINT=ON/OFF . Enable/disable colored printing. If omitted, FreeTensor will guess this option with runtime information. FT_PRINT_ALL_ID=ON/OFF . Print (or not) IDs of all statements in an AST. Defaults to OFF . FT_PRINT_SOURCE_LOCATION=ON/OFF . Print (or not) Python source location of all statements in an AST. Defaults to OFF . FT_FAST_MATH=ON/OFF . Run (or not) pass/float_simplify optimization pass, and enable (or not) fast math on backend compilers. Defaults to ON . FT_WERROR=ON/OFF . Treat warnings as errors (or not). Defaults to OFF . FT_BACKEND_COMPILER_CXX=<path/to/compiler> . The C++ compiler used to compile the optimized program. Default to the same compiler found when building FreeTensor itself, and compilers found in the PATH enviroment variable. This environment variable should be set to a colon-separated list of paths, in which the paths are searched from left to right. FT_BACKEND_COMPILER_NVCC=<path/to/compiler> . The CUDA compiler used to compile the optimized program (if built with CUDA). Default to the same compiler found when building FreeTensor itself, and compilers found in the PATH enviroment variable. This environment variable should be set to a colon-separated list of paths, in which the paths are searched from left to right. FT_BACKEND_OPENMP . Path to an OpenMP library linked to the optimized program. Default to the same library linked to FreeTensor itself. This environment variable should be set to a colon-separated list of paths, in which the libraries are linked from left to right. FT_DEBUG_RUNTIME_CHECK=ON/OFF . If ON , check out-of-bound access and integer overflow at the generated code at runtime. This option is only for debugging, and will introduce significant runtime overhead. Currently the checker cannot print the error site, please also enable FT_DEBUG_BINARY and then use GDB to locate the error site (by setting a breakpoint on exit ). Defaults to OFF . FT_DEBUG_BINARY=ON/OFF (for developers). If ON , compile with -g at backend. FreeTensor will not delete the binary file after loading it. Defaults to OFF . FT_DEBUG_CUDA_WITH_UM=ON/OFF . If ON , allocate CUDA buffers on Unified Memory, for faster (debugging) access of GPU Array from CPU, but with slower Array allocations and more synchronizations. No performance effect on normal in-kernel computations. Defaults to OFF . This configurations can also set at runtime in ft.config . Run the Tests \u00b6 To run the test, first change into the test/ directory, then PYTHONPATH=../python:../build:$PYTHONPATH pytest To run a single test case, specify the test case name, and optionally use pytest -s to display the standard output. E.g, PYTHONPATH=../python:../build:$PYTHONPATH pytest -s 00.hello_world/test_basic.py::test_hello_world Debugging (for developers) If using GDB, one should invoke PyTest with python3 -m : PYTHONPATH=../python:../build:$PYTHONPATH gdb --args python3 -m pytest If using Valgrind, one should set Python to use the system malloc: PYTHONPATH=../python:../build:$PYTHONPATH PYTHONMALLOC=malloc valgrind python3 -m pytest Sometimes Valgrind is not enough to detect some errors. An alternative is to use the sanitizer from GCC. For example, if you are using the \"address\" sanitizer, first set -DFT_DEBUG_SANITIZE=address to cmake , and then: PYTHONPATH=../python:../build:$PYTHONPATH LD_PRELOAD=`gcc -print-file-name=libasan.so` pytest -s If you are using another sanitizer, change the string set to FT_DEBUG_SANITIZE and the library's name. For example, -DFT_DEBUG_SANITIZE=undefined and libubsan.so . Build this Document \u00b6 First install some dependencies: pip3 install --user mkdocs mkdocstrings==0.18.1 \"pytkdocs[numpy-style]\" From the root directory of FreeTensor, run a HTTP server to serve the document (recommended, but without document on C++ interface due to a limitation ): PYTHONPATH=./python:./build:$PYTHONPATH mkdocs serve Or build and save the pages (with document on C++ interface, requiring Doxygen and Graphviz): doxygen Doxyfile && PYTHONPATH=./python:./build:$PYTHONPATH mkdocs build Publish the documents to GitHub Pages (for developers) doxygen Doxyfile && PYTHONPATH=./python:./build:$PYTHONPATH mkdocs gh-deploy","title":"Build and Run"},{"location":"guide/build-and-run/#dependencies","text":"Linux Python (>= 3.8, for the Python frontend) C++ compiler (GCC >= 11 or Clang >= 16, to have enough C++20 support and the \"unroll\" pragma) CUDA (>= 11.4.1, to support GCC 11, Optional, only supported with GCC) MKL (Optional) PyTorch (Optional, see below) Java (= 11, Build-time dependency only) Other Python dependencies: pip3 install --user numpy sourceinspect astor Pygments Note on Python version Because we are analyzing Python AST, which is sensitive to Python version, there may be potential bugs for Python strictly later than 3.8. Please file an issue if something goes wrong Conflict with PyTorch FreeTensor can be used together with PyTorch, and FreeTensor provides an optional integration with PyTorch. With or without this integration, FreeTensor may conflict with PyTorch if they both linked against some common dependencies but these dependencies are of different versions. This conflict can lead to weird error both at compile time and run time, and silent performance drop. Thus we highly recommand to build FreeTensor and PyTorch locally in the same environment. We also provide scripts to build a docker container for this purpose (see below).","title":"Dependencies"},{"location":"guide/build-and-run/#build","text":"First, clone this repo. Don't forget there are some submodules. git clone --recursive <path/to/this/repo> Then, build. mkdir build cd build cmake .. make -j # Or use Ninja There are some options to cmake : -DFT_WITH_CUDA=ON/OFF : build with/without CUDA (defaults to ON ). -DFT_WITH_MKL=ON/<path/to/mkl/root>/OFF : build with MKL (defaults to OFF ). The path accepts by CMake should be a raw unescaped path; i.e. -DFT_WITH_MKL=\"/some path\" is good since the quotes are resolved by the shell but -DFT_WITH_MKL=\\\"/some\\ path\\\" is not. -DFT_WITH_PYTORCH=ON/OFF : build with/without PyTorch integration (including copy-free interface from/to PyTorch), requring PyTorch installed on the system (defaults to OFF ). -DFT_DEBUG_BLAME_AST=ON (for developers): enables tracing to tell by which pass a specific AST node is modified. -DFT_DEBUG_PROFILE=ON (for developers): profiles some heavy functions in the compiler. -DFT_DEBUG_SANITIZE=<sanitizer_name> (for developers): build with GCC sanitizer (set it to a sanitizer name to use, e.g. address). It will build a shared library with a name like freetensor_ffi.cpython-37m-x86_64-linux-gnu.so , which can be used in Python via import freetensor . Alternatively, you can build our docker images by make -f docker.Makefile <variant> , where <variant> can be: minimal-dev , for -DFT_WITH_CUDA=OFF -DFT_WITH_MKL=OFF -DFT_WITH_PYTORCH=OFF , or cuda-mkl-dev , for -DFT_WITH_CUDA=ON -DFT_WITH_MKL=ON -DFT_WITH_PYTORCH=OFF , or cuda-mkl-pytorch-dev , for -DFT_WITH_CUDA=ON -DFT_WITH_MKL=ON -DFT_WITH_PYTORCH=ON .","title":"Build"},{"location":"guide/build-and-run/#run-a-program-with-freetensor","text":"To run any program with FreeTensor, one should add the python/ and build/ directory to PYTHONPATH first. E.g. to run a python program a.py with FreeTensor in the build/ directory, PYTHONPATH=../python:../build:$PYTHONPATH python3 a.py If you are running our docker images, you don't have to set PYTHONPATH .","title":"Run a Program with FreeTensor"},{"location":"guide/build-and-run/#global-configurations","text":"There are serveral global configurations can be set via environment variables: FT_PRETTY_PRINT=ON/OFF . Enable/disable colored printing. If omitted, FreeTensor will guess this option with runtime information. FT_PRINT_ALL_ID=ON/OFF . Print (or not) IDs of all statements in an AST. Defaults to OFF . FT_PRINT_SOURCE_LOCATION=ON/OFF . Print (or not) Python source location of all statements in an AST. Defaults to OFF . FT_FAST_MATH=ON/OFF . Run (or not) pass/float_simplify optimization pass, and enable (or not) fast math on backend compilers. Defaults to ON . FT_WERROR=ON/OFF . Treat warnings as errors (or not). Defaults to OFF . FT_BACKEND_COMPILER_CXX=<path/to/compiler> . The C++ compiler used to compile the optimized program. Default to the same compiler found when building FreeTensor itself, and compilers found in the PATH enviroment variable. This environment variable should be set to a colon-separated list of paths, in which the paths are searched from left to right. FT_BACKEND_COMPILER_NVCC=<path/to/compiler> . The CUDA compiler used to compile the optimized program (if built with CUDA). Default to the same compiler found when building FreeTensor itself, and compilers found in the PATH enviroment variable. This environment variable should be set to a colon-separated list of paths, in which the paths are searched from left to right. FT_BACKEND_OPENMP . Path to an OpenMP library linked to the optimized program. Default to the same library linked to FreeTensor itself. This environment variable should be set to a colon-separated list of paths, in which the libraries are linked from left to right. FT_DEBUG_RUNTIME_CHECK=ON/OFF . If ON , check out-of-bound access and integer overflow at the generated code at runtime. This option is only for debugging, and will introduce significant runtime overhead. Currently the checker cannot print the error site, please also enable FT_DEBUG_BINARY and then use GDB to locate the error site (by setting a breakpoint on exit ). Defaults to OFF . FT_DEBUG_BINARY=ON/OFF (for developers). If ON , compile with -g at backend. FreeTensor will not delete the binary file after loading it. Defaults to OFF . FT_DEBUG_CUDA_WITH_UM=ON/OFF . If ON , allocate CUDA buffers on Unified Memory, for faster (debugging) access of GPU Array from CPU, but with slower Array allocations and more synchronizations. No performance effect on normal in-kernel computations. Defaults to OFF . This configurations can also set at runtime in ft.config .","title":"Global Configurations"},{"location":"guide/build-and-run/#run-the-tests","text":"To run the test, first change into the test/ directory, then PYTHONPATH=../python:../build:$PYTHONPATH pytest To run a single test case, specify the test case name, and optionally use pytest -s to display the standard output. E.g, PYTHONPATH=../python:../build:$PYTHONPATH pytest -s 00.hello_world/test_basic.py::test_hello_world Debugging (for developers) If using GDB, one should invoke PyTest with python3 -m : PYTHONPATH=../python:../build:$PYTHONPATH gdb --args python3 -m pytest If using Valgrind, one should set Python to use the system malloc: PYTHONPATH=../python:../build:$PYTHONPATH PYTHONMALLOC=malloc valgrind python3 -m pytest Sometimes Valgrind is not enough to detect some errors. An alternative is to use the sanitizer from GCC. For example, if you are using the \"address\" sanitizer, first set -DFT_DEBUG_SANITIZE=address to cmake , and then: PYTHONPATH=../python:../build:$PYTHONPATH LD_PRELOAD=`gcc -print-file-name=libasan.so` pytest -s If you are using another sanitizer, change the string set to FT_DEBUG_SANITIZE and the library's name. For example, -DFT_DEBUG_SANITIZE=undefined and libubsan.so .","title":"Run the Tests"},{"location":"guide/build-and-run/#build-this-document","text":"First install some dependencies: pip3 install --user mkdocs mkdocstrings==0.18.1 \"pytkdocs[numpy-style]\" From the root directory of FreeTensor, run a HTTP server to serve the document (recommended, but without document on C++ interface due to a limitation ): PYTHONPATH=./python:./build:$PYTHONPATH mkdocs serve Or build and save the pages (with document on C++ interface, requiring Doxygen and Graphviz): doxygen Doxyfile && PYTHONPATH=./python:./build:$PYTHONPATH mkdocs build Publish the documents to GitHub Pages (for developers) doxygen Doxyfile && PYTHONPATH=./python:./build:$PYTHONPATH mkdocs gh-deploy","title":"Build this Document"},{"location":"guide/first-program/","text":"Your First Program with FreeTenor \u00b6 Example: Vector addition Declare and Define Tensors Manipulating Tensors Dynamic or Static Just-in-Time (JIT) Compilation Dynamic Tensor Shapes Copy-free interface from/to PyTorch In this page, we introduce some basic concepts of FreeTensor. Example: Vector addition \u00b6 import freetensor as ft import numpy as np n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Here is a basic example program in FreeTensor. You write a Python function that manipulates FreeTensor's tensor type ft.Var , decorate the function with ft.optimize , and finally invoke the decorated function. FreeTensor will generate C++ code for this vector addition, compile it using a native compiler, and finally load it back to Python. Set verbose = 1 to optimize if you are interested in the generated native code. To write such a function, you need to follow some basic concept described in this page. Declare and Define Tensors \u00b6 All tensors, including function parameters, intermediate tensors and return values should be properly declared or defined. Scalars are 0-D tensors in FreeTensor. Declare or define a tensor with an empty shape, and you will get a scalar. Function parameters should be declared like x : ft.Var[shape, data_type] . Declaring a parameter either in the function signature or as a stand-alone statment is acceptable. If your parameter uses another parameter as shape, you will need the latter manner. An optional parameter atype can be set to \"output\" or \"inout\" if you want to mutate a function argument. Intermediate and returning tensors can be created by ft.empty , ft.var or ft.zeros . If you are using FreeTensor for GPU computing, an optional parameter mtype can be set to specify where to store the tensor. It defaults to the main memory of your currently chosen computing device. All tensors and their slices are implemented by an internal ft.VarRef type. If you are looking for a tensor's API, ft.VarRef is the right place. Manipulating Tensors \u00b6 To read or write tensors in a function, just write for ... in range(...) loops that iterate through elements in the tensors, and do arithmetic operations on them. We also provide some functions that operates on a whole tensor or a tensor slice in libop . Special note on tensor assignments We follow Python convention for tensor assignments, but sometimes it is a little counterintuitive. Suppose you have two list s in Python: a and b . a = b replaces the object a with the object b , while a[...] = b assigns data in b to a . FreeTensor does not support replacing a tensor object with another one. It supports assignments only. Therefore, we need to write a[...] = b to assign tensor. a[:] = b (for non-scalars), a[None] = b and a[()] = b is also supported. Dynamic or Static \u00b6 Another concept is that statements and expressions in your program are divided into two categories: dynamic and static . Dynamic statements or expressions are restricted to a small subset of Python, and are compiled to native code. Static statements or expressions can be any Python statements or expressions, and are executed before compilation. In other words, static statements or expressions are like macros or templates in C++, while dynamic ones are actually quotations in Multi-Stage Programming . The following statements and expressions are considered dynamic: Declarations, definitions and operations of FreeTensor's tensor type ft.Var (or its internal implementation ft.VarRef ). if statements, for ... in range(...) and assert statements that have a ft.Var condition or range. All other statements and expressions are considered static. With the help of dynamic and static categories, you can utilize complex Python functions as the static part, while still generate high-performance native code using dynamic loops. For example, the following code combines static and dynamic code to sum multiple vectors together: import freetensor as ft import numpy as np n = 4 @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"], c: ft.Var[(n,), \"int32\"]): inputs = [a, b, c] # Static y = ft.empty((n,), \"int32\") # Dynamic for i in range(n): # Dyanmic y[i] = 0 # Dynamic for item in inputs: # Static y[i] += item[i] # Dynamic return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\"), np.array([3, 4, 5, 6], dtype=\"int32\")).numpy() print(y) However, there might be some counterintuitive behaviours when using static statments or expressions. Please remember that static static statements or expressions are executed before compilation, so the following piece of code will result in a list containing only one item: the expression i , instead of 10 numbers: lst = [] for i in range(10): # Dynamic lst.append(i) # Static. Appends only once Just-in-Time (JIT) Compilation \u00b6 In the example of vector addition above, we support any vector length, but only in a static way. This means each time you change the vector length n , you need to recompile (run optimize again) the function. You may find it inconvenient because you need to write your own code to control whether or when to do the re-compilation, and the compilation code is entangled with the computation code. Therefore, FreeTensor supports automating this recompilation procedure, which can be considered as Just-in-Time (JIT) compilation. To enable this feature, just declare n as an additional parameter, with the type ft.JIT : import freetensor as ft import numpy as np @ft.optimize def test(n: ft.JIT, a, b): # Or `n: ft.JIT[int]` if you like, but it is only for documentation a: ft.Var[(n,), \"int32\"] # After the function signature to use `n` b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(4, np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) For each different n you pass, test will be automatically recompiled. The compiled test for the same n will be memoized, so the same instance will not be repelated compiled. You can also custom the memoization by setting the jit_cache parameter of ft.optimize . Note on parameter declaration You may have note that it is non-trivial to include a parameter n in other parameters a and b 's type annotation. Python actually forbids such a declaration if a and b 's type annotation is inside the function signature. To cope with this restriction, FreeTensor allows declaring a and b 's type AFTER the function sigature as statements. But this off-sigature annoation is only supported for ft.Var types, NOT ft.JIT . Dynamic Tensor Shapes \u00b6 Frequent recompilation does not meet many requirements, so FreeTensor also supports defining tensors with dynamic shapes, just by setting their shapes to a dynamic values. The following code shows an example: import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] # After the function signature to use `n` b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) assert np.array_equal(y, [3, 5, 7, 9]) In this way, in only have to compile your program once. You will expect a single but longer compiling time, and some optimizations are not possible with dynamic shapes. Copy-free interface from/to PyTorch \u00b6 If FreeTensor is built with WITH_PYTORCH=ON , you can directly pass PyTorch tensors to or get them from FreeTensor. For example, import freetensor as ft import torch n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(torch.tensor([1, 2, 3, 4], dtype=torch.int32), torch.tensor([2, 3, 4, 5], dtype=torch.int32)).torch() print(y) FreeTensor also supports integration with PyTorch's \"function\" interface. You can use @ft.optimize_to_pytorch to directly generate a PyTorch \"function\" (specifically, a function wrapper around PyTorch's Function.invoke , just like usual PyTorch functions). This approach seamlessly integrates with PyTorch's autograd mechanism, but incurs some more runtime overhead. Please also note that, because we do not know whether we need to do autograd and which input tensors need gradients until we first run a function, compiling of the FreeTensor code will be delayed to run time. The compiled binary code will be cached and reused if following runs requires the same set of inputs to be derived. The following code shows an example of this approach: import freetensor as ft import torch n = 4 # Change this line to ft.optimize_to_pytorch(verbose=1) to see the resulting # native code @ft.optimize_to_pytorch def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.empty((n,), \"float32\") for i in range(n): y[i] = a[i] * b[i] return y # Forward a = torch.tensor([1, 2, 3, 4], requires_grad=True, dtype=torch.float32) b = torch.tensor([2, 3, 4, 5], requires_grad=True, dtype=torch.float32) y = test(a, b) print(\"y = \", y) # Backward y.grad = torch.tensor([1, 1, 1, 1], dtype=torch.float32) y.backward(y.grad) print(\"a.grad = \", a.grad) print(\"b.grad = \", b.grad)","title":"Your First Program with FreeTenor"},{"location":"guide/first-program/#example-vector-addition","text":"import freetensor as ft import numpy as np n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Here is a basic example program in FreeTensor. You write a Python function that manipulates FreeTensor's tensor type ft.Var , decorate the function with ft.optimize , and finally invoke the decorated function. FreeTensor will generate C++ code for this vector addition, compile it using a native compiler, and finally load it back to Python. Set verbose = 1 to optimize if you are interested in the generated native code. To write such a function, you need to follow some basic concept described in this page.","title":"Example: Vector addition"},{"location":"guide/first-program/#declare-and-define-tensors","text":"All tensors, including function parameters, intermediate tensors and return values should be properly declared or defined. Scalars are 0-D tensors in FreeTensor. Declare or define a tensor with an empty shape, and you will get a scalar. Function parameters should be declared like x : ft.Var[shape, data_type] . Declaring a parameter either in the function signature or as a stand-alone statment is acceptable. If your parameter uses another parameter as shape, you will need the latter manner. An optional parameter atype can be set to \"output\" or \"inout\" if you want to mutate a function argument. Intermediate and returning tensors can be created by ft.empty , ft.var or ft.zeros . If you are using FreeTensor for GPU computing, an optional parameter mtype can be set to specify where to store the tensor. It defaults to the main memory of your currently chosen computing device. All tensors and their slices are implemented by an internal ft.VarRef type. If you are looking for a tensor's API, ft.VarRef is the right place.","title":"Declare and Define Tensors"},{"location":"guide/first-program/#manipulating-tensors","text":"To read or write tensors in a function, just write for ... in range(...) loops that iterate through elements in the tensors, and do arithmetic operations on them. We also provide some functions that operates on a whole tensor or a tensor slice in libop . Special note on tensor assignments We follow Python convention for tensor assignments, but sometimes it is a little counterintuitive. Suppose you have two list s in Python: a and b . a = b replaces the object a with the object b , while a[...] = b assigns data in b to a . FreeTensor does not support replacing a tensor object with another one. It supports assignments only. Therefore, we need to write a[...] = b to assign tensor. a[:] = b (for non-scalars), a[None] = b and a[()] = b is also supported.","title":"Manipulating Tensors"},{"location":"guide/first-program/#dynamic-or-static","text":"Another concept is that statements and expressions in your program are divided into two categories: dynamic and static . Dynamic statements or expressions are restricted to a small subset of Python, and are compiled to native code. Static statements or expressions can be any Python statements or expressions, and are executed before compilation. In other words, static statements or expressions are like macros or templates in C++, while dynamic ones are actually quotations in Multi-Stage Programming . The following statements and expressions are considered dynamic: Declarations, definitions and operations of FreeTensor's tensor type ft.Var (or its internal implementation ft.VarRef ). if statements, for ... in range(...) and assert statements that have a ft.Var condition or range. All other statements and expressions are considered static. With the help of dynamic and static categories, you can utilize complex Python functions as the static part, while still generate high-performance native code using dynamic loops. For example, the following code combines static and dynamic code to sum multiple vectors together: import freetensor as ft import numpy as np n = 4 @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"], c: ft.Var[(n,), \"int32\"]): inputs = [a, b, c] # Static y = ft.empty((n,), \"int32\") # Dynamic for i in range(n): # Dyanmic y[i] = 0 # Dynamic for item in inputs: # Static y[i] += item[i] # Dynamic return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\"), np.array([3, 4, 5, 6], dtype=\"int32\")).numpy() print(y) However, there might be some counterintuitive behaviours when using static statments or expressions. Please remember that static static statements or expressions are executed before compilation, so the following piece of code will result in a list containing only one item: the expression i , instead of 10 numbers: lst = [] for i in range(10): # Dynamic lst.append(i) # Static. Appends only once","title":"Dynamic or Static"},{"location":"guide/first-program/#just-in-time-jit-compilation","text":"In the example of vector addition above, we support any vector length, but only in a static way. This means each time you change the vector length n , you need to recompile (run optimize again) the function. You may find it inconvenient because you need to write your own code to control whether or when to do the re-compilation, and the compilation code is entangled with the computation code. Therefore, FreeTensor supports automating this recompilation procedure, which can be considered as Just-in-Time (JIT) compilation. To enable this feature, just declare n as an additional parameter, with the type ft.JIT : import freetensor as ft import numpy as np @ft.optimize def test(n: ft.JIT, a, b): # Or `n: ft.JIT[int]` if you like, but it is only for documentation a: ft.Var[(n,), \"int32\"] # After the function signature to use `n` b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(4, np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) For each different n you pass, test will be automatically recompiled. The compiled test for the same n will be memoized, so the same instance will not be repelated compiled. You can also custom the memoization by setting the jit_cache parameter of ft.optimize . Note on parameter declaration You may have note that it is non-trivial to include a parameter n in other parameters a and b 's type annotation. Python actually forbids such a declaration if a and b 's type annotation is inside the function signature. To cope with this restriction, FreeTensor allows declaring a and b 's type AFTER the function sigature as statements. But this off-sigature annoation is only supported for ft.Var types, NOT ft.JIT .","title":"Just-in-Time (JIT) Compilation"},{"location":"guide/first-program/#dynamic-tensor-shapes","text":"Frequent recompilation does not meet many requirements, so FreeTensor also supports defining tensors with dynamic shapes, just by setting their shapes to a dynamic values. The following code shows an example: import freetensor as ft import numpy as np @ft.optimize def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] # After the function signature to use `n` b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) assert np.array_equal(y, [3, 5, 7, 9]) In this way, in only have to compile your program once. You will expect a single but longer compiling time, and some optimizations are not possible with dynamic shapes.","title":"Dynamic Tensor Shapes"},{"location":"guide/first-program/#copy-free-interface-fromto-pytorch","text":"If FreeTensor is built with WITH_PYTORCH=ON , you can directly pass PyTorch tensors to or get them from FreeTensor. For example, import freetensor as ft import torch n = 4 # Change this line to ft.optimize(verbose=1) to see the resulting native code @ft.optimize def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") for i in range(n): y[i] = a[i] + b[i] return y y = test(torch.tensor([1, 2, 3, 4], dtype=torch.int32), torch.tensor([2, 3, 4, 5], dtype=torch.int32)).torch() print(y) FreeTensor also supports integration with PyTorch's \"function\" interface. You can use @ft.optimize_to_pytorch to directly generate a PyTorch \"function\" (specifically, a function wrapper around PyTorch's Function.invoke , just like usual PyTorch functions). This approach seamlessly integrates with PyTorch's autograd mechanism, but incurs some more runtime overhead. Please also note that, because we do not know whether we need to do autograd and which input tensors need gradients until we first run a function, compiling of the FreeTensor code will be delayed to run time. The compiled binary code will be cached and reused if following runs requires the same set of inputs to be derived. The following code shows an example of this approach: import freetensor as ft import torch n = 4 # Change this line to ft.optimize_to_pytorch(verbose=1) to see the resulting # native code @ft.optimize_to_pytorch def test(a: ft.Var[(n,), \"float32\"], b: ft.Var[(n,), \"float32\"]): y = ft.empty((n,), \"float32\") for i in range(n): y[i] = a[i] * b[i] return y # Forward a = torch.tensor([1, 2, 3, 4], requires_grad=True, dtype=torch.float32) b = torch.tensor([2, 3, 4, 5], requires_grad=True, dtype=torch.float32) y = test(a, b) print(\"y = \", y) # Backward y.grad = torch.tensor([1, 1, 1, 1], dtype=torch.float32) y.backward(y.grad) print(\"a.grad = \", a.grad) print(\"b.grad = \", b.grad)","title":"Copy-free interface from/to PyTorch"},{"location":"guide/gpu/","text":"Running on a GPU \u00b6 Example: Vector addition on a GPU mtype=\"byvalue\" for Dynamic Tensor Shapes Example: Vector addition on a GPU \u00b6 If FreeTensor is built with a CUDA backend, you can compile your program to a GPU. We still take a vector addition as an example: import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): n = 4 # Add verbose=1 to see the resulting native code @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize('Li', 'threadIdx.x')) def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li # Label the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Similar to parallelizing to OpenMP threads , in this example, we parallelize Loop Li to the threadIdx.x dimension of CUDA. There are two major differences: You are now calling parallelize schedule with a threadIdx.x parameter, instead of openmp . All the code are enclosed by a with ft.GPU(0) scope. Usually, you not only parallelize your loops to threadIdx.x , but also other CUDA dimensions like blockIdx.x . To achieve this, you either parallelize different loops in a loop nests to different CUDA dimensions, or split your loops before parallelizing them. As for the with ft.GPU(0) scope, ft.GPU(0) specifies a Device (a specific hardware device of GPU). By calling with on a device, default values of several classes and functions are set, but currently you only need to be aware of two things: It sets the Device of optimize . It sets the default mtype of all tensors in the program, which is an optional parameter of ft.Var , ft.empty , etc. mtype refers to memory type. It controls where a tensor is stored. It defaults to \"cpu\" for a CPU program, and \"gpu/global\" for a GPU program. You probably GPU requires putting each variable to a right place (global memory, shared memory, registers, etc.), and this can be done by setting mtype s of each tensor. There are several ways to set mtype s: (Recommended) Leave them to the default \"gpu/global\" first, and modify them with the set_mem_type schedule. In this way, you write some architecture-dependent schedules, but keep your function architecture-independent. (Experimental) Leave them to the default \"gpu/global\" first, and modify them automatically using auto_schedule , or the auto_set_mem_type schedule (which is a part of auto_schedule ). Set them explicitly in the program by setting an optional mtype parameter of ft.Var , ft.empty , etc. mtype=\"byvalue\" for Dynamic Tensor Shapes \u00b6 Tensors with normal mtypes ( \"cpu\" , \"gpu/global\" , etc.) are passed by references, which means a \"cpu\" tensor can only be accessed from a CPU, and a \"gpu/global\" tensor can only be accessed from a GPU. However, sometimes, and especially for dynamic tensor shapes, we want the shapes to be passed by values, and accessible from both CPUs and GPUs (remember we need tensor's shape both when launching a kernel from the CPU side, and during actual computatoin on the GPU side). In this case, we can set the shape-related tensors a \"byvalue\" mtype , and here is an example: import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize(\"Li\", \"threadIdx.x\")) # Use \"byvalue\" for `n` so it can be used both during kernel launching # and inside a kernel def test(n: ft.Var[(), \"int32\", \"input\", \"byvalue\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li # Label the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y)","title":"Running on a GPU"},{"location":"guide/gpu/#example-vector-addition-on-a-gpu","text":"If FreeTensor is built with a CUDA backend, you can compile your program to a GPU. We still take a vector addition as an example: import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): n = 4 # Add verbose=1 to see the resulting native code @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize('Li', 'threadIdx.x')) def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li # Label the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Similar to parallelizing to OpenMP threads , in this example, we parallelize Loop Li to the threadIdx.x dimension of CUDA. There are two major differences: You are now calling parallelize schedule with a threadIdx.x parameter, instead of openmp . All the code are enclosed by a with ft.GPU(0) scope. Usually, you not only parallelize your loops to threadIdx.x , but also other CUDA dimensions like blockIdx.x . To achieve this, you either parallelize different loops in a loop nests to different CUDA dimensions, or split your loops before parallelizing them. As for the with ft.GPU(0) scope, ft.GPU(0) specifies a Device (a specific hardware device of GPU). By calling with on a device, default values of several classes and functions are set, but currently you only need to be aware of two things: It sets the Device of optimize . It sets the default mtype of all tensors in the program, which is an optional parameter of ft.Var , ft.empty , etc. mtype refers to memory type. It controls where a tensor is stored. It defaults to \"cpu\" for a CPU program, and \"gpu/global\" for a GPU program. You probably GPU requires putting each variable to a right place (global memory, shared memory, registers, etc.), and this can be done by setting mtype s of each tensor. There are several ways to set mtype s: (Recommended) Leave them to the default \"gpu/global\" first, and modify them with the set_mem_type schedule. In this way, you write some architecture-dependent schedules, but keep your function architecture-independent. (Experimental) Leave them to the default \"gpu/global\" first, and modify them automatically using auto_schedule , or the auto_set_mem_type schedule (which is a part of auto_schedule ). Set them explicitly in the program by setting an optional mtype parameter of ft.Var , ft.empty , etc.","title":"Example: Vector addition on a GPU"},{"location":"guide/gpu/#mtypebyvalue-for-dynamic-tensor-shapes","text":"Tensors with normal mtypes ( \"cpu\" , \"gpu/global\" , etc.) are passed by references, which means a \"cpu\" tensor can only be accessed from a CPU, and a \"gpu/global\" tensor can only be accessed from a GPU. However, sometimes, and especially for dynamic tensor shapes, we want the shapes to be passed by values, and accessible from both CPUs and GPUs (remember we need tensor's shape both when launching a kernel from the CPU side, and during actual computatoin on the GPU side). In this case, we can set the shape-related tensors a \"byvalue\" mtype , and here is an example: import freetensor as ft import numpy as np # Using the 0-th GPU device with ft.GPU(0): @ft.optimize( # Parallel Loop Li as GPU threads schedule_callback=lambda s: s.parallelize(\"Li\", \"threadIdx.x\")) # Use \"byvalue\" for `n` so it can be used both during kernel launching # and inside a kernel def test(n: ft.Var[(), \"int32\", \"input\", \"byvalue\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li # Label the loop below as \"Li\" for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(4, dtype=\"int32\"), np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y)","title":"mtype=\"byvalue\" for Dynamic Tensor Shapes"},{"location":"guide/hint/","text":"Optimize a Program with Hints \u00b6 Types with Sign Information Provide Information by Assertions Hint Free of Dependence by no_deps Expressions and statements in a program do not always provide enough mathematical information for the compiler. Since the compiler must ensure safety for all possible cases, optimizations might be missed. In FreeTensor, you may provide additional information in some ways to guide the compiler, in other words, guide FreeTensor. Types with Sign Information \u00b6 Suppose you are filling a n by m matrix from 0 to n * m - 1 in row-major order, you may loop from 0 to n * m - 1 with a single loop, and the iterator i by m to find each element in the i // m -th row and i % m -th column: y = ft.empty((n, m), \"int32\") for i in range(n * m): y[i // m, i % m] = i Definitely there are other solutions, for example using two loops, but we are going to use the single-loop program to show a common but unobvious performance pitfall: Integer division in Python (including in FreeTensor) rounds to negative infinity, but integer division in most target instructions and target languages like C++ or CUDA rounds to 0. There is only a difference when dividend is negative, but compiling a general Python division to target architectures has to involve an extra branch to check it. In our example, n and m refer to the shape of a matrix, so it cannot be negative. If we can hint FreeTensor, we can avoid the redundant branch. FreeTensor supports adding a suffix to the data type string to show the sign of a number. Simply changing \"int32\" to \"int32>=0\" will make a difference. All supported suffices are \">0\" , >=0 , <0 , <=0 , !=0 and ==0 . A complete example is below: import freetensor as ft print(\"Without hint\") @ft.optimize(verbose=1) # `verbose=1` prints the code def test_no_hint(n: ft.Var[(), \"int32\"], m: ft.Var[(), \"int32\"]): y = ft.empty((n, m), \"int32\") for i in range(n * m): y[i // m, i % m] = i return y # You will find `runtime_mod` in the code, which involves additional branching assert \"runtime_mod\" in test_no_hint.native_code().code assert \"%\" not in test_no_hint.native_code().code print(\"With hint\") @ft.optimize(verbose=1) # `verbose=1` prints the code def test_hint(n: ft.Var[(), \"int32\"], m: ft.Var[(), \"int32>=0\"]): y = ft.empty((n, m), \"int32\") for i in range(n * m): y[i // m, i % m] = i return y # You will find native C++ `%` in the code, which compiles directly to mod # instructions assert \"runtime_mod\" not in test_hint.native_code().code assert \"%\" in test_hint.native_code().code The sign hint also works for other optimizations. One example is ft.sqrt(x * x) can be automatically optimized to x if x is non-negative. Another example is ft.min(a, b) can be automatically optimized to a if a is negative while b is positive. Provide Information by Assertions \u00b6 Another way to hint FreeTensor is to add some assert statements in the program. In this way, you can add some more precise hints, which reveals mathematical properties among specifc elements, instead of the whole tensor. Here is an example of adding two n -length vectors, and the program is scheduled to execute in parallel. def sch(s): outer, inner = s.split('Li', 32) s.parallelize(outer, 'openmp') @ft.optimize(schedule_callback=sch, verbose=1) def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li for i in range(n): y[i] = a[i] + b[i] return y The algorithm is simple: we use (at most) n // 32 threads, each computing 32 elements. However, if you look the code, you will find the length of the serial loop is not as simple as 32. Instead, it is a complex expression that results in 31 or 32. This is because n is not always divisible by 32 . Suppose in our case, n is really divisible by 32, we can add an assert statement to hint FreeTensor: assert n % 32 == 0 , and the serial loop will have a neat length 32. A complete example is below: import freetensor as ft import re def sch(s): outer, inner = s.split('Li', 32) s.parallelize(outer, 'openmp') @ft.optimize(schedule_callback=sch, verbose=1) def test_no_hint(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li for i in range(n): y[i] = a[i] + b[i] return y # You will not find a 32-length loop assert not re.search(r\".* = 0; .* < 32; .*\\+\\+\", test_no_hint.native_code().code) @ft.optimize(schedule_callback=sch, verbose=1) def test_hint(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") assert n % 32 == 0 #! label: Li for i in range(n): y[i] = a[i] + b[i] return y # You will find a 32-length loop assert re.search(r\".* = 0; .* < 32; .*\\+\\+\", test_hint.native_code().code) Hint Free of Dependence by no_deps \u00b6 Schedules and optimization passes in FreeTensor are guaranteed not to break the program semantics by analyzing the dependence of reads and writes. But sometimes there will be false positives: the program still runs correctly, but some schedules and optimization passes are unable to apply to it. Here is an exmple of iterating through all elements in a sparse matrix in CSR format: import freetensor as ft @ft.schedule(callback=lambda s: s.parallelize(\"Li\", \"openmp\")) @ft.transform def test(ptr, edge1, edge2): ptr: ft.Var[(11,), \"int32\", \"input\", \"cpu\"] edge1: ft.Var[(50,), \"int32\", \"input\", \"cpu\"] edge2: ft.Var[(50,), \"int32\", \"output\", \"cpu\"] #! label: Li #! no_deps: edge2 # ^^^^^^^^^^^^^^ LOOK HERE for i in range(10): for j in range(ptr[i], ptr[i + 1]): edge2[j] = edge1[j] + i Suppose in this example users guarantee values in ptr is monotonically increasing (because it is a CSR format), the i loop can then be parallelized, but FreeTensor does not aknowledge it. Without this assumption, the ranges of j may overlap for different i , which can be expressed as dependence between different i , and it stops FreeTensor to parallelize the i loop. By adding a #! no_deps: edge2 line as in the example, you can hint FreeTensor there is actually no dependence on variable edge1 at all, and FreeTensor can safely do the parallelization. If you are hinting for multiple variables, add one line for each. Note for Automatic Differentiation If you are performing an Automatic Differentiation , you may hint for more variables to make the schedules and passes also applicable to the differentiated program. Specifically, all read-only variables' gradient will be written in the differentiated program. In the case above, edge1 's gradient will be written. Thus, you need to hint no_deps for both edge1 and edge2 (hint for a variable is automatically applied to its gradient).","title":"Optimize a Program with Hints"},{"location":"guide/hint/#types-with-sign-information","text":"Suppose you are filling a n by m matrix from 0 to n * m - 1 in row-major order, you may loop from 0 to n * m - 1 with a single loop, and the iterator i by m to find each element in the i // m -th row and i % m -th column: y = ft.empty((n, m), \"int32\") for i in range(n * m): y[i // m, i % m] = i Definitely there are other solutions, for example using two loops, but we are going to use the single-loop program to show a common but unobvious performance pitfall: Integer division in Python (including in FreeTensor) rounds to negative infinity, but integer division in most target instructions and target languages like C++ or CUDA rounds to 0. There is only a difference when dividend is negative, but compiling a general Python division to target architectures has to involve an extra branch to check it. In our example, n and m refer to the shape of a matrix, so it cannot be negative. If we can hint FreeTensor, we can avoid the redundant branch. FreeTensor supports adding a suffix to the data type string to show the sign of a number. Simply changing \"int32\" to \"int32>=0\" will make a difference. All supported suffices are \">0\" , >=0 , <0 , <=0 , !=0 and ==0 . A complete example is below: import freetensor as ft print(\"Without hint\") @ft.optimize(verbose=1) # `verbose=1` prints the code def test_no_hint(n: ft.Var[(), \"int32\"], m: ft.Var[(), \"int32\"]): y = ft.empty((n, m), \"int32\") for i in range(n * m): y[i // m, i % m] = i return y # You will find `runtime_mod` in the code, which involves additional branching assert \"runtime_mod\" in test_no_hint.native_code().code assert \"%\" not in test_no_hint.native_code().code print(\"With hint\") @ft.optimize(verbose=1) # `verbose=1` prints the code def test_hint(n: ft.Var[(), \"int32\"], m: ft.Var[(), \"int32>=0\"]): y = ft.empty((n, m), \"int32\") for i in range(n * m): y[i // m, i % m] = i return y # You will find native C++ `%` in the code, which compiles directly to mod # instructions assert \"runtime_mod\" not in test_hint.native_code().code assert \"%\" in test_hint.native_code().code The sign hint also works for other optimizations. One example is ft.sqrt(x * x) can be automatically optimized to x if x is non-negative. Another example is ft.min(a, b) can be automatically optimized to a if a is negative while b is positive.","title":"Types with Sign Information"},{"location":"guide/hint/#provide-information-by-assertions","text":"Another way to hint FreeTensor is to add some assert statements in the program. In this way, you can add some more precise hints, which reveals mathematical properties among specifc elements, instead of the whole tensor. Here is an example of adding two n -length vectors, and the program is scheduled to execute in parallel. def sch(s): outer, inner = s.split('Li', 32) s.parallelize(outer, 'openmp') @ft.optimize(schedule_callback=sch, verbose=1) def test(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li for i in range(n): y[i] = a[i] + b[i] return y The algorithm is simple: we use (at most) n // 32 threads, each computing 32 elements. However, if you look the code, you will find the length of the serial loop is not as simple as 32. Instead, it is a complex expression that results in 31 or 32. This is because n is not always divisible by 32 . Suppose in our case, n is really divisible by 32, we can add an assert statement to hint FreeTensor: assert n % 32 == 0 , and the serial loop will have a neat length 32. A complete example is below: import freetensor as ft import re def sch(s): outer, inner = s.split('Li', 32) s.parallelize(outer, 'openmp') @ft.optimize(schedule_callback=sch, verbose=1) def test_no_hint(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") #! label: Li for i in range(n): y[i] = a[i] + b[i] return y # You will not find a 32-length loop assert not re.search(r\".* = 0; .* < 32; .*\\+\\+\", test_no_hint.native_code().code) @ft.optimize(schedule_callback=sch, verbose=1) def test_hint(n: ft.Var[(), \"int32\"], a, b): a: ft.Var[(n,), \"int32\"] b: ft.Var[(n,), \"int32\"] y = ft.empty((n,), \"int32\") assert n % 32 == 0 #! label: Li for i in range(n): y[i] = a[i] + b[i] return y # You will find a 32-length loop assert re.search(r\".* = 0; .* < 32; .*\\+\\+\", test_hint.native_code().code)","title":"Provide Information by Assertions"},{"location":"guide/hint/#hint-free-of-dependence-by-no_deps","text":"Schedules and optimization passes in FreeTensor are guaranteed not to break the program semantics by analyzing the dependence of reads and writes. But sometimes there will be false positives: the program still runs correctly, but some schedules and optimization passes are unable to apply to it. Here is an exmple of iterating through all elements in a sparse matrix in CSR format: import freetensor as ft @ft.schedule(callback=lambda s: s.parallelize(\"Li\", \"openmp\")) @ft.transform def test(ptr, edge1, edge2): ptr: ft.Var[(11,), \"int32\", \"input\", \"cpu\"] edge1: ft.Var[(50,), \"int32\", \"input\", \"cpu\"] edge2: ft.Var[(50,), \"int32\", \"output\", \"cpu\"] #! label: Li #! no_deps: edge2 # ^^^^^^^^^^^^^^ LOOK HERE for i in range(10): for j in range(ptr[i], ptr[i + 1]): edge2[j] = edge1[j] + i Suppose in this example users guarantee values in ptr is monotonically increasing (because it is a CSR format), the i loop can then be parallelized, but FreeTensor does not aknowledge it. Without this assumption, the ranges of j may overlap for different i , which can be expressed as dependence between different i , and it stops FreeTensor to parallelize the i loop. By adding a #! no_deps: edge2 line as in the example, you can hint FreeTensor there is actually no dependence on variable edge1 at all, and FreeTensor can safely do the parallelization. If you are hinting for multiple variables, add one line for each. Note for Automatic Differentiation If you are performing an Automatic Differentiation , you may hint for more variables to make the schedules and passes also applicable to the differentiated program. Specifically, all read-only variables' gradient will be written in the differentiated program. In the case above, edge1 's gradient will be written. Thus, you need to hint no_deps for both edge1 and edge2 (hint for a variable is automatically applied to its gradient).","title":"Hint Free of Dependence by no_deps"},{"location":"guide/schedules/","text":"Optimize a Program with Schedules \u00b6 Example: Parallel Vector addition Combining Multiple Schdules Specify What to Schedule by Selectors Auto Scheduling (Experimental) Oftentimes, only compiling your programs to native code is not enough, and you need further optimizations. This can be done by applying \"schedules\" (explicit program transformations) to you program. Example: Parallel Vector addition \u00b6 import freetensor as ft import numpy as np n = 4 # Add verbose=1 to see the resulting native code @ft.optimize(schedule_callback=lambda s: s.parallelize('Li', 'openmp') ) # <-- 2. Apply the schedule def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li # <-- 1. Label the loop as Li for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Here is an example of a parallel vector addition executed with OpenMP multithreading. Each element is computed by one thread. To achieve this, there are two steps: Label the loop to be parallelized with a #! label: comment. Here label refers to label of an AST node, which is not required to be unique. Apply a parallelize schedule to Li in the schedule_callback argument to optimize ; since the Li label is unambiguous here, the only Li loop is selectd and parallelized. And you are done. You can have a look at the generated OpenMP multithreaded code by setting verbose=1 . Parameter s in schedule_callback is a Schedule object. Besides parallelize , there are more supported scheduling primitives. If you are using the @optimize_to_pytorch integration , you need to set schedules for the forward pass and the backward pass separately. Combining Multiple Schdules \u00b6 Some optimizations can be done by applying multiple schedules. For example, a tiled matrix-multiplication can be done by first split the loops, then reorder them, and finally apply cache s to create tile tensors. In order to demonstrate the idea, we show a simplier example here: still a vector addtion, but with the loop split and only the outer one parallelize d. Please note that this is an example only for demonstration. Usually you do not need it because OpenMP has its own \"schedule(static)\" for parallelized loops. import freetensor as ft import numpy as np n = 1024 def sch(s): outer, inner = s.split('Li', 32) s.parallelize(outer, 'openmp') # Set verbose=1 to see the resulting native code # Set verbose=2 to see the code after EVERY schedule @ft.optimize(schedule_callback=sch) def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(np.arange(1024), dtype=\"int32\"), np.array(np.arange(1024), dtype=\"int32\")).numpy() print(y) One important thing is to track labels of the loops, because the labels will change after schedules. You get labels (to be precise, IDs, which is can be looked-up by labels) of new loops generated from one schedule from its return values ( outer and inner in this case), and pass them to a next schedule. Specify What to Schedule by Selectors \u00b6 In the example above, we label a loop Li and apply schedules on it. It is straight-forward in a tiny example, but as programs grow, it often gets hard to track each statement by a unique label, especially there are inlined function calls. To make things easy, FreeTensor supports specifying a statement by a selector, written in the following rules: A label is a selector. E.g., Li matches a statement with a label Li . (For debugging only) A numerical ID is also a selector. E.g., #31 . A node type surrounded in angle brackets ( <> ) is also a selector. E.g., <For> matches for-loop statements. A selector can be extended to match a new statement produced by a previous schedule. E.g., $split.0{Li} matches the outer loop split from the loop Li . This is useful when return values from schedules are hard to track. Please refer the API document for detailed grammar. Selectors can be combined to match a statement by nesting order. A<-B matches a statement A DIRECTLY NESTED IN another statement B . A<<-B matches a statement DIRECTLY or INDIRECTLY nested in another statement B . A<-(B<-)*C matches a statement A DIRECTLY or INDIRECTLY nested in another statement C with intermedaite nesting statements satisfying the condition in B . B->A matches a statement B directly OUT OF another statement A . B->>A and C->(B->)*A are alike. ( A , B , C can be nested selectors.) Use <-| for the root node, and ->| for a leaf node. Selectors can be combined to match a statement by DFS order. A<:B matches a statement A DIRECTLY BEFORE another statement B . A<<:B matches a statement A DIRECTLY or INDIRECTLY before another statement B . B:>A matches a statment B directly AFTER another statement A . B:>>A matches a statement B directly or indirectly after another statement A . A statement's descents (other statements nested in) or ancestors (other statements nesting it) are neither considered before or after the statement in the DFS order. \"Directly\" means there is no other statement between the comparing statements, so a statment can \"directly before\" both another statement and its parent. Selectors can be combined to match a statement in a function call. A<~B matches a statement A DIRECTLY called by a call site B . A<<~B matches a statement DIRECTLY or INDIRECTLY called by a call site B . A<~(B<~)*C matches a statement A DIRECTLY or INDIRECTLY called by a call site C with intermediate call sites satisfying the condition in B . ( A , B , C can be nested selectors.) Use <~| for the root function. All the arrow-like selectors ( <- , <~ , <: , etc.) are right-associated. For example, A<-B<-C matches A nested in B , where B is nested in C . All the arrow-like selectors can be used with the first argument omitted. For example, <-B matches ALL statements nested in B . Selectors can be combined with logical \"and\" ( & ), \"or\" ( | ), \"not\" ( ! ) and parentheses. E.g., Li|Lj matches a statement labeled Li OR Lj . Li&Lj matches a statement labeled Li&Lj . All schedules support passing selectors. Auto Scheduling (Experimental) \u00b6 Manually scheduling a program requires a lot of efforts. We provide an experimental automatic scheduling functions in Schedule . You can call s.auto_schedule to pick schedules fully automatically. s.auto_schedule calls other s.auto_xxxxxx functions internally, you can also call one or some of them instead. Please note that these auto-scheduling functions are experimental, and their API is subject to changes.","title":"Optimize a Program with Schedules"},{"location":"guide/schedules/#example-parallel-vector-addition","text":"import freetensor as ft import numpy as np n = 4 # Add verbose=1 to see the resulting native code @ft.optimize(schedule_callback=lambda s: s.parallelize('Li', 'openmp') ) # <-- 2. Apply the schedule def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li # <-- 1. Label the loop as Li for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array([1, 2, 3, 4], dtype=\"int32\"), np.array([2, 3, 4, 5], dtype=\"int32\")).numpy() print(y) Here is an example of a parallel vector addition executed with OpenMP multithreading. Each element is computed by one thread. To achieve this, there are two steps: Label the loop to be parallelized with a #! label: comment. Here label refers to label of an AST node, which is not required to be unique. Apply a parallelize schedule to Li in the schedule_callback argument to optimize ; since the Li label is unambiguous here, the only Li loop is selectd and parallelized. And you are done. You can have a look at the generated OpenMP multithreaded code by setting verbose=1 . Parameter s in schedule_callback is a Schedule object. Besides parallelize , there are more supported scheduling primitives. If you are using the @optimize_to_pytorch integration , you need to set schedules for the forward pass and the backward pass separately.","title":"Example: Parallel Vector addition"},{"location":"guide/schedules/#combining-multiple-schdules","text":"Some optimizations can be done by applying multiple schedules. For example, a tiled matrix-multiplication can be done by first split the loops, then reorder them, and finally apply cache s to create tile tensors. In order to demonstrate the idea, we show a simplier example here: still a vector addtion, but with the loop split and only the outer one parallelize d. Please note that this is an example only for demonstration. Usually you do not need it because OpenMP has its own \"schedule(static)\" for parallelized loops. import freetensor as ft import numpy as np n = 1024 def sch(s): outer, inner = s.split('Li', 32) s.parallelize(outer, 'openmp') # Set verbose=1 to see the resulting native code # Set verbose=2 to see the code after EVERY schedule @ft.optimize(schedule_callback=sch) def test(a: ft.Var[(n,), \"int32\"], b: ft.Var[(n,), \"int32\"]): y = ft.empty((n,), \"int32\") #! label: Li for i in range(n): y[i] = a[i] + b[i] return y y = test(np.array(np.arange(1024), dtype=\"int32\"), np.array(np.arange(1024), dtype=\"int32\")).numpy() print(y) One important thing is to track labels of the loops, because the labels will change after schedules. You get labels (to be precise, IDs, which is can be looked-up by labels) of new loops generated from one schedule from its return values ( outer and inner in this case), and pass them to a next schedule.","title":"Combining Multiple Schdules"},{"location":"guide/schedules/#specify-what-to-schedule-by-selectors","text":"In the example above, we label a loop Li and apply schedules on it. It is straight-forward in a tiny example, but as programs grow, it often gets hard to track each statement by a unique label, especially there are inlined function calls. To make things easy, FreeTensor supports specifying a statement by a selector, written in the following rules: A label is a selector. E.g., Li matches a statement with a label Li . (For debugging only) A numerical ID is also a selector. E.g., #31 . A node type surrounded in angle brackets ( <> ) is also a selector. E.g., <For> matches for-loop statements. A selector can be extended to match a new statement produced by a previous schedule. E.g., $split.0{Li} matches the outer loop split from the loop Li . This is useful when return values from schedules are hard to track. Please refer the API document for detailed grammar. Selectors can be combined to match a statement by nesting order. A<-B matches a statement A DIRECTLY NESTED IN another statement B . A<<-B matches a statement DIRECTLY or INDIRECTLY nested in another statement B . A<-(B<-)*C matches a statement A DIRECTLY or INDIRECTLY nested in another statement C with intermedaite nesting statements satisfying the condition in B . B->A matches a statement B directly OUT OF another statement A . B->>A and C->(B->)*A are alike. ( A , B , C can be nested selectors.) Use <-| for the root node, and ->| for a leaf node. Selectors can be combined to match a statement by DFS order. A<:B matches a statement A DIRECTLY BEFORE another statement B . A<<:B matches a statement A DIRECTLY or INDIRECTLY before another statement B . B:>A matches a statment B directly AFTER another statement A . B:>>A matches a statement B directly or indirectly after another statement A . A statement's descents (other statements nested in) or ancestors (other statements nesting it) are neither considered before or after the statement in the DFS order. \"Directly\" means there is no other statement between the comparing statements, so a statment can \"directly before\" both another statement and its parent. Selectors can be combined to match a statement in a function call. A<~B matches a statement A DIRECTLY called by a call site B . A<<~B matches a statement DIRECTLY or INDIRECTLY called by a call site B . A<~(B<~)*C matches a statement A DIRECTLY or INDIRECTLY called by a call site C with intermediate call sites satisfying the condition in B . ( A , B , C can be nested selectors.) Use <~| for the root function. All the arrow-like selectors ( <- , <~ , <: , etc.) are right-associated. For example, A<-B<-C matches A nested in B , where B is nested in C . All the arrow-like selectors can be used with the first argument omitted. For example, <-B matches ALL statements nested in B . Selectors can be combined with logical \"and\" ( & ), \"or\" ( | ), \"not\" ( ! ) and parentheses. E.g., Li|Lj matches a statement labeled Li OR Lj . Li&Lj matches a statement labeled Li&Lj . All schedules support passing selectors.","title":"Specify What to Schedule by Selectors"},{"location":"guide/schedules/#auto-scheduling-experimental","text":"Manually scheduling a program requires a lot of efforts. We provide an experimental automatic scheduling functions in Schedule . You can call s.auto_schedule to pick schedules fully automatically. s.auto_schedule calls other s.auto_xxxxxx functions internally, you can also call one or some of them instead. Please note that these auto-scheduling functions are experimental, and their API is subject to changes.","title":"Auto Scheduling (Experimental)"}]}